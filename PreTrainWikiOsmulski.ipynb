{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e2ffc8-e1c3-4d44-8e6b-248b073af990",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre Train on Wiki & Train on Osmulski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fdfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "from datasets import Dataset # HuggingFace\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForMultipleChoice, TrainingArguments,\\\n",
    "                         Trainer, AutoModel, IntervalStrategy,logging, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04506376-fe46-4750-8e8e-c6bb559af9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'PreTrainWikiOsmulski.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385d217-eec6-4afb-ae8c-9e42d52425f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79c1af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install polars -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b40123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c680d-d26f-41ad-add6-d205c2c9ce25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deberta_v3_large = 'microsoft/deberta-v3-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a2323-3b93-4387-bba7-42d425d38e46",
   "metadata": {},
   "source": [
    "## Pretain on Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78822ded-1230-4f40-a404-980ae73504a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79118, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>title</th><th>page_id</th><th>parent_id</th><th>revision_id</th><th>revision_ts</th><th>short_description</th><th>sha1</th><th>page_bytes</th><th>section_index</th><th>section_title</th><th>section_level</th><th>section_text</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>datetime[μs]</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;&#x27;t Hooft loop&quot;</td><td>3386119</td><td>1136040727</td><td>1150612405</td><td>2023-04-19 03:13:11</td><td>&quot;Magnetic loop …</td><td>&quot;0u40ciisnuhjlc…</td><td>12559</td><td>0</td><td>&quot;Summary&quot;</td><td>1</td><td>&quot;In quantum fie…</td></tr><tr><td>&quot;&#x27;t Hooft loop&quot;</td><td>3386119</td><td>1136040727</td><td>1150612405</td><td>2023-04-19 03:13:11</td><td>&quot;Magnetic loop …</td><td>&quot;0u40ciisnuhjlc…</td><td>12559</td><td>1</td><td>&quot;Definition&quot;</td><td>2</td><td>&quot;There are a nu…</td></tr><tr><td>&quot;&#x27;t Hooft loop&quot;</td><td>3386119</td><td>1136040727</td><td>1150612405</td><td>2023-04-19 03:13:11</td><td>&quot;Magnetic loop …</td><td>&quot;0u40ciisnuhjlc…</td><td>12559</td><td>2</td><td>&quot;Disorder opera…</td><td>2</td><td>&quot;The &#x27;t Hooft l…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 12)\n",
       "┌────────────┬─────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ title      ┆ page_id ┆ parent_id ┆ revision_ ┆ … ┆ section_i ┆ section_t ┆ section_l ┆ section_t │\n",
       "│ ---        ┆ ---     ┆ ---       ┆ id        ┆   ┆ ndex      ┆ itle      ┆ evel      ┆ ext       │\n",
       "│ str        ┆ i64     ┆ i64       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆         ┆           ┆ i64       ┆   ┆ i64       ┆ str       ┆ i64       ┆ str       │\n",
       "╞════════════╪═════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 't Hooft   ┆ 3386119 ┆ 113604072 ┆ 115061240 ┆ … ┆ 0         ┆ Summary   ┆ 1         ┆ In        │\n",
       "│ loop       ┆         ┆ 7         ┆ 5         ┆   ┆           ┆           ┆           ┆ quantum   │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ field     │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ theory,   │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ the 't …  │\n",
       "│ 't Hooft   ┆ 3386119 ┆ 113604072 ┆ 115061240 ┆ … ┆ 1         ┆ Definitio ┆ 2         ┆ There are │\n",
       "│ loop       ┆         ┆ 7         ┆ 5         ┆   ┆           ┆ n         ┆           ┆ a number  │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ of ways   │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ to de…    │\n",
       "│ 't Hooft   ┆ 3386119 ┆ 113604072 ┆ 115061240 ┆ … ┆ 2         ┆ Disorder  ┆ 2         ┆ The 't    │\n",
       "│ loop       ┆         ┆ 7         ┆ 5         ┆   ┆           ┆ operator  ┆           ┆ Hooft     │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ loop is a │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ disorder  │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ …         │\n",
       "└────────────┴─────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sections = pl.read_parquet('./data/wiki_physics_math.parquet')\n",
    "print(wiki_sections.shape)\n",
    "wiki_sections[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b3eb6b-f41d-444c-826f-a3e7d9e1fce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "deberta_v3_large = 'microsoft/deberta-v3-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large)\n",
    "model = AutoModelForMaskedLM.from_pretrained(deberta_v3_large)\n",
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6cea2df-9b78-475d-9524-b8b5d6f9eab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_wiki_section(section_text):\n",
    "    return tokenizer.encode_plus(section_text['section_text'],\n",
    "                                 add_special_tokens=True,\n",
    "                                 max_length=512,\n",
    "                                 stride=64,\n",
    "                                 truncation=True,\n",
    "                                 return_overflowing_tokens=True,\n",
    "                                 padding='max_length')\n",
    "\n",
    "retokenize = False\n",
    "wiki_flat_tokenized_path = Path('./data/wiki_flat_tokenized.arrow')\n",
    "\n",
    "if retokenize or not wiki_flat_tokenized_path.exists():\n",
    "    wiki_dataset = Dataset.from_pandas(wiki_sections.to_pandas()[['section_text']], preserve_index=False)\n",
    "    wiki_tokenized = wiki_dataset.map(tokenize_wiki_section, \n",
    "                                      remove_columns=['section_text'])\n",
    "\n",
    "    flat_tokenized = {}\n",
    "    for key in tqdm(list(wiki_tokenized.features)):\n",
    "        flattened_list = [item for sublist in wiki_tokenized[key] for item in sublist]\n",
    "        flat_tokenized[key] = flattened_list\n",
    "    \n",
    "    flat_tokenized = Dataset.from_dict(flat_tokenized)\n",
    "    flat_tokenized.save_to_disk(str(wiki_flat_tokenized_path))\n",
    "else:\n",
    "    flat_tokenized = Dataset.load_from_disk(str(wiki_flat_tokenized_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb8661b3-2a94-4769-adf0-d2d7d106dd64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 80000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 80000\n",
    "test_size = int(0.025 * train_size)\n",
    "wiki_split_sample = flat_tokenized.train_test_split(train_size=train_size, test_size=test_size)\n",
    "wiki_split_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3a20e16-8a39-4dc0-9641-6144491eead9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "''' Apply random masking once on the whole test data, then uses the default data collector\n",
    "to handle the test dataset in batches '''\n",
    "\n",
    "masking_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = masking_data_collator(features)\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "eval_dataset = wiki_split_sample[\"test\"].map(insert_random_mask,batched=True,\n",
    "    remove_columns=wiki_split_sample[\"test\"].column_names)\n",
    "\n",
    "eval_dataset = eval_dataset.rename_columns({\n",
    "    \"masked_input_ids\": \"input_ids\",\n",
    "    \"masked_attention_mask\": \"attention_mask\",\n",
    "    \"masked_labels\": \"labels\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e807f-0dc1-41ca-8a8d-8e8783f2a64c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06b98f6d-fec4-4461-b17a-3d1f2c2e2a18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdatadan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniel/code/kaggle-science-exam/wandb/run-20230810_234845-s1x1w5e0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/datadan/huggingface/runs/s1x1w5e0' target=\"_blank\">Train on Wiki</a></strong> to <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">https://wandb.ai/datadan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/datadan/huggingface/runs/s1x1w5e0' target=\"_blank\">https://wandb.ai/datadan/huggingface/runs/s1x1w5e0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46501' max='60000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46501/60000 5:36:59 < 1:37:49, 2.30 it/s, Epoch 2.33/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>11.531000</td>\n",
       "      <td>11.398396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>9.909500</td>\n",
       "      <td>9.710987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>8.991400</td>\n",
       "      <td>8.950085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.536300</td>\n",
       "      <td>8.468560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>8.155600</td>\n",
       "      <td>8.162786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.860900</td>\n",
       "      <td>7.930933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>7.656800</td>\n",
       "      <td>7.678810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>7.363900</td>\n",
       "      <td>7.467319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>7.215700</td>\n",
       "      <td>7.270891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>7.026000</td>\n",
       "      <td>7.127275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>6.800600</td>\n",
       "      <td>6.955063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.603800</td>\n",
       "      <td>6.824952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>6.483500</td>\n",
       "      <td>6.714836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>6.384700</td>\n",
       "      <td>6.584514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>6.357900</td>\n",
       "      <td>6.498496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>6.197600</td>\n",
       "      <td>6.383639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>6.023700</td>\n",
       "      <td>6.302174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>6.008600</td>\n",
       "      <td>6.208351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.876400</td>\n",
       "      <td>6.105667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.771500</td>\n",
       "      <td>6.048708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>5.731700</td>\n",
       "      <td>5.963768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.568500</td>\n",
       "      <td>5.890453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>5.637900</td>\n",
       "      <td>5.842842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>5.442900</td>\n",
       "      <td>5.776756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>5.426200</td>\n",
       "      <td>5.720446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.435500</td>\n",
       "      <td>5.676692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>5.227400</td>\n",
       "      <td>5.594121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.210700</td>\n",
       "      <td>5.572110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>5.151800</td>\n",
       "      <td>5.490376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>5.057300</td>\n",
       "      <td>5.427224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>5.077900</td>\n",
       "      <td>5.365080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.994000</td>\n",
       "      <td>5.316794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>4.991700</td>\n",
       "      <td>5.261258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>4.945300</td>\n",
       "      <td>5.251460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>4.953100</td>\n",
       "      <td>5.180873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>4.801600</td>\n",
       "      <td>5.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>4.781800</td>\n",
       "      <td>5.102199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>4.727500</td>\n",
       "      <td>5.073906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>4.673500</td>\n",
       "      <td>5.037654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>4.544400</td>\n",
       "      <td>4.973937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>4.530000</td>\n",
       "      <td>4.945246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>4.475400</td>\n",
       "      <td>4.863017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>4.496000</td>\n",
       "      <td>4.851706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.513700</td>\n",
       "      <td>4.828101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>4.553200</td>\n",
       "      <td>4.780302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>4.402900</td>\n",
       "      <td>4.749976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>4.353000</td>\n",
       "      <td>4.729233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>4.341200</td>\n",
       "      <td>4.683784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>4.264800</td>\n",
       "      <td>4.637178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>4.257200</td>\n",
       "      <td>4.597590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.237000</td>\n",
       "      <td>4.584415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.154800</td>\n",
       "      <td>4.568751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>4.170200</td>\n",
       "      <td>4.518249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>4.182400</td>\n",
       "      <td>4.491341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>4.103300</td>\n",
       "      <td>4.468292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>4.032100</td>\n",
       "      <td>4.428428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>4.058500</td>\n",
       "      <td>4.401951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.979800</td>\n",
       "      <td>4.379922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.905600</td>\n",
       "      <td>4.340766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.976200</td>\n",
       "      <td>4.348774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.838000</td>\n",
       "      <td>4.312339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.852300</td>\n",
       "      <td>4.239895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.834600</td>\n",
       "      <td>4.261010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.850500</td>\n",
       "      <td>4.215486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.841500</td>\n",
       "      <td>4.219498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.725200</td>\n",
       "      <td>4.202852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.762700</td>\n",
       "      <td>4.164887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.744100</td>\n",
       "      <td>4.184446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.691400</td>\n",
       "      <td>4.154107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.644500</td>\n",
       "      <td>4.130332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.715000</td>\n",
       "      <td>4.109548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.625800</td>\n",
       "      <td>4.079093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.630600</td>\n",
       "      <td>4.056569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.667400</td>\n",
       "      <td>4.039770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.651700</td>\n",
       "      <td>4.060060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.536400</td>\n",
       "      <td>4.050238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>3.576100</td>\n",
       "      <td>4.015827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.595800</td>\n",
       "      <td>3.961769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>3.550600</td>\n",
       "      <td>4.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.566600</td>\n",
       "      <td>3.987325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>3.510900</td>\n",
       "      <td>3.968434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.427200</td>\n",
       "      <td>3.952715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>3.452000</td>\n",
       "      <td>3.921002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.479400</td>\n",
       "      <td>3.934726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>3.427500</td>\n",
       "      <td>3.921959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.436000</td>\n",
       "      <td>3.909147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>3.482600</td>\n",
       "      <td>3.914302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.455700</td>\n",
       "      <td>3.897415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>3.506700</td>\n",
       "      <td>3.880064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.472400</td>\n",
       "      <td>3.866134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>3.452300</td>\n",
       "      <td>3.852868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.442700</td>\n",
       "      <td>3.846886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain = True\n",
    "\n",
    "output_path = Path('./checkpoints/wiki_pretrain')\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.5,\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=10000,\n",
    "    report_to='wandb',\n",
    "    output_dir=str(output_path),\n",
    "    run_name='Train on Wiki'\n",
    ")\n",
    "\n",
    "if not output_path.exists() or retrain:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(deberta_v3_large)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=masking_data_collator,\n",
    "        train_dataset=wiki_split_sample['train'],\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "else:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(output_path/'checkpoint-19500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a3f721-0a16-45d8-bce3-78e89ef49a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_pretrained_path = './checkpoints/wiki_pretraiend'\n",
    "trainer.model.save_pretrained(wiki_pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04c2871d-5642-4dfc-9251-24741856201a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4a3c2-220a-4f7e-b241-dc15daf907a2",
   "metadata": {},
   "source": [
    "## Train on Osmulski Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a60a98de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/train.csv')\n",
    "df_test = df_test.drop(columns=\"id\")\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33962dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5800, 7) (200, 7) (500, 7)\n"
     ]
    }
   ],
   "source": [
    "df_6000 = pd.read_csv('data/osmulski_6000.csv')\n",
    "df_train = df_6000[:5800]\n",
    "df_test_1 = df_6000[5800:]\n",
    "df_test_2 = pd.read_csv('data/osmulski_extra_train.csv')\n",
    "print(df_train.shape, df_test_1.shape, df_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "264141cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentences = [example[option] for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=True)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b6ccf20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 5800\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train, preserve_index=False)\n",
    "tokenized_train = train_dataset.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "602f4aa9-2fbe-43fd-aff3-f78babd299f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "tokenized_test = test_dataset.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319e9c6-0623-476e-a621-19a8f363f7ec",
   "metadata": {},
   "source": [
    "## Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aeae4b46-2117-45bc-9730-e90506c099ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision_at_k(predictions, actuals, k=3):        \n",
    "    if isinstance(actuals, list):\n",
    "        actuals = np.array(actuals)\n",
    "        \n",
    "    found_at = np.where(predictions == actuals.reshape(-1, 1))\n",
    "    # found_at is a tuple with the array of found indices in the second position\n",
    "    score = 1 / (1 + found_at[1])\n",
    "    score[score < 1/k] = 0\n",
    "    return score\n",
    "\n",
    "def mean_avg_precision_at_k(predictions, actual, k=3):\n",
    "    n = predictions.shape[0]\n",
    "    row_precision = precision_at_k(predictions, actual)\n",
    "    return row_precision.sum()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f891f3a1-350c-4d01-be6b-5541ba828e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def score_random():\n",
    "    n_permutations = 200\n",
    "    n_numbers = 5 \n",
    "    # In this code, np.random.rand(n_permutations, n_numbers) generates a 2D array of random numbers.\n",
    "    # argsort(axis=1) then sorts along the second dimension (i.e., sorts each row) but instead of \n",
    "    # sorting the actual numbers, it sorts their indices, effectively creating a permutation.\n",
    "    random_predictions = np.random.rand(n_permutations, n_numbers).argsort(axis=1)\n",
    "    random_actuals = np.random.randint(0, n_numbers-1, n_permutations)\n",
    "    return mean_avg_precision_at_k(random_predictions, random_actuals)\n",
    "    \n",
    "scores = []\n",
    "for i in range(100000):\n",
    "    scores.append(score_random())\n",
    "    \n",
    "pd.Series(scores).hist(bins=25)\n",
    "plt.title('Scores of Random Predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1adf4c20-dad1-4475-8d06-c754d80c394f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.flip(predictions.argsort(axis=1), axis=1)\n",
    "    accuracy = acc_metric.compute(predictions=predictions[:,0], references=labels)['accuracy']\n",
    "    map_at_3 = mean_avg_precision_at_k(predictions, labels)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'map_at_3': round(map_at_3, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28898fcf",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c95a3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./checkpoints/wiki_pretraiend were not used when initializing DebertaV2ForMultipleChoice: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at ./checkpoints/wiki_pretraiend and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniel/code/kaggle-science-exam/wandb/run-20230811_084236-urlyvk73</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/datadan/huggingface/runs/urlyvk73' target=\"_blank\">Train on reduced Osmulski</a></strong> to <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">https://wandb.ai/datadan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/datadan/huggingface/runs/urlyvk73' target=\"_blank\">https://wandb.ai/datadan/huggingface/runs/urlyvk73</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8700' max='8700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8700/8700 18:51, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Map At 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.609500</td>\n",
       "      <td>1.607671</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.614300</td>\n",
       "      <td>1.607365</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.652700</td>\n",
       "      <td>1.605772</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.595000</td>\n",
       "      <td>1.604456</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.600700</td>\n",
       "      <td>1.594494</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.539600</td>\n",
       "      <td>1.568919</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.557200</td>\n",
       "      <td>1.522659</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.499900</td>\n",
       "      <td>1.468443</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.371700</td>\n",
       "      <td>1.388031</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.345700</td>\n",
       "      <td>1.319852</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.508500</td>\n",
       "      <td>1.253971</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.159214</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.428200</td>\n",
       "      <td>1.182011</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.224600</td>\n",
       "      <td>1.066139</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.105900</td>\n",
       "      <td>1.024527</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.163600</td>\n",
       "      <td>1.009152</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.079100</td>\n",
       "      <td>1.106492</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.168900</td>\n",
       "      <td>0.984842</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.872300</td>\n",
       "      <td>0.917172</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>0.918428</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.947600</td>\n",
       "      <td>0.893340</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.099300</td>\n",
       "      <td>1.111903</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.727200</td>\n",
       "      <td>0.907250</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>0.985373</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.918886</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.893793</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.749800</td>\n",
       "      <td>1.009217</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.909400</td>\n",
       "      <td>0.977056</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>0.949197</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>1.048883</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>1.051345</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.530700</td>\n",
       "      <td>1.134770</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.396700</td>\n",
       "      <td>1.118265</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.531200</td>\n",
       "      <td>1.167373</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.786000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▂▂▂▂▄▄▆▆▆▇██▇▆████▆██▇██▇▇▇████</td></tr><tr><td>eval/loss</td><td>██████▇▇▆▅▅▄▄▃▂▂▃▂▁▁▁▃▁▂▁▁▂▂▂▃▃▃▃▄</td></tr><tr><td>eval/map_at_3</td><td>▁▁▁▁▂▂▂▄▄▆▆▆▇▇█▇▇████▆██▇██▇▇▇████</td></tr><tr><td>eval/runtime</td><td>▅▂▃▂▃▂▃▃▁▃▄▆▃▂▄█▄▅▂▅▅▄▅▄▇▅▅▄▅▂▃▅▃█</td></tr><tr><td>eval/samples_per_second</td><td>▄▇▆▇▆▇▆▆█▆▅▃▆▇▅▁▅▄▇▄▄▅▄▅▂▄▄▅▄▇▆▄▆▁</td></tr><tr><td>eval/steps_per_second</td><td>▄▇▆▇▆▇▆▆█▆▅▃▆▇▅▂▅▄▇▄▄▅▄▅▂▄▄▅▄▇▆▄▆▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇████▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>████████▇▇█▆▆▇▅▆▅▅▆▇▅▅▄▄▃▄▅▂▃▃▂▃▃▂▂▁▁▂▂▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.665</td></tr><tr><td>eval/loss</td><td>1.16737</td></tr><tr><td>eval/map_at_3</td><td>0.786</td></tr><tr><td>eval/runtime</td><td>2.386</td></tr><tr><td>eval/samples_per_second</td><td>83.822</td></tr><tr><td>eval/steps_per_second</td><td>8.382</td></tr><tr><td>train/epoch</td><td>6.0</td></tr><tr><td>train/global_step</td><td>8700</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5376</td></tr><tr><td>train/total_flos</td><td>1.608493474744236e+16</td></tr><tr><td>train/train_loss</td><td>1.02463</td></tr><tr><td>train/train_runtime</td><td>1133.9247</td></tr><tr><td>train/train_samples_per_second</td><td>30.69</td></tr><tr><td>train/train_steps_per_second</td><td>7.672</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Train on reduced Osmulski</strong> at: <a href='https://wandb.ai/datadan/huggingface/runs/urlyvk73' target=\"_blank\">https://wandb.ai/datadan/huggingface/runs/urlyvk73</a><br/> View job at <a href='https://wandb.ai/datadan/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg5NTM0Nzc2/version_details/v7' target=\"_blank\">https://wandb.ai/datadan/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg5NTM0Nzc2/version_details/v7</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230811_084236-urlyvk73/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrain = True\n",
    "\n",
    "output_path = Path('./checkpoints/osmulski_train')\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.8,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=10,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    logging_steps=10,\n",
    "    eval_steps=250,\n",
    "    num_train_epochs=6,\n",
    "    save_steps=5000,\n",
    "    report_to='wandb',\n",
    "    output_dir=str(output_path),\n",
    "    run_name='Train on reduced Osmulski'\n",
    ")\n",
    "\n",
    "if not output_path.exists() or retrain:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(wiki_pretrained_path)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "else:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(output_path/'checkpoint-19500')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ac5ec",
   "metadata": {},
   "source": [
    "## Predicting on the Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11a90242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.67, 'map_at_3': 0.788}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.55, 'map_at_3': 0.695}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.55, 'map_at_3': 0.687}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(test_set):\n",
    "    tokenized_test_dataset = Dataset.from_pandas(test_set).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E'])\n",
    "    test_predictions = trainer.predict(tokenized_test_dataset).predictions\n",
    "    print(compute_metrics([test_predictions, tokenized_test_dataset['label']]))\n",
    "    \n",
    "evaluate_model(df_test)\n",
    "evaluate_model(df_test_1)\n",
    "evaluate_model(df_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84d1e74a-6365-4f84-83a0-3ade8664b72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#trainer_predict = trainer.predict(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f0840fe-6b1a-4a3f-a572-5c02ec51e72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test_dataset = Dataset.from_pandas(df_test).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E'])\n",
    "test_logits = trainer.predict(tokenized_test_dataset).predictions\n",
    "test_probs = softmax(test_logits, axis=1)\n",
    "predictions = np.flip(test_logits.argsort(axis=1), axis=1)\n",
    "pred_1 = predictions[:, 0]\n",
    "pred_2 = predictions[:, 1]\n",
    "pred_3 = predictions[:, 2]\n",
    "\n",
    "row_indcs = np.arange(pred_1.shape[0])\n",
    "res = pd.DataFrame({\n",
    "    'pred_1': pred_1, \n",
    "    'prob_1': test_probs[row_indcs, pred_1],\n",
    "    'pred_2': pred_2, \n",
    "    'prob_2': test_probs[row_indcs, pred_2],\n",
    "    'pred_3': pred_3, \n",
    "    'prob_3': test_probs[row_indcs, pred_3],\n",
    "    'logit_sum': test_probs.sum(axis=1),\n",
    "    'prob_3_sum': test_probs[row_indcs, pred_1] + test_probs[row_indcs, pred_2] + test_probs[row_indcs, pred_3],\n",
    "    'actual': tokenized_test_dataset['label'],\n",
    "    'accurate': pred_1 == tokenized_test_dataset['label'],\n",
    "    'precision_at_3': precision_at_k(predictions, tokenized_test_dataset['label']).round(2)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04a71d78-655f-46a8-9742-a398e9dbafc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_1</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>prob_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>prob_3</th>\n",
       "      <th>logit_sum</th>\n",
       "      <th>prob_3_sum</th>\n",
       "      <th>actual</th>\n",
       "      <th>accurate</th>\n",
       "      <th>precision_at_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.867177</td>\n",
       "      <td>2</td>\n",
       "      <td>0.060082</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973232</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.384794</td>\n",
       "      <td>3</td>\n",
       "      <td>0.229879</td>\n",
       "      <td>0</td>\n",
       "      <td>0.216674</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.831346</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.692946</td>\n",
       "      <td>2</td>\n",
       "      <td>0.154489</td>\n",
       "      <td>4</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939527</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.350951</td>\n",
       "      <td>1</td>\n",
       "      <td>0.244707</td>\n",
       "      <td>0</td>\n",
       "      <td>0.237140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.832798</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.345061</td>\n",
       "      <td>3</td>\n",
       "      <td>0.286901</td>\n",
       "      <td>1</td>\n",
       "      <td>0.263349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.895311</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.674848</td>\n",
       "      <td>2</td>\n",
       "      <td>0.183446</td>\n",
       "      <td>3</td>\n",
       "      <td>0.059358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.917651</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.599422</td>\n",
       "      <td>2</td>\n",
       "      <td>0.315494</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991035</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0.540555</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352013</td>\n",
       "      <td>4</td>\n",
       "      <td>0.096040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.988608</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.509058</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390489</td>\n",
       "      <td>2</td>\n",
       "      <td>0.080096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979643</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.997954</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999747</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.311665</td>\n",
       "      <td>4</td>\n",
       "      <td>0.293321</td>\n",
       "      <td>0</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846486</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447903</td>\n",
       "      <td>1</td>\n",
       "      <td>0.278869</td>\n",
       "      <td>2</td>\n",
       "      <td>0.144810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871583</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.997349</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0.996248</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998803</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.967386</td>\n",
       "      <td>2</td>\n",
       "      <td>0.023223</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998418</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.478593</td>\n",
       "      <td>2</td>\n",
       "      <td>0.235248</td>\n",
       "      <td>3</td>\n",
       "      <td>0.168150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881990</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>0.889369</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995578</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0.426521</td>\n",
       "      <td>0</td>\n",
       "      <td>0.263443</td>\n",
       "      <td>3</td>\n",
       "      <td>0.261408</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951372</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>0.457016</td>\n",
       "      <td>0</td>\n",
       "      <td>0.418056</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.942232</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>0.996525</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>0.978562</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996114</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.298158</td>\n",
       "      <td>4</td>\n",
       "      <td>0.261631</td>\n",
       "      <td>3</td>\n",
       "      <td>0.221704</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781493</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0.694618</td>\n",
       "      <td>2</td>\n",
       "      <td>0.192220</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965157</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>0.612315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333254</td>\n",
       "      <td>3</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.981866</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.319313</td>\n",
       "      <td>4</td>\n",
       "      <td>0.313440</td>\n",
       "      <td>3</td>\n",
       "      <td>0.293677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.926429</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.513298</td>\n",
       "      <td>4</td>\n",
       "      <td>0.270580</td>\n",
       "      <td>2</td>\n",
       "      <td>0.215069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998947</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>0.991860</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>0.345262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>3</td>\n",
       "      <td>0.162558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.758986</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0.948775</td>\n",
       "      <td>3</td>\n",
       "      <td>0.032165</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>0.969604</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992756</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>0.338083</td>\n",
       "      <td>1</td>\n",
       "      <td>0.270573</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202366</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811022</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>0.480130</td>\n",
       "      <td>3</td>\n",
       "      <td>0.377145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.136106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993381</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>0.292815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.283962</td>\n",
       "      <td>4</td>\n",
       "      <td>0.183530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.760307</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>0.500728</td>\n",
       "      <td>3</td>\n",
       "      <td>0.408555</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987832</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>0.416513</td>\n",
       "      <td>0</td>\n",
       "      <td>0.186835</td>\n",
       "      <td>1</td>\n",
       "      <td>0.164088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767437</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0.951308</td>\n",
       "      <td>3</td>\n",
       "      <td>0.041580</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999765</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>0.809734</td>\n",
       "      <td>3</td>\n",
       "      <td>0.189950</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>0.557977</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245381</td>\n",
       "      <td>4</td>\n",
       "      <td>0.136614</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939972</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred_1    prob_1  pred_2    prob_2  pred_3    prob_3  logit_sum  \\\n",
       "0        3  0.867177       2  0.060082       1  0.045974        1.0   \n",
       "1        2  0.384794       3  0.229879       0  0.216674        1.0   \n",
       "2        0  0.692946       2  0.154489       4  0.092092        1.0   \n",
       "3        2  0.350951       1  0.244707       0  0.237140        1.0   \n",
       "4        0  0.345061       3  0.286901       1  0.263349        1.0   \n",
       "5        1  0.674848       2  0.183446       3  0.059358        1.0   \n",
       "6        0  0.599422       2  0.315494       1  0.076119        1.0   \n",
       "7        3  0.540555       1  0.352013       4  0.096040        1.0   \n",
       "8        0  0.509058       1  0.390489       2  0.080096        1.0   \n",
       "9        0  0.997954       1  0.001455       4  0.000338        1.0   \n",
       "10       1  0.311665       4  0.293321       0  0.241500        1.0   \n",
       "11       0  0.447903       1  0.278869       2  0.144810        1.0   \n",
       "12       2  0.997349       4  0.001130       1  0.000765        1.0   \n",
       "13       4  0.996248       3  0.001792       2  0.000762        1.0   \n",
       "14       1  0.967386       2  0.023223       3  0.007810        1.0   \n",
       "15       1  0.478593       2  0.235248       3  0.168150        1.0   \n",
       "16       2  0.889369       0  0.074125       1  0.032083        1.0   \n",
       "17       4  0.426521       0  0.263443       3  0.261408        1.0   \n",
       "18       3  0.457016       0  0.418056       4  0.067159        1.0   \n",
       "19       4  0.996525       3  0.002975       1  0.000205        1.0   \n",
       "20       3  0.978562       2  0.009485       1  0.008066        1.0   \n",
       "21       1  0.298158       4  0.261631       3  0.221704        1.0   \n",
       "22       3  0.694618       2  0.192220       0  0.078320        1.0   \n",
       "23       2  0.612315       1  0.333254       3  0.036297        1.0   \n",
       "24       0  0.319313       4  0.313440       3  0.293677        1.0   \n",
       "25       4  0.999945       1  0.000014       3  0.000014        1.0   \n",
       "26       0  0.513298       4  0.270580       2  0.215069        1.0   \n",
       "27       3  0.991860       0  0.004413       2  0.002777        1.0   \n",
       "28       2  0.345262       0  0.251166       3  0.162558        1.0   \n",
       "29       2  0.999991       1  0.000005       4  0.000001        1.0   \n",
       "30       1  0.948775       3  0.032165       4  0.006818        1.0   \n",
       "31       4  0.969604       0  0.011620       3  0.011532        1.0   \n",
       "32       3  0.338083       1  0.270573       0  0.202366        1.0   \n",
       "33       4  0.480130       3  0.377145       1  0.136106        1.0   \n",
       "34       2  0.292815       1  0.283962       4  0.183530        1.0   \n",
       "35       4  0.500728       3  0.408555       1  0.078549        1.0   \n",
       "36       4  0.416513       0  0.186835       1  0.164088        1.0   \n",
       "37       0  0.951308       3  0.041580       2  0.006876        1.0   \n",
       "38       4  0.809734       3  0.189950       1  0.000250        1.0   \n",
       "39       2  0.557977       0  0.245381       4  0.136614        1.0   \n",
       "\n",
       "    prob_3_sum  actual  accurate  precision_at_3  \n",
       "0     0.973232       3      True            1.00  \n",
       "1     0.831346       0     False            0.33  \n",
       "2     0.939527       0      True            1.00  \n",
       "3     0.832798       2      True            1.00  \n",
       "4     0.895311       3     False            0.50  \n",
       "5     0.917651       1      True            1.00  \n",
       "6     0.991035       0      True            1.00  \n",
       "7     0.988608       3      True            1.00  \n",
       "8     0.979643       2     False            0.33  \n",
       "9     0.999747       0      True            1.00  \n",
       "10    0.846486       4     False            0.50  \n",
       "11    0.871583       0      True            1.00  \n",
       "12    0.999244       2      True            1.00  \n",
       "13    0.998803       3     False            0.50  \n",
       "14    0.998418       1      True            1.00  \n",
       "15    0.881990       1      True            1.00  \n",
       "16    0.995578       4     False            0.00  \n",
       "17    0.951372       4      True            1.00  \n",
       "18    0.942232       0     False            0.50  \n",
       "19    0.999705       4      True            1.00  \n",
       "20    0.996114       3      True            1.00  \n",
       "21    0.781493       3     False            0.33  \n",
       "22    0.965157       2     False            0.50  \n",
       "23    0.981866       2      True            1.00  \n",
       "24    0.926429       4     False            0.50  \n",
       "25    0.999973       4      True            1.00  \n",
       "26    0.998947       0      True            1.00  \n",
       "27    0.999049       3      True            1.00  \n",
       "28    0.758986       4     False            0.00  \n",
       "29    0.999997       2      True            1.00  \n",
       "30    0.987758       1      True            1.00  \n",
       "31    0.992756       4      True            1.00  \n",
       "32    0.811022       4     False            0.00  \n",
       "33    0.993381       3     False            0.50  \n",
       "34    0.760307       2      True            1.00  \n",
       "35    0.987832       1     False            0.33  \n",
       "36    0.767437       4      True            1.00  \n",
       "37    0.999765       0      True            1.00  \n",
       "38    0.999934       4      True            1.00  \n",
       "39    0.939972       4     False            0.33  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b89c75e-68d4-4350-8d43-57e9d43a4f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.00    129\n",
       "0.50     39\n",
       "0.33     17\n",
       "0.00     15\n",
       "Name: precision_at_3, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['precision_at_3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f2c8820-1c49-4084-a52e-235a3dcdc6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res[res['accurate']]['prob_sum'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a030acc-067f-49b2-9932-eb28aeddd6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e0a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f101ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_as_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions_as_answer_letters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mABCDE\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[43mpredictions_as_ids\u001b[49m]\n\u001b[1;32m      2\u001b[0m predictions_as_answer_letters[:\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_as_ids' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "predictions_as_answer_letters[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "591ead9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D B C', 'A E B', 'A C E']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_as_string = df_test['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]\n",
    "predictions_as_string[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb36aac-1eeb-4096-9bf6-afdd3f7bd0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
