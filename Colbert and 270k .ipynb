{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84ed421-c478-4163-9ac6-190defda2e72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train With Colbert Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312ab180-4c9b-4642-a6e7-0aca642a279b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "project_name = \"Science Exam - Kaggle\"\n",
    "run_name = \"ensemble_with_270_wiki\"\n",
    "\n",
    "config = {\n",
    "    'max_words': 100,\n",
    "    'nbits': 1,\n",
    "    'colbert_indexer_version': 6,\n",
    "    'hf_model_id': 'microsoft/deberta-v3-large',\n",
    "    'deberta_max_length': 512,\n",
    "    'large_train_deduped': True, # if this is True the individual ones below are skipped\n",
    "    'use_train_daniel': False,\n",
    "    'use_osmu_sci_6k': False,\n",
    "    'use_osmu_21k': False,\n",
    "    'use_mgoksu_13k': False,\n",
    "    'use_gigkpea_3k': False,\n",
    "    'n_extra_test_rows' : 300,\n",
    "    'lr_layer_factor': None, # set to an int to layer, None to not layer, need to be adapted to work with 8-bit Adam\n",
    "    'learning_rate': 1e-5,\n",
    "    'num_train_epochs': 1,\n",
    "    '8_bit_adam': True\n",
    "}\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'garbage_collection_threshold:0.6,max_split_size_mb:128'\n",
    "\n",
    "from transformers import TrainingArguments, IntervalStrategy\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path('./checkpoints')\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=config['learning_rate'],\n",
    "    num_train_epochs=config['num_train_epochs'],\n",
    "    # fp16=True,\n",
    "    # warmup_ratio=0.5,\n",
    "    weight_decay=0,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=12,\n",
    "    gradient_accumulation_steps=4, # number of batches to sum up gradient for\n",
    "    gradient_checkpointing=True,\n",
    "    # optim='adafactor',\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    save_strategy='no',\n",
    "    # save_steps=20000,\n",
    "    report_to='wandb',\n",
    "    output_dir=str(output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d1d025-bc05-459d-9265-e6188fa78ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3657bda4-1331-4604-a1a6-d6164caef4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import math\n",
    "import gc\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from scipy.special import softmax\n",
    "from functools import partial\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries\n",
    "from colbert import Indexer, Searcher\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer, \\\n",
    "                        get_linear_schedule_with_warmup\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers.utils.notebook import NotebookProgressCallback\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from datasets import Dataset # HuggingFace\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad42dc4-4e75-4be7-8520-f67737420379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e77bf4-c145-4e2d-8c61-69e002ebe21e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'54,425'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_extra_test = config['n_extra_test_rows']\n",
    "train_sets = []\n",
    "test_sets = {}\n",
    "\n",
    "test = pl.read_csv('data/train.csv')\n",
    "test = test.rename({'prompt': 'question'})\n",
    "test = test.drop(columns=\"id\")\n",
    "test_sets['test'] = test\n",
    "\n",
    "\n",
    "def read_training_files(file_paths, test_name):\n",
    "    if type(file_paths) == str:\n",
    "        file_paths = [file_paths]\n",
    "        \n",
    "    if file_paths[0].endswith('.parquet'):\n",
    "        train_raw = pl.concat(\n",
    "            [pl.read_parquet(file) for file in file_paths]\n",
    "        )\n",
    "    elif file_paths[0].endswith('.csv'):\n",
    "        train_raw = pl.concat(\n",
    "            [pl.read_csv(file) for file in file_paths]\n",
    "        )\n",
    "    \n",
    "    if 'prompt' in train_raw.columns:\n",
    "        train_raw = train_raw.rename({'prompt': 'question'})\n",
    "    train_cols = ['question', 'A', 'C', 'B', 'D', 'E', 'answer']\n",
    "    train_raw = train_raw[train_cols]\n",
    "    train_raw = train_raw.select(pl.all().fill_null('N/A'))\n",
    "    \n",
    "    test_split = train_raw[train_raw.shape[0] - n_extra_test:]\n",
    "    train = train_raw[:train_raw.shape[0] - n_extra_test]\n",
    "    \n",
    "    train_sets.append(train)\n",
    "    test_sets[test_name] = test_split\n",
    "\n",
    "if config['large_train_deduped']:\n",
    "    read_training_files('./data/large_train_deduped.parquet', 'combined')\n",
    "else:        \n",
    "    if config['use_train_daniel']:\n",
    "        read_training_files('./data/train_dedupe/daniel.parquet', 'daniel')\n",
    "\n",
    "    if config['use_osmu_sci_6k']:\n",
    "        read_training_files('./data/train_dedupe/osmu_sci_6k.parquet', 'osmu_sci_6k')\n",
    "    if config['use_osmu_21k']:\n",
    "        read_training_files([\n",
    "            './data/train_dedupe/osmu_15k.parquet',\n",
    "            './data/train_dedupe/osmu_5_9k.parquet'\n",
    "            ], 'osmu_21k')\n",
    "\n",
    "    if config['use_mgoksu_13k']:\n",
    "        read_training_files('./data/train_dedupe/mgoksu.parquet', 'mgoksu')\n",
    "\n",
    "    if config['use_gigkpea_3k']:\n",
    "        read_training_files('./data/train_dedupe/gigkpea.parquet', 'gigkpea')\n",
    "\n",
    "train = pl.concat(train_sets)\n",
    "f'{train.shape[0]:,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c5098b0-a471-46c4-aee2-0c6a23d3d112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    }
   ],
   "source": [
    "df_chunk_size=5000\n",
    "if is_local:\n",
    "    paraphs_parsed_dataset = load_from_disk(\"./data/wiki-270k\")\n",
    "else:\n",
    "    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n",
    "modified_texts_parsed = paraphs_parsed_dataset.map(lambda example:\n",
    "                                                     {'temp_text':\n",
    "                                                      f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n",
    "                                                     num_proc=2)[\"temp_text\"]\n",
    "\n",
    "if is_local:\n",
    "    cohere_dataset_filtered = load_from_disk(\"./data/stem-wiki-cohere\")\n",
    "else:\n",
    "    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n",
    "\n",
    "modified_texts = cohere_dataset_filtered.map(lambda example:\n",
    "                                         {'temp_text':\n",
    "                                          unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n",
    "                                         num_proc=2)[\"temp_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05a1600-d54f-4f2d-8b59-088c824085f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SplitList(mylist, chunk_size):\n",
    "    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n",
    "\n",
    "def get_relevant_documents_parsed(df_valid):\n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts_parsed)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"title\"],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"text\"],\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_documents(df_valid):\n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         cohere_dataset_filtered[idx.item()][\"title\"],\n",
    "                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def retrieval(df_valid, modified_texts):\n",
    "    \n",
    "    corpus_df_valid = df_valid.apply(lambda row:\n",
    "                                     f'{row[\"question\"]}\\n{row[\"question\"]}\\n{row[\"question\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n",
    "                                     axis=1).values\n",
    "    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words)\n",
    "    vectorizer1.fit(corpus_df_valid)\n",
    "    vocab_df_valid = vectorizer1.get_feature_names_out()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words,\n",
    "                                 vocabulary=vocab_df_valid)\n",
    "    vectorizer.fit(modified_texts[:500000])\n",
    "    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n",
    "    \n",
    "    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    chunk_size = 100\n",
    "    top_per_chunk = 3\n",
    "    top_per_query = 3\n",
    "\n",
    "    all_chunk_top_indices = []\n",
    "    all_chunk_top_values = []\n",
    "\n",
    "    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n",
    "        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n",
    "        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n",
    "        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n",
    "        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n",
    "\n",
    "        all_chunk_top_indices.append(chunk_top_indices + idx)\n",
    "        all_chunk_top_values.append(chunk_top_values)\n",
    "\n",
    "    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n",
    "    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n",
    "    \n",
    "    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n",
    "    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n",
    "    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n",
    "    \n",
    "    return articles_indices, merged_top_scores\n",
    "\n",
    "\n",
    "def prepare_answering_input(\n",
    "        tokenizer, \n",
    "        question,  \n",
    "        options,   \n",
    "        context,   \n",
    "        max_seq_length=4096,\n",
    "    ):\n",
    "    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n",
    "    c_plus_q_4 = [c_plus_q] * len(options)\n",
    "    tokenized_examples = tokenizer(\n",
    "        c_plus_q_4, options,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"longest\",\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n",
    "    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n",
    "    example_encoded = {\n",
    "        \"input_ids\": input_ids.to(model.device.index),\n",
    "        \"attention_mask\": attention_mask.to(model.device.index),\n",
    "    }\n",
    "    return example_encoded\n",
    "\n",
    "\n",
    "stop_words = ['each', 'you', 'the', 'use', 'used',\n",
    "                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n",
    "                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n",
    "                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n",
    "                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n",
    "                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n",
    "                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n",
    "                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n",
    "                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n",
    "                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n",
    "                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n",
    "                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n",
    "                  'did', 'theirs', 'can', 'those',\n",
    "                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n",
    "                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n",
    "                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n",
    "                  'yours', 'but', 'being', \"wasn't\", 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49852a01-c5bc-4865-8240-e3d32601639d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61ad09bb1334c8abf7d171552713285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 6286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c7b52f2a394276af0960c520f68b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 5933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd71d6807bf4e9186bc82179a030e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dd43afbcc643d788761cdbaa81ebef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 6118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61da878b89e4df5ba3b6e5cc783de33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 6245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7275d29ff9334535b30aeb267e41119e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 5971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ae62da9bcd4e2e903b99e797c2ed71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 6857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b31b772e1e14b639dd2cd3ca4e14449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 6528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cd279d301243afbe06ba15e9e2ebf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 5952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848bec3b8a2648cbbe5ab096551244f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1381: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 5928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90a8a9fa6674cf9bc7e38fc18d4c8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_sets[test_name]\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      3\u001b[0m test_wiki_tfidf_parsed \u001b[38;5;241m=\u001b[39m get_relevant_documents_parsed(test_data)\n\u001b[0;32m----> 4\u001b[0m train_wiki_ifidf_parsed \u001b[38;5;241m=\u001b[39m get_relevant_documents_parsed(train\u001b[38;5;241m.\u001b[39mto_pandas())\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mget_relevant_documents_parsed\u001b[0;34m(df_valid)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, df_valid\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], df_chunk_size)):\n\u001b[1;32m      8\u001b[0m     df_valid_ \u001b[38;5;241m=\u001b[39m df_valid\u001b[38;5;241m.\u001b[39miloc[idx: idx\u001b[38;5;241m+\u001b[39mdf_chunk_size]\n\u001b[0;32m---> 10\u001b[0m     articles_indices, merged_top_scores \u001b[38;5;241m=\u001b[39m retrieval(df_valid_, modified_texts_parsed)\n\u001b[1;32m     11\u001b[0m     all_articles_indices\u001b[38;5;241m.\u001b[39mappend(articles_indices)\n\u001b[1;32m     12\u001b[0m     all_articles_values\u001b[38;5;241m.\u001b[39mappend(merged_top_scores)\n",
      "Cell \u001b[0;32mIn[9], line 81\u001b[0m, in \u001b[0;36mretrieval\u001b[0;34m(df_valid, modified_texts)\u001b[0m\n\u001b[1;32m     78\u001b[0m all_chunk_top_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(modified_texts), chunk_size)):\n\u001b[0;32m---> 81\u001b[0m     wiki_vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(modified_texts[idx: idx\u001b[38;5;241m+\u001b[39mchunk_size])\n\u001b[1;32m     82\u001b[0m     temp_scores \u001b[38;5;241m=\u001b[39m (corpus_tf_idf \u001b[38;5;241m*\u001b[39m wiki_vectors\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m     83\u001b[0m     chunk_top_indices \u001b[38;5;241m=\u001b[39m temp_scores\u001b[38;5;241m.\u001b[39margpartition(\u001b[38;5;241m-\u001b[39mtop_per_chunk, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;241m-\u001b[39mtop_per_chunk:]\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2163\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \n\u001b[1;32m   2148\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2161\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2163\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1434\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1434\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, fixed_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1436\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:115\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc, stop_words)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc)\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:248\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[0;34m(self, tokens, stop_words)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# handle token n-grams\u001b[39;00m\n\u001b[1;32m    251\u001b[0m min_n, max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:248\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# handle token n-grams\u001b[39;00m\n\u001b[1;32m    251\u001b[0m min_n, max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_name = 'test'\n",
    "test_data = test_sets[test_name].to_pandas()\n",
    "test_wiki_tfidf_parsed = get_relevant_documents_parsed(test_data)\n",
    "train_wiki_ifidf_parsed = get_relevant_documents_parsed(train.to_pandas())\n",
    "# test_wiki_tfidf = get_relevant_documents(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21046c0f-71e6-4f6b-b9d0-cc7fd27cbf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_articles(example):\n",
    "    articles = example['retrieved_articles_parsed']\n",
    "    first_sentence = [f\"\"\"[CLS] {articles[-1]} [SEP] {articles[-2]} [SEP] {articles[-3]}\"\"\"] * 5\n",
    "    second_sentences = []\n",
    "    for option in 'ABCDE':\n",
    "        answer = example[option]\n",
    "        if answer is not None:\n",
    "            second_sentences.append(\" #### \" + example[\"question\"] + \" [SEP] \" + example[option] + \" [SEP]\")\n",
    "        else:\n",
    "            second_sentences.append('N/A')\n",
    "    try:\n",
    "        tokenized_example = tokenizer(first_sentence, second_sentences, truncation='longest_first', max_length=max_length)\n",
    "    except:\n",
    "        print(first_sentence, second_sentences)\n",
    "        raise\n",
    "    if 'answer' in example.keys():\n",
    "        tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "def tokenized_articles(data):\n",
    "    columns_to_keep = set(['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "    dataset = Dataset.from_pandas(data.to_pandas(), preserve_index=False)\n",
    "    col_to_remove = set(dataset.map(preprocess_articles).features.keys()) - columns_to_keep\n",
    "    tokenized = dataset.map(preprocess_articles, remove_columns=col_to_remove)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13cb4e-475c-471c-8845-78609b2376c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97a0a6-d4d7-4caa-bbdb-55aa56019dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer, TrainingArguments, IntervalStrategy, get_linear_schedule_with_warmup\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from datasets import Dataset # HuggingFace\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0912c-a359-4187-986f-90b5b15dd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_path = '/kaggle/input/deberta-for-colbert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2043a7-88bf-44be-b0e2-5c132bd870e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 450\n",
    "\n",
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "\n",
    "    \n",
    "    def __call__(self, input_batch):\n",
    "        # input_batch is list of samples, choices, tokens\n",
    "        additional_cols = set(input_batch[0].keys()) - set(['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "        if len(additional_cols) > 0:\n",
    "            print(f'{additional_cols=}')\n",
    "        \n",
    "        if 'label' in input_batch[0].keys():\n",
    "            label_name = 'label' \n",
    "            labels = [feature.pop(label_name) for feature in input_batch]\n",
    "        batch_size = len(input_batch)\n",
    "        num_choices = len(input_batch[0]['input_ids'])\n",
    "        flattened_input = [\n",
    "            [{k: v[i] for k, v in sample.items()} for i in range(num_choices)] for sample in input_batch\n",
    "        ]\n",
    "        flattened_input = sum(flattened_input, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_input,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # batch.shape = (n_samples, n_choices, n_tokens)\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        if 'label' in input_batch[0].keys():\n",
    "            batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        #print(np.array(batch['input_ids']).shape)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849ada6-1ab9-4739-96c8-fd7f0103541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_articles = tokenized_articles(train)\n",
    "tokenized_test_articles = tokenized_articles(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23829a0-7253-47bb-b75b-62a14b69c10d",
   "metadata": {},
   "source": [
    "## Train DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eea03f-76c1-45e7-b28c-d33ddb606ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(predictions, actuals, k=3):        \n",
    "    if isinstance(actuals, list):\n",
    "        actuals = np.array(actuals)\n",
    "        \n",
    "    found_at = np.where(predictions == actuals.reshape(-1, 1))\n",
    "    # found_at is a tuple with the array of found indices in the second position\n",
    "    score = 1 / (1 + found_at[1])\n",
    "    score[score < 1/k] = 0\n",
    "    return score\n",
    "\n",
    "def mean_avg_precision_at_k(predictions, actual, k=3):\n",
    "    n = predictions.shape[0]\n",
    "    row_precision = precision_at_k(predictions, actual)\n",
    "    return row_precision.sum()/n\n",
    "\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.flip(predictions.argsort(axis=1), axis=1)\n",
    "    accuracy = acc_metric.compute(predictions=predictions[:,0], references=labels)['accuracy']\n",
    "    map_at_3 = mean_avg_precision_at_k(predictions, labels)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'map_at_3': round(map_at_3, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b73d0e-c413-4cfa-8a38-c4482e00feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain = True\n",
    "\n",
    "if not output_path.exists() or retrain:\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        job_type='train',\n",
    "        config=config\n",
    "        # group=\"bert\"\n",
    "    )\n",
    "       \n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(config['hf_model_id'])\n",
    "\n",
    "    total_steps = len(tokenized_train) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) \n",
    "    \n",
    "    if config['8_bit_adam']:\n",
    "        decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "        decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "                \"weight_decay\": training_args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer_kwargs = {\n",
    "            \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
    "            \"eps\": training_args.adam_epsilon,\n",
    "        }\n",
    "        optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
    "        optimizer = bnb.optim.Adam8bit(\n",
    "            optimizer_grouped_parameters,\n",
    "            betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "            eps=training_args.adam_epsilon,\n",
    "            lr=training_args.learning_rate,\n",
    "        )\n",
    "        max_lr=training_args.learning_rate\n",
    "    elif type(config['lr_layer_factor']) == int:\n",
    "        factor = config['lr_layer_factor']\n",
    "        base_lr = training_args.learning_rate\n",
    "        \n",
    "        embedding_lr = base_lr / factor**4\n",
    "        early_layers_lr = base_lr / factor**3\n",
    "        middle_layers_lr = base_lr / factor**2\n",
    "        late_layers_lr = base_lr / factor\n",
    "        classifier_lr = base_lr\n",
    "\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': model.deberta.embeddings.parameters()},\n",
    "            {'params': model.deberta.encoder.layer[:8].parameters()},\n",
    "            {'params': model.deberta.encoder.layer[8:16].parameters()},\n",
    "            {'params': model.deberta.encoder.layer[16:].parameters()},\n",
    "            {'params': model.classifier.parameters()},\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          weight_decay=training_args.weight_decay)\n",
    "        max_lr = [base_lr / config['lr_layer_factor']**i for i in range(5,0,-1)]\n",
    "        \n",
    "    else:\n",
    "        optimizer = AdamW(model.parameters(),\n",
    "                          lr=training_args.learning_rate,\n",
    "                          weight_decay=training_args.weight_decay)\n",
    "        max_lr=training_args.learning_rate\n",
    "    \n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps)\n",
    "      \n",
    "    #warmup_steps = int(total_steps * training_args.warmup_ratio)\n",
    "    #scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer, max_length=600),\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=eval_datasets,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, scheduler)\n",
    "    )\n",
    "\n",
    "    # needed when there are multiple eval datasets\n",
    "    trainer.remove_callback(NotebookProgressCallback)\n",
    "    trainer.train()\n",
    "    # wandb.config.update(config)\n",
    "    wandb.finish()\n",
    "    trainer.save_model(output_path/run_name)\n",
    "else:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(output_path/run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b66f3e-4c52-4475-ae85-2347d00ef3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a04db-687f-4513-9510-55a76944a33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a2712-9635-41fd-b34d-40971359a32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9bf457-da71-419f-a218-025a2456cc36",
   "metadata": {},
   "source": [
    "## Predict with DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5a88e-13fa-4211-a61e-2c0ddc3be99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(fine_tuned_path)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer, max_length=600),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7c28f-dc9b-4086-b010-f349abf49768",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits = trainer.predict(tokenized_test).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7963ea1-a0b9-48e8-bee6-4ac4cbec647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article_logits = trainer.predict(tokenized_tokenized_articles).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc68d9-72cc-4cf9-8fb8-cb81c5b207b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36408fa-f108-48f1-a0ed-a30f7074e85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9655ac5-496f-453f-a141-0148673136a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
