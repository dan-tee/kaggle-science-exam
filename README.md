# kaggle-science-exam [link](https://www.kaggle.com/competitions/kaggle-llm-science-exam)

## Challenge
> Inspired by the OpenBookQA dataset, this competition challenges participants to answer difficult science-based questions written by a Large Language Model. ... Right now we estimate that the largest models run on Kaggle are around 10 billion parameters, whereas gpt3.5 clocks in at 175 billion parameters. If a question-answering model can ace a test written by a question-writing model more than 10 times its size, this would be a genuinely interesting result.

The challenge did not include training data, so part of the challenge was to generate a good training dataset.


## My Approach
As the questions in this competition are LLM generated based on Wikipedia articles, retrival augmented generation (RAG) is a shortcut to answering them. Find the article the questions is based on and use it as context for the model answering the question. I've decided to use [ColBERT](https://github.com/stanford-futuredata/ColBERT) for as retriever. ColBERT creates a seperate embedding vector for each word in a sentence, thus being able to retain a lot of sematic structure. ColBERT matches a query to a document via a MaxSim based operator by matching each token from the query to the token in a document that has the highest cosine similarty in the embedding space to the query token and suming this over all query tokens.

ColBERT creates relatively large embeddings with the advantage of perserving sematics well. This backfired during the competition, as it was quite hard to make the retrival from the large ColBERT index work in the limited Kaggle submission environment.

I've been using two approaches for the ColBERT search. Once I used a question-answer pair as query against all Science articles in Wikipida as documents, I secondly just the question against the articles. Both top matches would then be added as context to the question answer pairs to be fed into the downstream model.

As downstream model I used a DeBERTa V3 and trained it on a binary classifiation task for indidual question-answer pairs. I've created synthetic questions via the GPT-3.5 API and used some of the synthetic questions other users had uploaded as Kaggle Datasets.

## Differences to the winning solution
The winning solution also used a RAG approach and achieved an Mean Average Precision @ 3 (MAP@3) of 0.93, while my solution only achieved a MAP@3 of 0.88.

- The winning solution used larger LLMs than DeBERTa, a mix of Llama-2-7b, Mistral-7B-v0.1, xgen-7b-8k-base, Llama-2-13b.
- The winning solution used [SentenceTranformers](https://www.sbert.net/) e5-base-v2, e5-large-v2, gte-base, gte-large and bge-large to create a single embedding vector per query and document. This creates more compact embeddings and allowed them to embed all of Wikipedia, not just the science articles. It seems like quantity was more imporatant tha qualty for the context embedding.
- They trained their models with binary classification heads on individual quesiton answer pairs, same as in my approach, but they had an added trick: They averaged next token logits of all other options as an additional input to the final classification head. So their final classification head gets logits for all possible tokens for the answer at hand, and the average of all logits for all possible tokens of all the other four answers.

## Overview over the repo
- [Train With Colbert Q & A.ipynb](https://github.com/dan-tee/kaggle-science-exam/blob/main/Train%20Wtih%20Colbert%20Q%26A.ipynb) My DeBERTa v3 model trained on question-answer pairs with context retrieved from Wikipedia articles via ColBERT.
- [Wiki Indexing with ColBERT.ipynb](https://github.com/dan-tee/kaggle-science-exam/blob/main/Wiki%20Indexing%20with%20ColBERT.ipynb) Uses the dump of science based Wikipedia articles and runs the ColBERT model to create an index for ColBERT retriever.
- [Wikipedia STEM Pages.ipynb](https://github.com/dan-tee/kaggle-science-exam/blob/main/Wikipedia%20STEM%20Pages.ipynb) has my custom crawling of science questions from Wikipedia, plus cusom parsing of articles. There were some questions involving formulas that standard Wiki parsers would drop from the text. In the end I combined my science pages with another set of Wikipages uploaded by another Kaggle user generated by using a simple classifier to filter for science pages.
- [DeBERTa V3 Osmulski.ipynb](https://github.com/dan-tee/kaggle-science-exam/blob/main/DeBERTa%20V3%20Osmulski.ipynb) this is a simple baseline model I've created training a DeBERTa model on the science questions uploaded by fellow Kaggler Radek Osmulski.
- [Generate Questions With GPT 3.5.ipynb](https://github.com/dan-tee/kaggle-science-exam/blob/main/Generate%20Questions%20With%20GPT-3.5.ipynb) Uses the GPT-3.5 API to generate science questions based on Wikipedia snippets.
