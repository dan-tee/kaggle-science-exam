{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec1e2d8-3265-4c14-99c1-7bf8e6a56512",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wikipedia Search - ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79a20bc5-89d2-41cf-a9c3-ad6c2569b4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Indexer, Searcher\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "# from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "# import transformers\n",
    "# import torch\n",
    "# import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ded6d6-ef12-4fde-9050-1da3dd0ec8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.Config(fmt_str_lengths=2000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2e8160-d42c-4986-9b2c-c09a6f7374cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3b5c92-cf1b-4dc4-bb78-acaf24655132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73c0300-dbbf-463a-868b-ef27095970fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda list | grep bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc30dded-8f0c-4399-a124-5b099d2f65fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge pyarrow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d06b87-6da5-4e7d-b6f5-510d3e5f95c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda update -c conda-forge 'auto-gptq[triton]' -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c174f049-e7d5-4c18-ba76-abab2e6ff4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# huggingface_hub.login(os.environ['HUGGING_FACE_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a5363a-7ae1-4ac9-ad64-2e4fd2093900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['prompt', 'A', 'B', 'C', 'D', 'E', 'answer']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pl.read_csv('data/train.csv')\n",
    "df_test = df_test.drop(columns=\"id\")\n",
    "print(f'{df_test.shape[0]:,}')\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03bbd53d-40b4-4876-b61a-337b302ae46a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153750, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sections = pl.read_parquet('./data/wiki_with_category.parquet')\n",
    "wiki_sections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65e7e7a-799b-4a16-af94-225834d64a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = [doc.split(\" \") for doc in wiki_sections['section_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab69587-428b-4c7f-8342-5ed8a92b3cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b24d8c-6305-4165-9fe0-000d3cca4aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e04ac92b-b9b5-43eb-a012-b10729d9ba33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = df_test['prompt'][4]\n",
    "tokenized_query = \"Diffracting object dimensions affect diffraction pattern features' angular spacing\".split(\" \")\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2aeea30-f37e-418a-bdca-bd6323943a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bm25_scores(query):\n",
    "    tokenized_query = query.split(\" \")\n",
    "    scores = pd.Series(bm25.get_scores(tokenized_query))\n",
    "    scores = scores.sort_values(ascending=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b52f8301-c58c-4bc8-9205-6992413725c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c273f185644165b9a2fbce1cf9d7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "for question in tqdm(df_test['prompt'][:10]):\n",
    "    q_scores = pd.Series(bm25.get_scores(question))\n",
    "    scores.append(q_scores.sort_values(ascending=False)[:10].index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1594fabf-3cd9-470b-9665-9506772baa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (100, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>qid</th><th>prompt</th><th>bm25</th><th>bm25_idx</th></tr><tr><td>u32</td><td>str</td><td>u32</td><td>i32</td></tr></thead><tbody><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>4767</td><td>1</td></tr><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>141812</td><td>2</td></tr><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>138900</td><td>3</td></tr><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>124632</td><td>4</td></tr><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>141815</td><td>5</td></tr><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>146258</td><td>6</td></tr><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>129281</td><td>7</td></tr><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>141811</td><td>8</td></tr><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>52568</td><td>9</td></tr><tr><td>0</td><td>&quot;Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed &quot;missing baryonic mass&quot; discrepancy in galaxy clusters?&quot;</td><td>97959</td><td>10</td></tr><tr><td>1</td><td>&quot;Which of the following is an accurate definition of dynamic scaling in self-similar systems?&quot;</td><td>141812</td><td>1</td></tr><tr><td>1</td><td>&quot;Which of the following is an accurate definition of dynamic scaling in self-similar systems?&quot;</td><td>4767</td><td>2</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>8</td><td>&quot;What is the term used in astrophysics to describe light-matter interactions resulting in energy shifts in the radiation field?&quot;</td><td>97959</td><td>9</td></tr><tr><td>8</td><td>&quot;What is the term used in astrophysics to describe light-matter interactions resulting in energy shifts in the radiation field?&quot;</td><td>138900</td><td>10</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>4767</td><td>1</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>138900</td><td>2</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>141812</td><td>3</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>52568</td><td>4</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>124632</td><td>5</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>141815</td><td>6</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>97959</td><td>7</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>141811</td><td>8</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>97136</td><td>9</td></tr><tr><td>9</td><td>&quot;What is the role of axioms in a formal theory?&quot;</td><td>97595</td><td>10</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (100, 4)\n",
       "┌─────┬────────────────────────────────────────────────────────────────────────┬────────┬──────────┐\n",
       "│ qid ┆ prompt                                                                 ┆ bm25   ┆ bm25_idx │\n",
       "│ --- ┆ ---                                                                    ┆ ---    ┆ ---      │\n",
       "│ u32 ┆ str                                                                    ┆ u32    ┆ i32      │\n",
       "╞═════╪════════════════════════════════════════════════════════════════════════╪════════╪══════════╡\n",
       "│ 0   ┆ Which of the following statements accurately describes the impact of   ┆ 4767   ┆ 1        │\n",
       "│     ┆ Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic   ┆        ┆          │\n",
       "│     ┆ mass\" discrepancy in galaxy clusters?                                  ┆        ┆          │\n",
       "│ 0   ┆ Which of the following statements accurately describes the impact of   ┆ 141812 ┆ 2        │\n",
       "│     ┆ Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic   ┆        ┆          │\n",
       "│     ┆ mass\" discrepancy in galaxy clusters?                                  ┆        ┆          │\n",
       "│ 0   ┆ Which of the following statements accurately describes the impact of   ┆ 138900 ┆ 3        │\n",
       "│     ┆ Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic   ┆        ┆          │\n",
       "│     ┆ mass\" discrepancy in galaxy clusters?                                  ┆        ┆          │\n",
       "│ 0   ┆ Which of the following statements accurately describes the impact of   ┆ 124632 ┆ 4        │\n",
       "│     ┆ Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic   ┆        ┆          │\n",
       "│     ┆ mass\" discrepancy in galaxy clusters?                                  ┆        ┆          │\n",
       "│ …   ┆ …                                                                      ┆ …      ┆ …        │\n",
       "│ 9   ┆ What is the role of axioms in a formal theory?                         ┆ 97959  ┆ 7        │\n",
       "│ 9   ┆ What is the role of axioms in a formal theory?                         ┆ 141811 ┆ 8        │\n",
       "│ 9   ┆ What is the role of axioms in a formal theory?                         ┆ 97136  ┆ 9        │\n",
       "│ 9   ┆ What is the role of axioms in a formal theory?                         ┆ 97595  ┆ 10       │\n",
       "└─────┴────────────────────────────────────────────────────────────────────────┴────────┴──────────┘"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25 = pl.Series(\"bm25\", scores, dtype=pl.List(pl.UInt32))\n",
    "info = df_test[:10][['prompt']].with_columns(bm25)\n",
    "info = info.with_row_count('qid')\n",
    "info = info.explode('bm25')\n",
    "info.with_columns(pl.lit(1).alias(\"ones\"))\\\n",
    "    .select([\n",
    "        pl.all().exclude(\"ones\"),\n",
    "        pl.col(\"ones\").cumsum().over(\"qid\").flatten().alias(\"bm25_idx\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64da4776-f313-4ecb-ac76-ecd246d59a27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************\n",
      "12381 40.986316880422116\n",
      "[['Diffraction' 'Patterns'\n",
      "  \"File:Diffraction on elliptic aperture with fft.png\\nSeveral qualitative observations can be made of diffraction in general:\\n The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: The smaller the diffracting object, the 'wider' the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.)\\n The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object.\\n When the diffracting object has a periodic structure, for example in a diffraction grating, the features generally become sharper. The third figure, for example, shows a comparison of a Double-slit experiment pattern with a pattern formed by five slits, both sets of slits having the same spacing, between the center of one slit and the next.\"]]\n"
     ]
    }
   ],
   "source": [
    "for item in scores[:1].items():\n",
    "    print('*************')\n",
    "    print(item[0], item[1])\n",
    "    print(wiki_sections[int(item[0])][['title', 'section_title', 'section_text']].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85203c40-d9f7-4f26-a72e-af6f195449fb",
   "metadata": {},
   "source": [
    "## Using ColBERT to find Wiki Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fb9d27d-7df5-41e1-bbd3-0573084db962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((208767, 3), ['chunk_id', 'section_id', 'section_text'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_text_into_chunks(text, max_words=350):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    last_period = -1\n",
    "    section_words = text.split()\n",
    "\n",
    "    for word in section_words:\n",
    "        if word.endswith('.'):\n",
    "            last_period = current_length\n",
    "\n",
    "        if current_length < max_words:\n",
    "            current_length += 1\n",
    "        else:\n",
    "            if last_period > -1:\n",
    "                cut_point = last_period + 1\n",
    "                chunks.append(\" \".join(section_words[:cut_point]))\n",
    "                section_words = section_words[cut_point:]\n",
    "                last_period = -1\n",
    "                current_length = 0\n",
    "            else:\n",
    "                # If no period exists, just split it at max_words\n",
    "                chunks.append(\" \".join(section_words[:current_length]))\n",
    "                section_words = section_words[current_length:]\n",
    "                current_length = 0           \n",
    "\n",
    "    if section_words:\n",
    "        chunks.append(\" \".join(section_words))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "passages = wiki_sections.with_row_count('section_id')[['section_id', 'section_text']]\n",
    "passages = passages.with_columns(\n",
    "    pl.col(\"section_text\")\n",
    "      .apply(split_text_into_chunks)\\\n",
    "      .cast(pl.List(pl.Utf8))\\\n",
    ")\n",
    "passages = passages.explode('section_text')\n",
    "passages = passages.with_row_count('chunk_id')\n",
    "passages.shape, passages.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ef46270-5875-48fb-9fe2-29bf3d793b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = passages.with_columns(pl.col('section_text').str.replace_all('\\n', ' '))                      \n",
    "passages_file = './data/wiki_passages.tsv'\n",
    "passages[['chunk_id', 'section_text']].write_csv(passages_file, separator='\\t', has_header=False)\n",
    "queries = df_test.with_row_count('qid')[['qid', 'prompt']]\n",
    "queries = queries.with_columns(pl.col('prompt').str.replace_all('\\n', ' ')) \n",
    "queries_file = './data/wiki_queries.tsv'\n",
    "queries.write_csv(queries_file, separator='\\t', has_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6bb2981-b1cf-446d-b066-116f657dda94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:06:08] #> Loading collection...\n",
      "0M \n",
      "[Aug 26, 17:06:09] #> Loading the queries from ./data/wiki_queries.tsv ...\n",
      "[Aug 26, 17:06:09] #> Got 200 queries. All QIDs are unique.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 200 queries and 208,767 passages'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_collection = Collection(passages_file)\n",
    "c_queries = Queries(queries_file)\n",
    "f'Loaded {len(c_queries)} queries and {len(c_collection):,} passages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db6f01a8-acbc-42b1-b7f6-f8f0c0d68046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 512   # truncate passages at 300 tokens\n",
    "\n",
    "index_folder = './data/colbert'\n",
    "checkpoint = './checkpoints/colbertv2.0'\n",
    "experiment = 'wiki-science'\n",
    "indexer_name = f\"wiki_pages_index_{nbits}bits\"\n",
    "\n",
    "os.environ['COLBERT_LOAD_TORCH_EXTENSION_VERBOSE'] = 'True'\n",
    "\n",
    "config = ColBERTConfig(\n",
    "    index_path=index_folder,\n",
    "    doc_maxlen=doc_maxlen,\n",
    "    nbits=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b14b8e2-8820-4591-8436-be1a5694e432",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Aug 26, 17:06:09] #> Note: Output directory ./data/colbert already exists\n",
      "\n",
      "\n",
      "[Aug 26, 17:06:09] #> Will delete 28 files already at ./data/colbert in 20 seconds...\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": \".\\/data\\/colbert\",\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 20000,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 64,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 512,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \".\\/checkpoints\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
      "    \"collection\": \".\\/data\\/wiki_passages.tsv\",\n",
      "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
      "    \"index_name\": \"wiki_pages_index_2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/daniel\\/code\\/kaggle-science-exam\\/experiments\",\n",
      "    \"experiment\": \"wiki-science\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-08\\/26\\/17.05.54\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1\n",
      "}\n",
      "[Aug 26, 17:06:34] [0] \t\t # of sampled PIDs = 80084 \t sampled_pids[:3] = [109214, 192069, 2665]\n",
      "[Aug 26, 17:06:34] [0] \t\t #> Encoding 80084 passages..\n",
      "[Aug 26, 17:11:41] [0] \t\t avg_doclen_est = 245.12925720214844 \t len(local_sample) = 80,084\n",
      "[Aug 26, 17:11:43] [0] \t\t Creaing 65,536 partitions.\n",
      "[Aug 26, 17:11:43] [0] \t\t *Estimated* 51,174,899 embeddings.\n",
      "[Aug 26, 17:11:43] [0] \t\t #> Saving the indexing plan to ./data/colbert/plan.json ..\n",
      "Sampling a subset of 16777216 / 19580932 for training\n",
      "Clustering 16777216 points in 128D to 65536 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 3.50 s\n",
      "  Iteration 19 (278.39 s, search 269.26 s): objective=3.62456e+06 imbalance=1.269 nsplit=0       \n",
      "[Aug 26, 17:16:29] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/daniel/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/daniel/.cache/torch_extensions/py38_cu117/decompress_residuals_cpp/build.ninja...\n",
      "Building extension module decompress_residuals_cpp...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "[Aug 26, 17:16:29] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module decompress_residuals_cpp...\n",
      "Using /home/daniel/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/daniel/.cache/torch_extensions/py38_cu117/packbits_cpp/build.ninja...\n",
      "Building extension module packbits_cpp...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "[0.033, 0.034, 0.033, 0.03, 0.032, 0.033, 0.033, 0.03, 0.031, 0.032, 0.031, 0.031, 0.032, 0.033, 0.031, 0.034, 0.029, 0.031, 0.032, 0.031, 0.032, 0.033, 0.032, 0.033, 0.031, 0.032, 0.033, 0.033, 0.032, 0.034, 0.032, 0.036, 0.033, 0.03, 0.032, 0.03, 0.035, 0.031, 0.031, 0.039, 0.034, 0.032, 0.032, 0.034, 0.031, 0.031, 0.032, 0.035, 0.033, 0.031, 0.032, 0.032, 0.033, 0.033, 0.032, 0.032, 0.038, 0.033, 0.039, 0.031, 0.032, 0.034, 0.033, 0.035, 0.033, 0.033, 0.036, 0.034, 0.032, 0.032, 0.035, 0.031, 0.032, 0.033, 0.032, 0.034, 0.035, 0.034, 0.034, 0.035, 0.034, 0.031, 0.034, 0.034, 0.031, 0.033, 0.031, 0.034, 0.031, 0.035, 0.033, 0.035, 0.032, 0.034, 0.032, 0.033, 0.035, 0.031, 0.032, 0.032, 0.032, 0.036, 0.034, 0.032, 0.035, 0.03, 0.031, 0.031, 0.033, 0.032, 0.034, 0.033, 0.034, 0.03, 0.036, 0.031, 0.035, 0.033, 0.031, 0.034, 0.031, 0.032, 0.034, 0.035, 0.03, 0.036, 0.032, 0.032]\n",
      "[Aug 26, 17:16:30] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Aug 26, 17:16:30] #> Got bucket_cutoffs = tensor([-2.6367e-02,  6.1035e-05,  2.6474e-02], device='cuda:0') and bucket_weights = tensor([-0.0464, -0.0123,  0.0124,  0.0466], device='cuda:0')\n",
      "[Aug 26, 17:16:30] avg_residual = 0.03277587890625\n",
      "[Aug 26, 17:16:30] [0] \t\t #> Encoding 25000 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module packbits_cpp...\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:17:30] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 6,095,873 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:02, 62.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:17:32] [0] \t\t #> Encoding 25000 passages..\n",
      "[Aug 26, 17:18:32] [0] \t\t #> Saving chunk 1: \t 25,000 passages and 6,158,019 embeddings. From #25,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:04, 62.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:18:34] [0] \t\t #> Encoding 25000 passages..\n",
      "[Aug 26, 17:19:34] [0] \t\t #> Saving chunk 2: \t 25,000 passages and 6,205,251 embeddings. From #50,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [03:06, 62.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:19:36] [0] \t\t #> Encoding 25000 passages..\n",
      "[Aug 26, 17:20:36] [0] \t\t #> Saving chunk 3: \t 25,000 passages and 6,522,781 embeddings. From #75,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [04:08, 62.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:20:39] [0] \t\t #> Encoding 25000 passages..\n",
      "[Aug 26, 17:21:39] [0] \t\t #> Saving chunk 4: \t 25,000 passages and 6,512,556 embeddings. From #100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [05:11, 62.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:21:41] [0] \t\t #> Encoding 25000 passages..\n",
      "[Aug 26, 17:22:41] [0] \t\t #> Saving chunk 5: \t 25,000 passages and 5,919,801 embeddings. From #125,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [06:13, 62.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:22:43] [0] \t\t #> Encoding 25000 passages..\n",
      "[Aug 26, 17:23:43] [0] \t\t #> Saving chunk 6: \t 25,000 passages and 5,705,500 embeddings. From #150,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [07:15, 62.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:23:45] [0] \t\t #> Encoding 25000 passages..\n",
      "[Aug 26, 17:24:45] [0] \t\t #> Saving chunk 7: \t 25,000 passages and 5,825,870 embeddings. From #175,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [08:17, 62.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:24:47] [0] \t\t #> Encoding 8767 passages..\n",
      "[Aug 26, 17:25:08] [0] \t\t #> Saving chunk 8: \t 8,767 passages and 1,996,656 embeddings. From #200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [08:39, 57.67s/it]\n",
      "100%|██████████| 9/9 [00:00<00:00, 184.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:25:09] [0] \t\t #> Checking all files were saved...\n",
      "[Aug 26, 17:25:09] [0] \t\t Found all files!\n",
      "[Aug 26, 17:25:09] [0] \t\t #> Building IVF...\n",
      "[Aug 26, 17:25:09] [0] \t\t #> Loading codes...\n",
      "[Aug 26, 17:25:09] [0] \t\t Sorting codes...\n",
      "[Aug 26, 17:25:11] [0] \t\t Getting unique codes...\n",
      "[Aug 26, 17:25:11] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Aug 26, 17:25:11] #> Building the emb2pid mapping..\n",
      "[Aug 26, 17:25:12] len(emb2pid) = 50942307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 65536/65536 [00:02<00:00, 30637.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 17:25:14] #> Saved optimized IVF to ./data/colbert/ivf.pid.pt\n",
      "[Aug 26, 17:25:14] [0] \t\t #> Saving the indexing metadata to ./data/colbert/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(nranks=1, experiment=experiment)):\n",
    "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "    indexer.index(name=indexer_name,\n",
    "                  collection=c_collection,\n",
    "                  overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6e31c-12d0-4872-acda-cc0b0424d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ColBERTConfig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a799e41-278d-4fe7-a307-68b8c4323927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 18:05:43] #> Loading collection...\n",
      "0M \n",
      "[Aug 26, 18:05:44] #> Loading codec...\n",
      "[Aug 26, 18:05:44] #> Loading IVF...\n",
      "[Aug 26, 18:05:44] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 2311.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 18:05:44] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 9/9 [00:00<00:00, 15.40it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 289.20it/s]\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(nranks=1, experiment=experiment)):\n",
    "    config = ColBERTConfig(\n",
    "            root=index_folder,\n",
    "        )\n",
    "    searcher = Searcher(index=indexer_name, config=config)\n",
    "    ranking = searcher.search_all(c_queries, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08e29ae5-d842-427c-a991-822cfa516a51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16871, 1, 20.5),\n",
       " (17943, 2, 18.4375),\n",
       " (2943, 3, 18.03125),\n",
       " (26938, 4, 17.359375),\n",
       " (55803, 5, 17.21875),\n",
       " (16896, 6, 17.203125),\n",
       " (1827, 7, 17.140625),\n",
       " (16894, 8, 17.078125),\n",
       " (85348, 9, 16.953125),\n",
       " (26933, 10, 16.859375)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.with_columns(pl.lit(1).alias(\"ones\"))\\\n",
    "    .select([\n",
    "        pl.all().exclude(\"ones\"),\n",
    "        pl.col(\"ones\").cumsum().over(\"qid\").flatten().alias(\"bm25_idx\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c058c245-892c-4d2e-ad01-038bd330790a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colbert = [tup[1] for tup in ranking.flat_ranking]\n",
    "colbert = pl.Series(\"colbert\", colbert[:100], dtype=pl.UInt32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eaf7a5be-d194-4a9a-a635-70dfc42cd415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'with_colums'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_colums\u001b[49m(colbert)\n",
      "File \u001b[0;32m~/anaconda3/envs/colbert/lib/python3.8/site-packages/polars/utils/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecate_renamed_methods.<locals>._redirecting_getattr_\u001b[0;34m(obj, item)\u001b[0m\n\u001b[1;32m    161\u001b[0m     issue_deprecation_warning(\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is deprecated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It has been renamed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_item_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    164\u001b[0m         version\u001b[38;5;241m=\u001b[39mversions[item],\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m     item \u001b[38;5;241m=\u001b[39m new_item_name\n\u001b[0;32m--> 168\u001b[0m attr \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_item, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    170\u001b[0m     attr \u001b[38;5;241m=\u001b[39m partial(attr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_item[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'with_colums'"
     ]
    }
   ],
   "source": [
    "info.with_colums(colbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823b16e-397a-45a0-aecf-0289421ccfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pl.Series(\"bm25\", scores, dtype=pl.List(pl.UInt32))\n",
    "info = df_test[:10][['prompt']].with_columns(bm25)\n",
    "info = info.with_row_count('qid')\n",
    "info = info.explode('bm25')\n",
    "info.with_columns(pl.lit(1).alias(\"ones\"))\\\n",
    "    .select([\n",
    "        pl.all().exclude(\"ones\"),\n",
    "        pl.col(\"ones\").cumsum().over(\"qid\").flatten().alias(\"bm25_idx\")\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:colbert]",
   "language": "python",
   "name": "conda-env-colbert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
