{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84ed421-c478-4163-9ac6-190defda2e72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train With Colbert Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312ab180-4c9b-4642-a6e7-0aca642a279b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "project_name = \"Science Exam - Kaggle\"\n",
    "run_name = \"ensemble_with_270_wiki\"\n",
    "\n",
    "config = {\n",
    "    'max_words': 100,\n",
    "    'nbits': 1,\n",
    "    'colbert_indexer_version': 6,\n",
    "    'hf_model_id': 'microsoft/deberta-v3-large',\n",
    "    'deberta_max_length': 512,\n",
    "    'large_train_deduped': True, # if this is True the individual ones below are skipped\n",
    "    'use_train_daniel': False,\n",
    "    'use_osmu_sci_6k': False,\n",
    "    'use_osmu_21k': False,\n",
    "    'use_mgoksu_13k': False,\n",
    "    'use_gigkpea_3k': False,\n",
    "    'n_extra_test_rows' : 300,\n",
    "    'lr_layer_factor': None, # set to an int to layer, None to not layer, need to be adapted to work with 8-bit Adam\n",
    "    'learning_rate': 1e-5,\n",
    "    'num_train_epochs': 1,\n",
    "    '8_bit_adam': True\n",
    "}\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'garbage_collection_threshold:0.6,max_split_size_mb:128'\n",
    "\n",
    "from transformers import TrainingArguments, IntervalStrategy\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path('./checkpoints')\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=config['learning_rate'],\n",
    "    num_train_epochs=config['num_train_epochs'],\n",
    "    # fp16=True,\n",
    "    # warmup_ratio=0.5,\n",
    "    weight_decay=0,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=12,\n",
    "    gradient_accumulation_steps=4, # number of batches to sum up gradient for\n",
    "    gradient_checkpointing=True,\n",
    "    # optim='adafactor',\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    save_strategy='no',\n",
    "    # save_steps=20000,\n",
    "    report_to='wandb',\n",
    "    output_dir=str(output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d1d025-bc05-459d-9265-e6188fa78ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3657bda4-1331-4604-a1a6-d6164caef4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import math\n",
    "import gc\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from scipy.special import softmax\n",
    "from functools import partial\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries\n",
    "from colbert import Indexer, Searcher\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer, \\\n",
    "                        get_linear_schedule_with_warmup\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers.utils.notebook import NotebookProgressCallback\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from datasets import Dataset # HuggingFace\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad42dc4-4e75-4be7-8520-f67737420379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e77bf4-c145-4e2d-8c61-69e002ebe21e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'54,425'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_extra_test = config['n_extra_test_rows']\n",
    "train_sets = []\n",
    "test_sets = {}\n",
    "\n",
    "test = pl.read_csv('data/train.csv')\n",
    "test = test.rename({'prompt': 'question'})\n",
    "test = test.drop(columns=\"id\")\n",
    "test_sets['test'] = test\n",
    "\n",
    "\n",
    "def read_training_files(file_paths, test_name):\n",
    "    if type(file_paths) == str:\n",
    "        file_paths = [file_paths]\n",
    "        \n",
    "    if file_paths[0].endswith('.parquet'):\n",
    "        train_raw = pl.concat(\n",
    "            [pl.read_parquet(file) for file in file_paths]\n",
    "        )\n",
    "    elif file_paths[0].endswith('.csv'):\n",
    "        train_raw = pl.concat(\n",
    "            [pl.read_csv(file) for file in file_paths]\n",
    "        )\n",
    "    \n",
    "    if 'prompt' in train_raw.columns:\n",
    "        train_raw = train_raw.rename({'prompt': 'question'})\n",
    "    train_cols = ['question', 'A', 'C', 'B', 'D', 'E', 'answer']\n",
    "    train_raw = train_raw[train_cols]\n",
    "    train_raw = train_raw.select(pl.all().fill_null('N/A'))\n",
    "    \n",
    "    test_split = train_raw[train_raw.shape[0] - n_extra_test:]\n",
    "    train = train_raw[:train_raw.shape[0] - n_extra_test]\n",
    "    \n",
    "    train_sets.append(train)\n",
    "    test_sets[test_name] = test_split\n",
    "\n",
    "if config['large_train_deduped']:\n",
    "    read_training_files('./data/large_train_deduped.parquet', 'combined')\n",
    "else:        \n",
    "    if config['use_train_daniel']:\n",
    "        read_training_files('./data/train_dedupe/daniel.parquet', 'daniel')\n",
    "\n",
    "    if config['use_osmu_sci_6k']:\n",
    "        read_training_files('./data/train_dedupe/osmu_sci_6k.parquet', 'osmu_sci_6k')\n",
    "    if config['use_osmu_21k']:\n",
    "        read_training_files([\n",
    "            './data/train_dedupe/osmu_15k.parquet',\n",
    "            './data/train_dedupe/osmu_5_9k.parquet'\n",
    "            ], 'osmu_21k')\n",
    "\n",
    "    if config['use_mgoksu_13k']:\n",
    "        read_training_files('./data/train_dedupe/mgoksu.parquet', 'mgoksu')\n",
    "\n",
    "    if config['use_gigkpea_3k']:\n",
    "        read_training_files('./data/train_dedupe/gigkpea.parquet', 'gigkpea')\n",
    "\n",
    "train = pl.concat(train_sets)\n",
    "f'{train.shape[0]:,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c5098b0-a471-46c4-aee2-0c6a23d3d112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_chunk_size=100\n",
    "if is_local:\n",
    "    paraphs_parsed_dataset = load_from_disk(\"./data/wiki-270k\")\n",
    "else:\n",
    "    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n",
    "modified_texts_parsed = paraphs_parsed_dataset.map(lambda example:\n",
    "                                                     {'temp_text':\n",
    "                                                      f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n",
    "                                                     num_proc=2)[\"temp_text\"]\n",
    "\n",
    "if is_local:\n",
    "    cohere_dataset_filtered = load_from_disk(\"./data/stem-wiki-cohere\")\n",
    "else:\n",
    "    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n",
    "\n",
    "modified_texts = cohere_dataset_filtered.map(lambda example:\n",
    "                                         {'temp_text':\n",
    "                                          unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n",
    "                                         num_proc=2)[\"temp_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a05a1600-d54f-4f2d-8b59-088c824085f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SplitList(mylist, chunk_size):\n",
    "    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n",
    "\n",
    "def get_relevant_documents_parsed(df_valid):\n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts_parsed)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"title\"],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"text\"],\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_documents(df_valid):\n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         cohere_dataset_filtered[idx.item()][\"title\"],\n",
    "                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def retrieval(df_valid, modified_texts):\n",
    "    \n",
    "    corpus_df_valid = df_valid.apply(lambda row:\n",
    "                                     f'{row[\"question\"]}\\n{row[\"question\"]}\\n{row[\"question\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n",
    "                                     axis=1).values\n",
    "    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words)\n",
    "    vectorizer1.fit(corpus_df_valid)\n",
    "    vocab_df_valid = vectorizer1.get_feature_names_out()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words,\n",
    "                                 vocabulary=vocab_df_valid)\n",
    "    vectorizer.fit(modified_texts[:500000])\n",
    "    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n",
    "    \n",
    "    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    chunk_size = 100000\n",
    "    top_per_chunk = 3\n",
    "    top_per_query = 3\n",
    "\n",
    "    all_chunk_top_indices = []\n",
    "    all_chunk_top_values = []\n",
    "\n",
    "    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n",
    "        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n",
    "        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n",
    "        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n",
    "        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n",
    "\n",
    "        all_chunk_top_indices.append(chunk_top_indices + idx)\n",
    "        all_chunk_top_values.append(chunk_top_values)\n",
    "\n",
    "    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n",
    "    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n",
    "    \n",
    "    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n",
    "    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n",
    "    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n",
    "    \n",
    "    return articles_indices, merged_top_scores\n",
    "\n",
    "\n",
    "def prepare_answering_input(\n",
    "        tokenizer, \n",
    "        question,  \n",
    "        options,   \n",
    "        context,   \n",
    "        max_seq_length=4096,\n",
    "    ):\n",
    "    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n",
    "    c_plus_q_4 = [c_plus_q] * len(options)\n",
    "    tokenized_examples = tokenizer(\n",
    "        c_plus_q_4, options,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"longest\",\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n",
    "    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n",
    "    example_encoded = {\n",
    "        \"input_ids\": input_ids.to(model.device.index),\n",
    "        \"attention_mask\": attention_mask.to(model.device.index),\n",
    "    }\n",
    "    return example_encoded\n",
    "\n",
    "\n",
    "stop_words = ['each', 'you', 'the', 'use', 'used',\n",
    "                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n",
    "                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n",
    "                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n",
    "                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n",
    "                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n",
    "                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n",
    "                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n",
    "                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n",
    "                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n",
    "                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n",
    "                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n",
    "                  'did', 'theirs', 'can', 'those',\n",
    "                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n",
    "                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n",
    "                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n",
    "                  'yours', 'but', 'being', \"wasn't\", 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49852a01-c5bc-4865-8240-e3d32601639d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5c2c59424141ce8493f4ab8a13c161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 6286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1402ae12bbd444387462edf5b3ca791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 5933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1383b162b8045b1978956ad320f42ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc42fd7a4b94700868cddfbd9b3e4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/mambaforge/envs/pytorch/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 6118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f7e2a7567149428a4367a225afe8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 6245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01b3ab66b854103b111a524583e1700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 5971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fb4f50068e40918c57c1ccc519345c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_name = 'test'\n",
    "test_data = test_sets[test_name].to_pandas()\n",
    "test_wiki_tfidf_parsed = get_relevant_documents_parsed(test_data)\n",
    "train_wiki_ifidf_parsed = get_relevant_documents_parsed(train.to_pandas())\n",
    "# test_wiki_tfidf = get_relevant_documents(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21046c0f-71e6-4f6b-b9d0-cc7fd27cbf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_articles(example):\n",
    "    articles = example['retrieved_articles_parsed']\n",
    "    first_sentence = [f\"\"\"[CLS] {articles[-1]} [SEP] {articles[-2]} [SEP] {articles[-3]}\"\"\"] * 5\n",
    "    second_sentences = []\n",
    "    for option in 'ABCDE':\n",
    "        answer = example[option]\n",
    "        if answer is not None:\n",
    "            second_sentences.append(\" #### \" + example[\"question\"] + \" [SEP] \" + example[option] + \" [SEP]\")\n",
    "        else:\n",
    "            second_sentences.append('N/A')\n",
    "    try:\n",
    "        tokenized_example = tokenizer(first_sentence, second_sentences, truncation='longest_first', max_length=max_length)\n",
    "    except:\n",
    "        print(first_sentence, second_sentences)\n",
    "        raise\n",
    "    if 'answer' in example.keys():\n",
    "        tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "def tokenized_articles(data):\n",
    "    columns_to_keep = set(['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "    dataset = Dataset.from_pandas(data.to_pandas(), preserve_index=False)\n",
    "    col_to_remove = set(dataset.map(preprocess_articles).features.keys()) - columns_to_keep\n",
    "    tokenized = dataset.map(preprocess_articles, remove_columns=col_to_remove)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13cb4e-475c-471c-8845-78609b2376c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97a0a6-d4d7-4caa-bbdb-55aa56019dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer, TrainingArguments, IntervalStrategy, get_linear_schedule_with_warmup\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from datasets import Dataset # HuggingFace\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0912c-a359-4187-986f-90b5b15dd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_path = '/kaggle/input/deberta-for-colbert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2043a7-88bf-44be-b0e2-5c132bd870e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 450\n",
    "\n",
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "\n",
    "    \n",
    "    def __call__(self, input_batch):\n",
    "        # input_batch is list of samples, choices, tokens\n",
    "        additional_cols = set(input_batch[0].keys()) - set(['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "        if len(additional_cols) > 0:\n",
    "            print(f'{additional_cols=}')\n",
    "        \n",
    "        if 'label' in input_batch[0].keys():\n",
    "            label_name = 'label' \n",
    "            labels = [feature.pop(label_name) for feature in input_batch]\n",
    "        batch_size = len(input_batch)\n",
    "        num_choices = len(input_batch[0]['input_ids'])\n",
    "        flattened_input = [\n",
    "            [{k: v[i] for k, v in sample.items()} for i in range(num_choices)] for sample in input_batch\n",
    "        ]\n",
    "        flattened_input = sum(flattened_input, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_input,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # batch.shape = (n_samples, n_choices, n_tokens)\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        if 'label' in input_batch[0].keys():\n",
    "            batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        #print(np.array(batch['input_ids']).shape)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849ada6-1ab9-4739-96c8-fd7f0103541d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98be1ba-22a3-47b4-81d5-ee8a5ba76ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5a88e-13fa-4211-a61e-2c0ddc3be99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7c28f-dc9b-4086-b010-f349abf49768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7963ea1-a0b9-48e8-bee6-4ac4cbec647a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc68d9-72cc-4cf9-8fb8-cb81c5b207b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36408fa-f108-48f1-a0ed-a30f7074e85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9655ac5-496f-453f-a141-0148673136a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
