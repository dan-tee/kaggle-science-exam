{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139fdfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "from datasets import Dataset # HuggingFace\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoModel, IntervalStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e8112e-6666-45df-aa0c-578309b69681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "#logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04506376-fe46-4750-8e8e-c6bb559af9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'DeBERTa V3 Osmulski.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1385d217-eec6-4afb-ae8c-9e42d52425f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec79c1af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install wandb -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7b40123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1c680d-d26f-41ad-add6-d205c2c9ce25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deberta_v3_large = 'microsoft/deberta-v3-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a60a98de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/train.csv')\n",
    "df_test = df_test.drop(columns=\"id\")\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33962dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5800, 7) (200, 7) (500, 7)\n"
     ]
    }
   ],
   "source": [
    "df_6000 = pd.read_csv('data/osmulski_6000.csv')\n",
    "df_train = df_6000[:5800]\n",
    "df_test_1 = df_6000[5800:]\n",
    "df_test_2 = pd.read_csv('data/osmulski_extra_train.csv')\n",
    "print(df_train.shape, df_test_1.shape, df_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264141cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentences = [example[option] for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=True)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b6ccf20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 5800\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta_v3_large = 'microsoft/deberta-v3-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train, preserve_index=False)\n",
    "tokenized_train = train_dataset.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602f4aa9-2fbe-43fd-aff3-f78babd299f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "tokenized_test = test_dataset.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28898fcf",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073ee6c5-f52c-43f8-831a-a8afcd46bf3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision_at_k(predictions, actuals, k=3):        \n",
    "    if isinstance(actuals, list):\n",
    "        actuals = np.array(actuals)\n",
    "        \n",
    "    found_at = np.where(predictions == actuals.reshape(-1, 1))\n",
    "    # found_at is a tuple with the array of found indices in the second position\n",
    "    score = 1 / (1 + found_at[1])\n",
    "    score[score < 1/k] = 0\n",
    "    return score\n",
    "\n",
    "def mean_avg_precision_at_k(predictions, actual, k=3):\n",
    "    n = predictions.shape[0]\n",
    "    row_precision = precision_at_k(predictions, actual)\n",
    "    return row_precision.sum()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce703c71-7a03-4023-8fa4-1b54654897b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_random():\n",
    "    n_permutations = 200\n",
    "    n_numbers = 5 \n",
    "    # In this code, np.random.rand(n_permutations, n_numbers) generates a 2D array of random numbers.\n",
    "    # argsort(axis=1) then sorts along the second dimension (i.e., sorts each row) but instead of \n",
    "    # sorting the actual numbers, it sorts their indices, effectively creating a permutation.\n",
    "    random_predictions = np.random.rand(n_permutations, n_numbers).argsort(axis=1)\n",
    "    random_actuals = np.random.randint(0, n_numbers-1, n_permutations)\n",
    "    return mean_avg_precision_at_k(random_predictions, random_actuals)\n",
    "    \n",
    "scores = []\n",
    "for i in range(100000):\n",
    "    scores.append(score_random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15bea069-b5b1-42fb-a67d-d1d3d110214d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(scores).hist(bins=25)\n",
    "plt.title('Scores of Random Predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d3b1cec-5e3c-41f1-8803-0a411d0672da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.flip(predictions.argsort(axis=1), axis=1)\n",
    "    accuracy = acc_metric.compute(predictions=predictions[:,0], references=labels)['accuracy']\n",
    "    map_at_3 = mean_avg_precision_at_k(predictions, labels)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'map_at_3': round(map_at_3, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c95a3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMultipleChoice: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find DeBERTa V3 Osmulski.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdatadan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniel/code/kaggle-science-exam/wandb/run-20230808_135108-nhdhoqy8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/datadan/huggingface/runs/nhdhoqy8' target=\"_blank\">Train on reduced Osmulski</a></strong> to <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">https://wandb.ai/datadan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/datadan/huggingface/runs/nhdhoqy8' target=\"_blank\">https://wandb.ai/datadan/huggingface/runs/nhdhoqy8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1115' max='4350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1115/4350 02:22 < 06:55, 7.79 it/s, Epoch 0.77/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Map At 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.608400</td>\n",
       "      <td>1.609052</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.615500</td>\n",
       "      <td>1.607504</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.625600</td>\n",
       "      <td>1.603064</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.589300</td>\n",
       "      <td>1.593777</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.679000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m\n\u001b[1;32m     20\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForMultipleChoice\u001b[38;5;241m.\u001b[39mfrom_pretrained(deberta_v3_large)\n\u001b[1;32m     21\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     22\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     23\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     29\u001b[0m     )\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/trainer.py:2007\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2005\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2007\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[1;32m   2010\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:463\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    461\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m    462\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m--> 463\u001b[0m denom \u001b[38;5;241m=\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    465\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retrain = True\n",
    "\n",
    "output_path = Path('./checkpoints')\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.8,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=10,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    logging_steps=10,\n",
    "    eval_steps=250,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=5000,\n",
    "    report_to='wandb',\n",
    "    output_dir=str(output_path),\n",
    "    run_name='Train on reduced Osmulski'\n",
    ")\n",
    "\n",
    "if not output_path.exists() or retrain:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(deberta_v3_large)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "else:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(output_path/'checkpoint-19500')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ac5ec",
   "metadata": {},
   "source": [
    "## Predicting on the Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11a90242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.645, 'map_at_3': 0.771}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.55, 'map_at_3': 0.702}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.554, 'map_at_3': 0.701}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(test_set):\n",
    "    tokenized_test_dataset = Dataset.from_pandas(test_set).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E'])\n",
    "    test_predictions = trainer.predict(tokenized_test_dataset).predictions\n",
    "    print(compute_metrics([test_predictions, tokenized_test_dataset['label']]))\n",
    "    \n",
    "evaluate_model(df_test)\n",
    "evaluate_model(df_test_1)\n",
    "evaluate_model(df_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84d1e74a-6365-4f84-83a0-3ade8664b72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_predict = trainer.predict(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c51d41eb-3108-4d15-afa2-2f2830ffacfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        ndarray\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "[[-9.81499255e-01  1.33976841e+00  1.60739362e+00  4.27693319e+00\n",
       "           6.14280045e-01]\n",
       "           [ 1.4250420 <...> 324652e+00]\n",
       "           [-1.70279473e-01 -3.30993199e+00 -1.98556447e+00 -9.87624466e-01\n",
       "           -2.38353133e+00]]\n",
       "\u001b[0;31mLength:\u001b[0m      200\n",
       "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "ndarray(shape, dtype=float, buffer=None, offset=0,\n",
       "        strides=None, order=None)\n",
       "\n",
       "An array object represents a multidimensional, homogeneous array\n",
       "of fixed-size items.  An associated data-type object describes the\n",
       "format of each element in the array (its byte-order, how many bytes it\n",
       "occupies in memory, whether it is an integer, a floating point number,\n",
       "or something else, etc.)\n",
       "\n",
       "Arrays should be constructed using `array`, `zeros` or `empty` (refer\n",
       "to the See Also section below).  The parameters given here refer to\n",
       "a low-level method (`ndarray(...)`) for instantiating an array.\n",
       "\n",
       "For more information, refer to the `numpy` module and examine the\n",
       "methods and attributes of an array.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "(for the __new__ method; see Notes below)\n",
       "\n",
       "shape : tuple of ints\n",
       "    Shape of created array.\n",
       "dtype : data-type, optional\n",
       "    Any object that can be interpreted as a numpy data type.\n",
       "buffer : object exposing buffer interface, optional\n",
       "    Used to fill the array with data.\n",
       "offset : int, optional\n",
       "    Offset of array data in buffer.\n",
       "strides : tuple of ints, optional\n",
       "    Strides of data in memory.\n",
       "order : {'C', 'F'}, optional\n",
       "    Row-major (C-style) or column-major (Fortran-style) order.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "T : ndarray\n",
       "    Transpose of the array.\n",
       "data : buffer\n",
       "    The array's elements, in memory.\n",
       "dtype : dtype object\n",
       "    Describes the format of the elements in the array.\n",
       "flags : dict\n",
       "    Dictionary containing information related to memory use, e.g.,\n",
       "    'C_CONTIGUOUS', 'OWNDATA', 'WRITEABLE', etc.\n",
       "flat : numpy.flatiter object\n",
       "    Flattened version of the array as an iterator.  The iterator\n",
       "    allows assignments, e.g., ``x.flat = 3`` (See `ndarray.flat` for\n",
       "    assignment examples; TODO).\n",
       "imag : ndarray\n",
       "    Imaginary part of the array.\n",
       "real : ndarray\n",
       "    Real part of the array.\n",
       "size : int\n",
       "    Number of elements in the array.\n",
       "itemsize : int\n",
       "    The memory use of each array element in bytes.\n",
       "nbytes : int\n",
       "    The total number of bytes required to store the array data,\n",
       "    i.e., ``itemsize * size``.\n",
       "ndim : int\n",
       "    The array's number of dimensions.\n",
       "shape : tuple of ints\n",
       "    Shape of the array.\n",
       "strides : tuple of ints\n",
       "    The step-size required to move from one element to the next in\n",
       "    memory. For example, a contiguous ``(3, 4)`` array of type\n",
       "    ``int16`` in C-order has strides ``(8, 2)``.  This implies that\n",
       "    to move from element to element in memory requires jumps of 2 bytes.\n",
       "    To move from row-to-row, one needs to jump 8 bytes at a time\n",
       "    (``2 * 4``).\n",
       "ctypes : ctypes object\n",
       "    Class containing properties of the array needed for interaction\n",
       "    with ctypes.\n",
       "base : ndarray\n",
       "    If the array is a view into another array, that array is its `base`\n",
       "    (unless that array is also a view).  The `base` array is where the\n",
       "    array data is actually stored.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "array : Construct an array.\n",
       "zeros : Create an array, each element of which is zero.\n",
       "empty : Create an array, but leave its allocated memory unchanged (i.e.,\n",
       "        it contains \"garbage\").\n",
       "dtype : Create a data-type.\n",
       "numpy.typing.NDArray : An ndarray alias :term:`generic <generic type>`\n",
       "                       w.r.t. its `dtype.type <numpy.dtype.type>`.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "There are two modes of creating an array using ``__new__``:\n",
       "\n",
       "1. If `buffer` is None, then only `shape`, `dtype`, and `order`\n",
       "   are used.\n",
       "2. If `buffer` is an object exposing the buffer interface, then\n",
       "   all keywords are interpreted.\n",
       "\n",
       "No ``__init__`` method is needed because the array is fully initialized\n",
       "after the ``__new__`` method.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "These examples illustrate the low-level `ndarray` constructor.  Refer\n",
       "to the `See Also` section above for easier ways of constructing an\n",
       "ndarray.\n",
       "\n",
       "First mode, `buffer` is None:\n",
       "\n",
       ">>> np.ndarray(shape=(2,2), dtype=float, order='F')\n",
       "array([[0.0e+000, 0.0e+000], # random\n",
       "       [     nan, 2.5e-323]])\n",
       "\n",
       "Second mode:\n",
       "\n",
       ">>> np.ndarray((2,), buffer=np.array([1,2,3]),\n",
       "...            offset=np.int_().itemsize,\n",
       "...            dtype=int) # offset = 1*itemsize, i.e. skip first element\n",
       "array([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_predict.predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f0840fe-6b1a-4a3f-a572-5c02ec51e72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test_dataset = Dataset.from_pandas(df_test).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E'])\n",
    "test_logits = trainer.predict(tokenized_test_dataset).predictions\n",
    "test_probs = softmax(test_logits, axis=1)\n",
    "predictions = np.flip(test_logits.argsort(axis=1), axis=1)\n",
    "pred_1 = predictions[:, 0]\n",
    "pred_2 = predictions[:, 1]\n",
    "pred_3 = predictions[:, 2]\n",
    "\n",
    "row_indcs = np.arange(pred_1.shape[0])\n",
    "res = pd.DataFrame({\n",
    "    'pred_1': pred_1, \n",
    "    'prob_1': test_probs[row_indcs, pred_1],\n",
    "    'pred_2': pred_2, \n",
    "    'prob_2': test_probs[row_indcs, pred_2],\n",
    "    'pred_3': pred_3, \n",
    "    'prob_3': test_probs[row_indcs, pred_3],\n",
    "    'logit_sum': test_probs.sum(axis=1),\n",
    "    'prob_3_sum': test_probs[row_indcs, pred_1] + test_probs[row_indcs, pred_2] + test_probs[row_indcs, pred_3],\n",
    "    'actual': tokenized_test_dataset['label'],\n",
    "    'accurate': pred_1 == tokenized_test_dataset['label'],\n",
    "    'precision_at_3': precision_at_k(predictions, tokenized_test_dataset['label']).round(2)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04a71d78-655f-46a8-9742-a398e9dbafc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_1</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>prob_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>prob_3</th>\n",
       "      <th>logit_sum</th>\n",
       "      <th>prob_3_sum</th>\n",
       "      <th>actual</th>\n",
       "      <th>accurate</th>\n",
       "      <th>precision_at_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.867177</td>\n",
       "      <td>2</td>\n",
       "      <td>0.060082</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973232</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.384794</td>\n",
       "      <td>3</td>\n",
       "      <td>0.229879</td>\n",
       "      <td>0</td>\n",
       "      <td>0.216674</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.831346</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.692946</td>\n",
       "      <td>2</td>\n",
       "      <td>0.154489</td>\n",
       "      <td>4</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939527</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.350951</td>\n",
       "      <td>1</td>\n",
       "      <td>0.244707</td>\n",
       "      <td>0</td>\n",
       "      <td>0.237140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.832798</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.345061</td>\n",
       "      <td>3</td>\n",
       "      <td>0.286901</td>\n",
       "      <td>1</td>\n",
       "      <td>0.263349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.895311</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.674848</td>\n",
       "      <td>2</td>\n",
       "      <td>0.183446</td>\n",
       "      <td>3</td>\n",
       "      <td>0.059358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.917651</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.599422</td>\n",
       "      <td>2</td>\n",
       "      <td>0.315494</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991035</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0.540555</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352013</td>\n",
       "      <td>4</td>\n",
       "      <td>0.096040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.988608</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.509058</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390489</td>\n",
       "      <td>2</td>\n",
       "      <td>0.080096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979643</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.997954</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999747</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.311665</td>\n",
       "      <td>4</td>\n",
       "      <td>0.293321</td>\n",
       "      <td>0</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846486</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447903</td>\n",
       "      <td>1</td>\n",
       "      <td>0.278869</td>\n",
       "      <td>2</td>\n",
       "      <td>0.144810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871583</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.997349</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0.996248</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998803</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.967386</td>\n",
       "      <td>2</td>\n",
       "      <td>0.023223</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998418</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.478593</td>\n",
       "      <td>2</td>\n",
       "      <td>0.235248</td>\n",
       "      <td>3</td>\n",
       "      <td>0.168150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881990</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>0.889369</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995578</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0.426521</td>\n",
       "      <td>0</td>\n",
       "      <td>0.263443</td>\n",
       "      <td>3</td>\n",
       "      <td>0.261408</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951372</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>0.457016</td>\n",
       "      <td>0</td>\n",
       "      <td>0.418056</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.942232</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>0.996525</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>0.978562</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996114</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.298158</td>\n",
       "      <td>4</td>\n",
       "      <td>0.261631</td>\n",
       "      <td>3</td>\n",
       "      <td>0.221704</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781493</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0.694618</td>\n",
       "      <td>2</td>\n",
       "      <td>0.192220</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965157</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>0.612315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333254</td>\n",
       "      <td>3</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.981866</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.319313</td>\n",
       "      <td>4</td>\n",
       "      <td>0.313440</td>\n",
       "      <td>3</td>\n",
       "      <td>0.293677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.926429</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.513298</td>\n",
       "      <td>4</td>\n",
       "      <td>0.270580</td>\n",
       "      <td>2</td>\n",
       "      <td>0.215069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998947</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>0.991860</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>0.345262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>3</td>\n",
       "      <td>0.162558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.758986</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0.948775</td>\n",
       "      <td>3</td>\n",
       "      <td>0.032165</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>0.969604</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992756</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>0.338083</td>\n",
       "      <td>1</td>\n",
       "      <td>0.270573</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202366</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811022</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>0.480130</td>\n",
       "      <td>3</td>\n",
       "      <td>0.377145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.136106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993381</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>0.292815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.283962</td>\n",
       "      <td>4</td>\n",
       "      <td>0.183530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.760307</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>0.500728</td>\n",
       "      <td>3</td>\n",
       "      <td>0.408555</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987832</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>0.416513</td>\n",
       "      <td>0</td>\n",
       "      <td>0.186835</td>\n",
       "      <td>1</td>\n",
       "      <td>0.164088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767437</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0.951308</td>\n",
       "      <td>3</td>\n",
       "      <td>0.041580</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999765</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>0.809734</td>\n",
       "      <td>3</td>\n",
       "      <td>0.189950</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>0.557977</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245381</td>\n",
       "      <td>4</td>\n",
       "      <td>0.136614</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939972</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred_1    prob_1  pred_2    prob_2  pred_3    prob_3  logit_sum  \\\n",
       "0        3  0.867177       2  0.060082       1  0.045974        1.0   \n",
       "1        2  0.384794       3  0.229879       0  0.216674        1.0   \n",
       "2        0  0.692946       2  0.154489       4  0.092092        1.0   \n",
       "3        2  0.350951       1  0.244707       0  0.237140        1.0   \n",
       "4        0  0.345061       3  0.286901       1  0.263349        1.0   \n",
       "5        1  0.674848       2  0.183446       3  0.059358        1.0   \n",
       "6        0  0.599422       2  0.315494       1  0.076119        1.0   \n",
       "7        3  0.540555       1  0.352013       4  0.096040        1.0   \n",
       "8        0  0.509058       1  0.390489       2  0.080096        1.0   \n",
       "9        0  0.997954       1  0.001455       4  0.000338        1.0   \n",
       "10       1  0.311665       4  0.293321       0  0.241500        1.0   \n",
       "11       0  0.447903       1  0.278869       2  0.144810        1.0   \n",
       "12       2  0.997349       4  0.001130       1  0.000765        1.0   \n",
       "13       4  0.996248       3  0.001792       2  0.000762        1.0   \n",
       "14       1  0.967386       2  0.023223       3  0.007810        1.0   \n",
       "15       1  0.478593       2  0.235248       3  0.168150        1.0   \n",
       "16       2  0.889369       0  0.074125       1  0.032083        1.0   \n",
       "17       4  0.426521       0  0.263443       3  0.261408        1.0   \n",
       "18       3  0.457016       0  0.418056       4  0.067159        1.0   \n",
       "19       4  0.996525       3  0.002975       1  0.000205        1.0   \n",
       "20       3  0.978562       2  0.009485       1  0.008066        1.0   \n",
       "21       1  0.298158       4  0.261631       3  0.221704        1.0   \n",
       "22       3  0.694618       2  0.192220       0  0.078320        1.0   \n",
       "23       2  0.612315       1  0.333254       3  0.036297        1.0   \n",
       "24       0  0.319313       4  0.313440       3  0.293677        1.0   \n",
       "25       4  0.999945       1  0.000014       3  0.000014        1.0   \n",
       "26       0  0.513298       4  0.270580       2  0.215069        1.0   \n",
       "27       3  0.991860       0  0.004413       2  0.002777        1.0   \n",
       "28       2  0.345262       0  0.251166       3  0.162558        1.0   \n",
       "29       2  0.999991       1  0.000005       4  0.000001        1.0   \n",
       "30       1  0.948775       3  0.032165       4  0.006818        1.0   \n",
       "31       4  0.969604       0  0.011620       3  0.011532        1.0   \n",
       "32       3  0.338083       1  0.270573       0  0.202366        1.0   \n",
       "33       4  0.480130       3  0.377145       1  0.136106        1.0   \n",
       "34       2  0.292815       1  0.283962       4  0.183530        1.0   \n",
       "35       4  0.500728       3  0.408555       1  0.078549        1.0   \n",
       "36       4  0.416513       0  0.186835       1  0.164088        1.0   \n",
       "37       0  0.951308       3  0.041580       2  0.006876        1.0   \n",
       "38       4  0.809734       3  0.189950       1  0.000250        1.0   \n",
       "39       2  0.557977       0  0.245381       4  0.136614        1.0   \n",
       "\n",
       "    prob_3_sum  actual  accurate  precision_at_3  \n",
       "0     0.973232       3      True            1.00  \n",
       "1     0.831346       0     False            0.33  \n",
       "2     0.939527       0      True            1.00  \n",
       "3     0.832798       2      True            1.00  \n",
       "4     0.895311       3     False            0.50  \n",
       "5     0.917651       1      True            1.00  \n",
       "6     0.991035       0      True            1.00  \n",
       "7     0.988608       3      True            1.00  \n",
       "8     0.979643       2     False            0.33  \n",
       "9     0.999747       0      True            1.00  \n",
       "10    0.846486       4     False            0.50  \n",
       "11    0.871583       0      True            1.00  \n",
       "12    0.999244       2      True            1.00  \n",
       "13    0.998803       3     False            0.50  \n",
       "14    0.998418       1      True            1.00  \n",
       "15    0.881990       1      True            1.00  \n",
       "16    0.995578       4     False            0.00  \n",
       "17    0.951372       4      True            1.00  \n",
       "18    0.942232       0     False            0.50  \n",
       "19    0.999705       4      True            1.00  \n",
       "20    0.996114       3      True            1.00  \n",
       "21    0.781493       3     False            0.33  \n",
       "22    0.965157       2     False            0.50  \n",
       "23    0.981866       2      True            1.00  \n",
       "24    0.926429       4     False            0.50  \n",
       "25    0.999973       4      True            1.00  \n",
       "26    0.998947       0      True            1.00  \n",
       "27    0.999049       3      True            1.00  \n",
       "28    0.758986       4     False            0.00  \n",
       "29    0.999997       2      True            1.00  \n",
       "30    0.987758       1      True            1.00  \n",
       "31    0.992756       4      True            1.00  \n",
       "32    0.811022       4     False            0.00  \n",
       "33    0.993381       3     False            0.50  \n",
       "34    0.760307       2      True            1.00  \n",
       "35    0.987832       1     False            0.33  \n",
       "36    0.767437       4      True            1.00  \n",
       "37    0.999765       0      True            1.00  \n",
       "38    0.999934       4      True            1.00  \n",
       "39    0.939972       4     False            0.33  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b89c75e-68d4-4350-8d43-57e9d43a4f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.00    129\n",
       "0.50     39\n",
       "0.33     17\n",
       "0.00     15\n",
       "Name: precision_at_3, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['precision_at_3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f2c8820-1c49-4084-a52e-235a3dcdc6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGdCAYAAADJ6dNTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiBUlEQVR4nO3de3BU9f3/8dcmLAtBggKFJCVI6g0lFS1oB0Ub1IRSRGhH2opFxtuIApbiWEDrl0VFLp2hOFBvbQedsQE7oyBTL7Ct3CxiCYSKOmJhuAkyDIFJgNRlST6/P/hlMSRgDjnHfWfzfMzsxD179pz3vnJ28/Jkw4acc04AAABGZKR6AAAAgK+jnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwpU2qBzhdbW2t9u3bp44dOyoUCqV6HAAA0ATOOR05ckR5eXnKyGjeuQ9z5WTfvn3Kz89P9RgAAOAc7NmzRz169GjWNsyVk44dO0o6+eCys7NTPE1DiURCK1asUElJicLhcKrHMY+8vCEvb8jLG/Lyhry8OXTokAoKCpI/x5vDXDmp+1VOdna22XKSlZWl7OxsDtYmIC9vyMsb8vKGvLwhL28SiYQk+fKWDN4QCwAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAU9qkegAANvWa8lYg2905a2gg2wWQPjhzAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUz+VkzZo1GjZsmPLy8hQKhbR06dIzrvvAAw8oFApp3rx5zRgRAAC0Jp7LybFjx9S3b18tWLDgrOstXbpUH374ofLy8s55OAAA0Pq08XqHIUOGaMiQIWddZ+/evRo/fryWL1+uoUOHnvNwAACg9fFcTr5JbW2tRo8erUcffVR9+vT5xvXj8bji8XjyelVVlSQpkUgokUj4PV6z1c1kcTaLyMsbS3lFMl0g2/XzsVnKqyUgL2/Iyxs/c/K9nMyePVtt2rTRww8/3KT1Z86cqenTpzdYvmLFCmVlZfk9nm9isViqR2hRyMsbC3nNuTaY7b799tu+b9NCXi0JeXlDXk1TXV3t27Z8LScbN27Us88+q02bNikUCjXpPlOnTtWkSZOS16uqqpSfn6+SkhJlZ2f7OZ4vEomEYrGYiouLFQ6HUz2OeS0pr8Lo8sC2/XF0cJPWs5RXUHk0NYumsJRXS0Be3pCXNxUVFb5ty9dysnbtWh04cEA9e/ZMLqupqdEjjzyiefPmaefOnQ3uE4lEFIlEGiwPh8OmDwbr81nTEvKK1zStUJ8Lr4/dQl5B5RHE47KQV0tCXt6QV9P4mZGv5WT06NG65ZZb6i0bPHiwRo8erbvvvtvPXQEAgDTluZwcPXpU27ZtS17fsWOHNm/erM6dO6tnz57q0qVLvfXD4bBycnJ02WWXNX9aAACQ9jyXk7KyMg0aNCh5ve79ImPGjNHLL7/s22AAAKB18lxOioqK5FzT/8SwsfeZAAAAnAmfrQMAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADDFczlZs2aNhg0bpry8PIVCIS1dujR5WyKR0OTJk/X9739fHTp0UF5enu666y7t27fPz5kBAEAa81xOjh07pr59+2rBggUNbquurtamTZv0xBNPaNOmTXrjjTf0+eef67bbbvNlWAAAkP7aeL3DkCFDNGTIkEZv69Spk2KxWL1l8+fP17XXXqvdu3erZ8+e5zYlAABoNTyXE68qKysVCoV0/vnnN3p7PB5XPB5PXq+qqpJ08ldEiUQi6PE8q5vJ4mwWtaS8IpkusG039fFbyiuoPPx8bJbyagnIyxvy8sbPnELOuXN+BQqFQlqyZIlGjBjR6O1fffWVBg4cqN69e+vVV19tdJ1oNKrp06c3WF5aWqqsrKxzHQ0AAHyLqqurNWrUKFVWVio7O7tZ2wqsnCQSCY0cOVK7d+/WqlWrzjhoY2dO8vPzdfDgwWY/uCAkEgnFYjEVFxcrHA6nehzzWlJehdHlgW374+jgJq1nKa+g8mhqFk1hKa+WgLy8IS9vKioqlJub60s5CeTXOolEQj//+c+1Y8cOvffee2cdMhKJKBKJNFgeDodNHwzW57OmJeQVrwkFtm2vj91CXkHlEcTjspBXS0Je3pBX0/iZke/lpK6Y/Pe//9XKlSvVpUsXv3cBAADSmOdycvToUW3bti15fceOHdq8ebM6d+6svLw83X777dq0aZP+/ve/q6amRvv375ckde7cWW3btvVvcgAAkJY8l5OysjINGjQoeX3SpEmSpDFjxigajWrZsmWSpKuuuqre/VauXKmioqJznxQAALQKnstJUVGRzvYe2ma8vxYAAIDP1gEAALZQTgAAgCmUEwAAYArlBAAAmEI5AQAAplBOAACAKZQTAABgCuUEAACYQjkBAACmUE4AAIAplBMAAGAK5QQAAJhCOQEAAKZQTgAAgCmUEwAAYArlBAAAmNIm1QMAOHe9pryV6hEAwHecOQEAAKZQTgAAgCmUEwAAYArlBAAAmEI5AQAAplBOAACAKZQTAABgCuUEAACYQjkBAACmUE4AAIAplBMAAGAK5QQAAJhCOQEAAKZQTgAAgCmUEwAAYArlBAAAmEI5AQAAplBOAACAKZ7LyZo1azRs2DDl5eUpFApp6dKl9W53zikajSovL0/t27dXUVGRPvnkE7/mBQAAac5zOTl27Jj69u2rBQsWNHr7nDlzNHfuXC1YsEAbNmxQTk6OiouLdeTIkWYPCwAA0l8br3cYMmSIhgwZ0uhtzjnNmzdPjz/+uH72s59Jkl555RV1795dpaWleuCBB5o3LQAASHu+vudkx44d2r9/v0pKSpLLIpGIfvSjH2ndunV+7goAAKQpz2dOzmb//v2SpO7du9db3r17d+3atavR+8TjccXj8eT1qqoqSVIikVAikfBzPF/UzWRxNotaUl6RTBfYtpv6+L3mFeTMQfHzWGhJx5cF5OUNeXnjZ04h59w5v7qFQiEtWbJEI0aMkCStW7dO119/vfbt26fc3Nzkevfff7/27Nmjd999t8E2otGopk+f3mB5aWmpsrKyznU0AADwLaqurtaoUaNUWVmp7OzsZm3L1zMnOTk5kk6eQfl6OTlw4ECDsyl1pk6dqkmTJiWvV1VVKT8/XyUlJc1+cEFIJBKKxWIqLi5WOBxO9TjmtaS8CqPLA9v2x9HBTVrPa15BztwSRDKcnupfqyfKMhSvDTU559aqJT0fLSAvbyoqKnzblq/lpKCgQDk5OYrFYrr66qslScePH9fq1as1e/bsRu8TiUQUiUQaLA+Hw6YPBuvzWdMS8orXhALbttfH3tS8gpy5JYnXhhSvCZk/xqxoCc9HS8irafzMyHM5OXr0qLZt25a8vmPHDm3evFmdO3dWz549NXHiRD3zzDO65JJLdMkll+iZZ55RVlaWRo0a5dvQAAAgfXkuJ2VlZRo0aFDyet2vZMaMGaOXX35Zv/3tb/W///1PDz30kA4fPqwf/vCHWrFihTp27Ojf1AAAIG15LidFRUU623toQ6GQotGootFoc+YCAACtFJ+tAwAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMaZPqAZCeek15S5IUyXSac61UGF2ueE0oxVMB9tQ9V4Kwc9bQwLYNBIkzJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwxfdycuLECf3ud79TQUGB2rdvr+9973t68sknVVtb6/euAABAGmrj9wZnz56tF154Qa+88or69OmjsrIy3X333erUqZN+/etf+707AACQZnwvJx988IGGDx+uoUOHSpJ69eqlRYsWqayszO9dAQCANOT7r3UGDhyof/7zn/r8888lSf/5z3/0/vvv6yc/+YnfuwIAAGnI9zMnkydPVmVlpXr37q3MzEzV1NRoxowZuuOOOxpdPx6PKx6PJ69XVVVJkhKJhBKJhN/jNVvdTBZnsySS6U5+zaj/tbVq6vHi9fiqy7m1Ov34aonPyyC/h6fnweuXN+TljZ85hZxzvj4zFi9erEcffVS///3v1adPH23evFkTJ07U3LlzNWbMmAbrR6NRTZ8+vcHy0tJSZWVl+TkaAAAISHV1tUaNGqXKykplZ2c3a1u+l5P8/HxNmTJF48aNSy57+umn9eqrr+qzzz5rsH5jZ07y8/N18ODBZj+4ICQSCcViMRUXFyscDqd6HLMKo8slnfw/2qf61+qJsgzFa0Mpnip1Po4ObtJ6Xo+vupxbq9OPr6bmbEmQ38PT8+D1yxvy8qaiokK5ubm+lBPff61TXV2tjIz6b2XJzMw8458SRyIRRSKRBsvD4bDpg8H6fKkWr6lfROK1oQbLWhOvx0pTj6/WnOnX1R1fLfE5GeT38Ex58PrlDXk1jZ8Z+V5Ohg0bphkzZqhnz57q06ePysvLNXfuXN1zzz1+7woAAKQh38vJ/Pnz9cQTT+ihhx7SgQMHlJeXpwceeED/93//5/euAABAGvK9nHTs2FHz5s3TvHnz/N40AABoBfhsHQAAYArlBAAAmEI5AQAAplBOAACAKZQTAABgCuUEAACYQjkBAACmUE4AAIAplBMAAGAK5QQAAJhCOQEAAKZQTgAAgCmUEwAAYArlBAAAmEI5AQAAplBOAACAKZQTAABgSptUDwAALUGvKW+legTPTp85kuk051qpMLpc8ZrQOW9356yhzR0NOCvOnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAlEDKyd69e/WrX/1KXbp0UVZWlq666ipt3LgxiF0BAIA008bvDR4+fFjXX3+9Bg0apHfeeUfdunXT9u3bdf755/u9KwAAkIZ8LyezZ89Wfn6+Fi5cmFzWq1cvv3cDAADSlO/lZNmyZRo8eLBGjhyp1atX67vf/a4eeugh3X///Y2uH4/HFY/Hk9erqqokSYlEQolEwu/xmq1uJouzWRLJdCe/ZtT/2lo19XjxenzV5dxanX58Bfm8TIes/Xo+tpbXP17vvfEzp5BzztdnXLt27SRJkyZN0siRI/Xvf/9bEydO1Isvvqi77rqrwfrRaFTTp09vsLy0tFRZWVl+jgYAAAJSXV2tUaNGqbKyUtnZ2c3alu/lpG3bturfv7/WrVuXXPbwww9rw4YN+uCDDxqs39iZk/z8fB08eLDZDy4IiURCsVhMxcXFCofD9W4rjC4PZJ8fRwcHst0g1WURyXB6qn+tnijLULw2lOKp7CMvb07PK8jnSlDP72+TX8dXS35N8qKpebXEPIJQUVGh3NxcX8qJ77/Wyc3N1RVXXFFv2eWXX67XX3+90fUjkYgikUiD5eFwuMEPf0samy9eE8wPE8s5nMnpWcRrQ4Hlk47Iy5u6vIJ8rqTT96O5x1c6vCZ5uu835NUS8wiCnzn4/qfE119/vbZu3Vpv2eeff64LL7zQ710BAIA05Hs5+c1vfqP169frmWee0bZt21RaWqqXXnpJ48aN83tXAAAgDfleTq655hotWbJEixYtUmFhoZ566inNmzdPd955p9+7AgAAacj395xI0q233qpbb701iE0DAIA0x2frAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABT2qR6AKRWrylvpXoEAADq4cwJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEyhnAAAAFMoJwAAwBTKCQAAMIVyAgAATKGcAAAAUygnAADAFMoJAAAwhXICAABMoZwAAABTKCcAAMAUygkAADCFcgIAAEwJvJzMnDlToVBIEydODHpXAAAgDQRaTjZs2KCXXnpJV155ZZC7AQAAaSSwcnL06FHdeeed+tOf/qQLLrggqN0AAIA00yaoDY8bN05Dhw7VLbfcoqeffvqM68XjccXj8eT1qqoqSVIikVAikQhqvHNWN1Njs0UyXaD7DEJQMye3n+HqfcXZkZc3p+fVkp8r3wa/ji+Lr83f5Fy+f03NqyXmEQQ/cwg553x/xi1evFgzZszQhg0b1K5dOxUVFemqq67SvHnzGqwbjUY1ffr0BstLS0uVlZXl92gAACAA1dXVGjVqlCorK5Wdnd2sbfleTvbs2aP+/ftrxYoV6tu3rySdtZw0duYkPz9fBw8ebPaDC0IikVAsFlNxcbHC4XC92wqjywPZ58fRwYFsVwpu5jqRDKen+tfqibIMxWtDge4rHZCXN+TljV95teTXJC+amleQebQkFRUVys3N9aWc+P5rnY0bN+rAgQPq169fcllNTY3WrFmjBQsWKB6PKzMzM3lbJBJRJBJpsJ1wONzgh78ljc0XrwnmxTHIHIKaucF+akPf2r7SAXl5Q17eNDevdHhN8uKb8rL8s+rb5GcOvpeTm2++WVu2bKm37O6771bv3r01efLkesUEAADgdL6Xk44dO6qwsLDesg4dOqhLly4NlgMAAJyOfyEWAACYEtifEn/dqlWrvo3dAACANMCZEwAAYArlBAAAmEI5AQAAplBOAACAKZQTAABgCuUEAACYQjkBAACmUE4AAIAplBMAAGAK5QQAAJhCOQEAAKZQTgAAgCmUEwAAYArlBAAAmEI5AQAAplBOAACAKZQTAABgCuUEAACYQjkBAACmUE4AAIAplBMAAGAK5QQAAJhCOQEAAKZQTgAAgCmUEwAAYArlBAAAmEI5AQAAplBOAACAKZQTAABgCuUEAACYQjkBAACmUE4AAIAplBMAAGAK5QQAAJhCOQEAAKZQTgAAgCm+l5OZM2fqmmuuUceOHdWtWzeNGDFCW7du9Xs3AAAgTfleTlavXq1x48Zp/fr1isViOnHihEpKSnTs2DG/dwUAANJQG783+O6779a7vnDhQnXr1k0bN27UjTfe6PfuAABAmvG9nJyusrJSktS5c+dGb4/H44rH48nrVVVVkqREIqFEIhH0eJ7VzdTYbJFMF+g+gxDUzMntZ7h6X3F25OUNeXnjV14t+TXJi6bmZfFnVSr4mUPIORfYkeCc0/Dhw3X48GGtXbu20XWi0aimT5/eYHlpaamysrKCGg0AAPiourpao0aNUmVlpbKzs5u1rUDLybhx4/TWW2/p/fffV48ePRpdp7EzJ/n5+Tp48GCzH1wQEomEYrGYiouLFQ6H691WGF2eoqnsimQ4PdW/Vk+UZSheG0r1OOaRlzfk5Q15edPUvD6ODg5k/0H+TAli5oqKCuXm5vpSTgL7tc6ECRO0bNkyrVmz5ozFRJIikYgikUiD5eFwuMEPf0samy9ew5P9TOK1IfLxgLy8IS9vyMubb8orqJ9VQX6PgpjZz236Xk6cc5owYYKWLFmiVatWqaCgwO9dAACANOZ7ORk3bpxKS0v15ptvqmPHjtq/f78kqVOnTmrfvr3fuwMAAGnG93/n5Pnnn1dlZaWKioqUm5ubvLz22mt+7woAAKShQH6tAwAAcK74bB0AAGAK5QQAAJhCOQEAAKZQTgAAgCmUEwAAYArlBAAAmEI5AQAAplBOAACAKZQTAABgCuUEAACYQjkBAACmUE4AAIAplBMAAGAK5QQAAJhCOQEAAKZQTgAAgCmUEwAAYEqbVA/wbes15a1m3T+S6TTnWqkwulzxmpBPUwEAgDqcOQEAAKZQTgAAgCmUEwAAYArlBAAAmEI5AQAAplBOAACAKZQTAABgCuUEAACYQjkBAACmUE4AAIAplBMAAGAK5QQAAJhCOQEAAKZQTgAAgCmUEwAAYArlBAAAmEI5AQAAplBOAACAKYGVk+eee04FBQVq166d+vXrp7Vr1wa1KwAAkEYCKSevvfaaJk6cqMcff1zl5eW64YYbNGTIEO3evTuI3QEAgDQSSDmZO3eu7r33Xt133326/PLLNW/ePOXn5+v5558PYncAACCNtPF7g8ePH9fGjRs1ZcqUestLSkq0bt26BuvH43HF4/Hk9crKSknSoUOHlEgk/B5PbU4ca979a52qq2vVJpGhmtqQT1OlL/Lyhry8IS9vyMubpuZVUVERzP6b+fPqbIKY+dChQ5Ik51zzN+Z8tnfvXifJ/etf/6q3fMaMGe7SSy9tsP60adOcJC5cuHDhwoVLGly2b9/e7C7h+5mTOqFQ/ZbpnGuwTJKmTp2qSZMmJa/X1tbq0KFD6tKlS6Prp1pVVZXy8/O1Z88eZWdnp3oc88jLG/Lyhry8IS9vyMubyspK9ezZU507d272tnwvJ127dlVmZqb2799fb/mBAwfUvXv3ButHIhFFIpF6y84//3y/x/JddnY2B6sH5OUNeXlDXt6Qlzfk5U1GRvPfzur7G2Lbtm2rfv36KRaL1Vsei8V03XXX+b07AACQZgL5tc6kSZM0evRo9e/fXwMGDNBLL72k3bt3a+zYsUHsDgAApJFAyskvfvELVVRU6Mknn9SXX36pwsJCvf3227rwwguD2N23KhKJaNq0aQ1+FYXGkZc35OUNeXlDXt6Qlzd+5hVyzo+/+QEAAPAHn60DAABMoZwAAABTKCcAAMAUygkAADCFcuLBbbfdpp49e6pdu3bKzc3V6NGjtW/fvnrr7N69W8OGDVOHDh3UtWtXPfzwwzp+/HiKJk6dnTt36t5771VBQYHat2+viy66SNOmTWuQBXmdMmPGDF133XXKyso64z9ESF6nPPfccyooKFC7du3Ur18/rV27NtUjmbBmzRoNGzZMeXl5CoVCWrp0ab3bnXOKRqPKy8tT+/btVVRUpE8++SQ1wxowc+ZMXXPNNerYsaO6deumESNGaOvWrfXWIbNTnn/+eV155ZXJf5huwIABeuedd5K3+5UV5cSDQYMG6W9/+5u2bt2q119/Xdu3b9ftt9+evL2mpkZDhw7VsWPH9P7772vx4sV6/fXX9cgjj6Rw6tT47LPPVFtbqxdffFGffPKJ/vCHP+iFF17QY489llyHvOo7fvy4Ro4cqQcffLDR28nrlNdee00TJ07U448/rvLyct1www0aMmSIdu/enerRUu7YsWPq27evFixY0Ojtc+bM0dy5c7VgwQJt2LBBOTk5Ki4u1pEjR77lSW1YvXq1xo0bp/Xr1ysWi+nEiRMqKSnRsWOnPnSPzE7p0aOHZs2apbKyMpWVlemmm27S8OHDkwXEt6ya/ek8rdibb77pQqGQO378uHPOubfffttlZGS4vXv3JtdZtGiRi0QirrKyMlVjmjFnzhxXUFCQvE5ejVu4cKHr1KlTg+Xkdcq1117rxo4dW29Z79693ZQpU1I0kU2S3JIlS5LXa2trXU5Ojps1a1Zy2VdffeU6derkXnjhhRRMaM+BAwecJLd69WrnHJk1xQUXXOD+/Oc/+5oVZ07O0aFDh/TXv/5V1113ncLhsCTpgw8+UGFhofLy8pLrDR48WPF4XBs3bkzVqGZUVlbW+0Ao8vKGvE46fvy4Nm7cqJKSknrLS0pKtG7duhRN1TLs2LFD+/fvr5ddJBLRj370I7L7/yorKyUp+VpFZmdWU1OjxYsX69ixYxowYICvWVFOPJo8ebI6dOigLl26aPfu3XrzzTeTt+3fv7/BhxtecMEFatu2bYMPQmxttm/frvnz59f7CAPy8oa8Tjp48KBqamoaZNG9e/dWlcO5qMuH7BrnnNOkSZM0cOBAFRYWSiKzxmzZskXnnXeeIpGIxo4dqyVLluiKK67wNatWX06i0ahCodBZL2VlZcn1H330UZWXl2vFihXKzMzUXXfdJfe1f2Q3FAo12IdzrtHlLZHXvCRp3759+vGPf6yRI0fqvvvuq3cbeTXM62zSPS8vTn/MrTWHc0F2jRs/frw++ugjLVq0qMFtZHbKZZddps2bN2v9+vV68MEHNWbMGH366afJ2/3IKpDP1mlJxo8fr1/+8pdnXadXr17J/+7atau6du2qSy+9VJdffrny8/O1fv16DRgwQDk5Ofrwww/r3ffw4cNKJBINmmRL5TWvffv2adCgQckPgPw68jrp63mdTWvIqym6du2qzMzMBv8nduDAgVaVw7nIycmRdPJsQG5ubnI52UkTJkzQsmXLtGbNGvXo0SO5nMwaatu2rS6++GJJUv/+/bVhwwY9++yzmjx5siR/smr15aSubJyLujMm8XhckjRgwADNmDFDX375ZfIbs2LFCkUiEfXr18+fgVPMS1579+7VoEGD1K9fPy1cuFAZGfVP1JGXN60hr6Zo27at+vXrp1gspp/+9KfJ5bFYTMOHD0/hZPYVFBQoJydHsVhMV199taST7+FZvXq1Zs+eneLpUsM5pwkTJmjJkiVatWqVCgoK6t1OZt/MOad4PO5vVn68U7c1+PDDD938+fNdeXm527lzp3vvvffcwIED3UUXXeS++uor55xzJ06ccIWFhe7mm292mzZtcv/4xz9cjx493Pjx41M8/bdv79697uKLL3Y33XST++KLL9yXX36ZvNQhr/p27drlysvL3fTp0915553nysvLXXl5uTty5Ihzjry+bvHixS4cDru//OUv7tNPP3UTJ050HTp0cDt37kz1aCl35MiR5LEjyc2dO9eVl5e7Xbt2OeecmzVrluvUqZN744033JYtW9wdd9zhcnNzXVVVVYonT40HH3zQderUya1atare61R1dXVyHTI7ZerUqW7NmjVux44d7qOPPnKPPfaYy8jIcCtWrHDO+ZcV5aSJPvroIzdo0CDXuXNnF4lEXK9evdzYsWPdF198UW+9Xbt2uaFDh7r27du7zp07u/HjxyfLS2uycOFCJ6nRy9eR1yljxoxpNK+VK1cm1yGvU/74xz+6Cy+80LVt29b94Ac/SP7pZ2u3cuXKRo+jMWPGOOdO/mnstGnTXE5OjotEIu7GG290W7ZsSe3QKXSm16mFCxcm1yGzU+65557k8+473/mOu/nmm5PFxDn/sgo597V3cwIAAKRYq/9rHQAAYAvlBAAAmEI5AQAAplBOAACAKZQTAABgCuUEAACYQjkBAACmUE4AAIAplBMAAGAK5QQAAJhCOQEAAKZQTgAAgCn/D0YBgpI7XYuTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res[res['accurate']]['prob_sum'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a030acc-067f-49b2-9932-eb28aeddd6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e0a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f101ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_as_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions_as_answer_letters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mABCDE\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[43mpredictions_as_ids\u001b[49m]\n\u001b[1;32m      2\u001b[0m predictions_as_answer_letters[:\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_as_ids' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "predictions_as_answer_letters[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "591ead9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D B C', 'A E B', 'A C E']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_as_string = df_test['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]\n",
    "predictions_as_string[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb36aac-1eeb-4096-9bf6-afdd3f7bd0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
