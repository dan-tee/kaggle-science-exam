{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e2ffc8-e1c3-4d44-8e6b-248b073af990",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre Train on Wiki & Train on Osmulski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139fdfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "from datasets import Dataset # HuggingFace\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForMultipleChoice, TrainingArguments,\\\n",
    "                         Trainer, AutoModel, IntervalStrategy,logging, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04506376-fe46-4750-8e8e-c6bb559af9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'PreTrainWikiOsmulski.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1385d217-eec6-4afb-ae8c-9e42d52425f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec79c1af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install polars -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b40123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1c680d-d26f-41ad-add6-d205c2c9ce25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deberta_v3_large = 'microsoft/deberta-v3-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a2323-3b93-4387-bba7-42d425d38e46",
   "metadata": {},
   "source": [
    "## Pretain on Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78822ded-1230-4f40-a404-980ae73504a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79118, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>title</th><th>page_id</th><th>parent_id</th><th>revision_id</th><th>revision_ts</th><th>short_description</th><th>sha1</th><th>page_bytes</th><th>section_index</th><th>section_title</th><th>section_level</th><th>section_text</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>datetime[μs]</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;&#x27;t Hooft loop&quot;</td><td>3386119</td><td>1136040727</td><td>1150612405</td><td>2023-04-19 03:13:11</td><td>&quot;Magnetic loop …</td><td>&quot;0u40ciisnuhjlc…</td><td>12559</td><td>0</td><td>&quot;Summary&quot;</td><td>1</td><td>&quot;In quantum fie…</td></tr><tr><td>&quot;&#x27;t Hooft loop&quot;</td><td>3386119</td><td>1136040727</td><td>1150612405</td><td>2023-04-19 03:13:11</td><td>&quot;Magnetic loop …</td><td>&quot;0u40ciisnuhjlc…</td><td>12559</td><td>1</td><td>&quot;Definition&quot;</td><td>2</td><td>&quot;There are a nu…</td></tr><tr><td>&quot;&#x27;t Hooft loop&quot;</td><td>3386119</td><td>1136040727</td><td>1150612405</td><td>2023-04-19 03:13:11</td><td>&quot;Magnetic loop …</td><td>&quot;0u40ciisnuhjlc…</td><td>12559</td><td>2</td><td>&quot;Disorder opera…</td><td>2</td><td>&quot;The &#x27;t Hooft l…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 12)\n",
       "┌────────────┬─────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ title      ┆ page_id ┆ parent_id ┆ revision_ ┆ … ┆ section_i ┆ section_t ┆ section_l ┆ section_t │\n",
       "│ ---        ┆ ---     ┆ ---       ┆ id        ┆   ┆ ndex      ┆ itle      ┆ evel      ┆ ext       │\n",
       "│ str        ┆ i64     ┆ i64       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆         ┆           ┆ i64       ┆   ┆ i64       ┆ str       ┆ i64       ┆ str       │\n",
       "╞════════════╪═════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 't Hooft   ┆ 3386119 ┆ 113604072 ┆ 115061240 ┆ … ┆ 0         ┆ Summary   ┆ 1         ┆ In        │\n",
       "│ loop       ┆         ┆ 7         ┆ 5         ┆   ┆           ┆           ┆           ┆ quantum   │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ field     │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ theory,   │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ the 't …  │\n",
       "│ 't Hooft   ┆ 3386119 ┆ 113604072 ┆ 115061240 ┆ … ┆ 1         ┆ Definitio ┆ 2         ┆ There are │\n",
       "│ loop       ┆         ┆ 7         ┆ 5         ┆   ┆           ┆ n         ┆           ┆ a number  │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ of ways   │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ to de…    │\n",
       "│ 't Hooft   ┆ 3386119 ┆ 113604072 ┆ 115061240 ┆ … ┆ 2         ┆ Disorder  ┆ 2         ┆ The 't    │\n",
       "│ loop       ┆         ┆ 7         ┆ 5         ┆   ┆           ┆ operator  ┆           ┆ Hooft     │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ loop is a │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ disorder  │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆           ┆           ┆ …         │\n",
       "└────────────┴─────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sections = pl.read_parquet('./data/wiki_physics_math.parquet')\n",
    "print(wiki_sections.shape)\n",
    "wiki_sections[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b3eb6b-f41d-444c-826f-a3e7d9e1fce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "deberta_v3_large = 'microsoft/deberta-v3-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large)\n",
    "model = AutoModelForMaskedLM.from_pretrained(deberta_v3_large)\n",
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6cea2df-9b78-475d-9524-b8b5d6f9eab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2399: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "    num_rows: 79118\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_wiki_section(section_text):\n",
    "    return tokenizer.encode_plus(section_text['section_text'],\n",
    "                                 add_special_tokens=True,\n",
    "                                 max_length=512,\n",
    "                                 stride=64,\n",
    "                                 truncation_strategy='longest_first',\n",
    "                                 return_overflowing_tokens=True,\n",
    "                                 padding='max_length')\n",
    "\n",
    "wiki_dataset = Dataset.from_pandas(wiki_sections.to_pandas()[['section_text']], preserve_index=False)\n",
    "wiki_tokenized = wiki_dataset.map(tokenize_wiki_section, \n",
    "                                  remove_columns=['section_text'])\n",
    "wiki_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d67232d-4502-4553-9bc4-b93880671c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84967756e62f4ce6939efef6c6caf0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flat_tokenized = {}\n",
    "for key in tqdm(list(wiki_tokenized.features)):\n",
    "    flattened_list = [item for sublist in wiki_tokenized[key] for item in sublist]\n",
    "    flat_tokenized[key] = flattened_list\n",
    "    \n",
    "flat_tokenized = Dataset.from_dict(flat_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb8661b3-2a94-4769-adf0-d2d7d106dd64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 1250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 25000\n",
    "test_size = int(0.05 * train_size)\n",
    "wiki_split_sample = flat_tokenized.train_test_split(train_size=train_size, test_size=test_size)\n",
    "wiki_split_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a20e16-8a39-4dc0-9641-6144491eead9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Apply random masking once on the whole test data, then uses the default data collector\n",
    "to handle the test dataset in batches '''\n",
    "\n",
    "masking_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = masking_data_collator(features)\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "eval_dataset = wiki_split_sample[\"test\"].map(insert_random_mask,batched=True,\n",
    "    remove_columns=wiki_split_sample[\"test\"].column_names)\n",
    "\n",
    "eval_dataset = eval_dataset.rename_columns({\n",
    "    \"masked_input_ids\": \"input_ids\",\n",
    "    \"masked_attention_mask\": \"attention_mask\",\n",
    "    \"masked_labels\": \"labels\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e807f-0dc1-41ca-8a8d-8e8783f2a64c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06b98f6d-fec4-4461-b17a-3d1f2c2e2a18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdatadan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniel/code/kaggle-science-exam/wandb/run-20230810_160542-2wkypilh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/datadan/huggingface/runs/2wkypilh' target=\"_blank\">Train on Wiki</a></strong> to <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">https://wandb.ai/datadan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/datadan/huggingface/runs/2wkypilh' target=\"_blank\">https://wandb.ai/datadan/huggingface/runs/2wkypilh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 23.65 GiB total capacity; 22.20 GiB already allocated; 514.38 MiB free; 22.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m\n\u001b[1;32m     20\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(deberta_v3_large)\n\u001b[1;32m     21\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     22\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     23\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m         eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset\n\u001b[1;32m     28\u001b[0m     )\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1940\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1943\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1946\u001b[0m ):\n\u001b[1;32m   1947\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/trainer.py:2767\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2766\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2767\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1171\u001b[0m, in \u001b[0;36mDebertaV2ForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1183\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1083\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1075\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1076\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1077\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1081\u001b[0m )\n\u001b[0;32m-> 1083\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:521\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    513\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    514\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         rel_embeddings,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    531\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    355\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m ):\n\u001b[0;32m--> 362\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    371\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    286\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    292\u001b[0m ):\n\u001b[0;32m--> 293\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    302\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:725\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    723\u001b[0m     scale_factor \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    724\u001b[0m scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(query_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat) \u001b[38;5;241m*\u001b[39m scale_factor)\n\u001b[0;32m--> 725\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[1;32m    727\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 23.65 GiB total capacity; 22.20 GiB already allocated; 514.38 MiB free; 22.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "retrain = True\n",
    "\n",
    "output_path = Path('./wiki_checkpoints')\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.8,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    logging_steps=10,\n",
    "    eval_steps=1000,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=5000,\n",
    "    report_to='wandb',\n",
    "    output_dir=str(output_path),\n",
    "    run_name='Train on Wiki'\n",
    ")\n",
    "\n",
    "if not output_path.exists() or retrain:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(deberta_v3_large)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=masking_data_collator,\n",
    "        train_dataset=wiki_split_sample['train'],\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "else:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(output_path/'checkpoint-19500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3f721-0a16-45d8-bce3-78e89ef49a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2871d-5642-4dfc-9251-24741856201a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ac4a3c2-220a-4f7e-b241-dc15daf907a2",
   "metadata": {},
   "source": [
    "## Train on Osmulski Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a60a98de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/train.csv')\n",
    "df_test = df_test.drop(columns=\"id\")\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33962dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5800, 7) (200, 7) (500, 7)\n"
     ]
    }
   ],
   "source": [
    "df_6000 = pd.read_csv('data/osmulski_6000.csv')\n",
    "df_train = df_6000[:5800]\n",
    "df_test_1 = df_6000[5800:]\n",
    "df_test_2 = pd.read_csv('data/osmulski_extra_train.csv')\n",
    "print(df_train.shape, df_test_1.shape, df_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "264141cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentences = [example[option] for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=True)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b6ccf20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 5800\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train, preserve_index=False)\n",
    "tokenized_train = train_dataset.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "602f4aa9-2fbe-43fd-aff3-f78babd299f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "tokenized_test = test_dataset.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319e9c6-0623-476e-a621-19a8f363f7ec",
   "metadata": {},
   "source": [
    "## Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae4b46-2117-45bc-9730-e90506c099ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(predictions, actuals, k=3):        \n",
    "    if isinstance(actuals, list):\n",
    "        actuals = np.array(actuals)\n",
    "        \n",
    "    found_at = np.where(predictions == actuals.reshape(-1, 1))\n",
    "    # found_at is a tuple with the array of found indices in the second position\n",
    "    score = 1 / (1 + found_at[1])\n",
    "    score[score < 1/k] = 0\n",
    "    return score\n",
    "\n",
    "def mean_avg_precision_at_k(predictions, actual, k=3):\n",
    "    n = predictions.shape[0]\n",
    "    row_precision = precision_at_k(predictions, actual)\n",
    "    return row_precision.sum()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891f3a1-350c-4d01-be6b-5541ba828e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_random():\n",
    "    n_permutations = 200\n",
    "    n_numbers = 5 \n",
    "    # In this code, np.random.rand(n_permutations, n_numbers) generates a 2D array of random numbers.\n",
    "    # argsort(axis=1) then sorts along the second dimension (i.e., sorts each row) but instead of \n",
    "    # sorting the actual numbers, it sorts their indices, effectively creating a permutation.\n",
    "    random_predictions = np.random.rand(n_permutations, n_numbers).argsort(axis=1)\n",
    "    random_actuals = np.random.randint(0, n_numbers-1, n_permutations)\n",
    "    return mean_avg_precision_at_k(random_predictions, random_actuals)\n",
    "    \n",
    "scores = []\n",
    "for i in range(100000):\n",
    "    scores.append(score_random())\n",
    "    \n",
    "pd.Series(scores).hist(bins=25)\n",
    "plt.title('Scores of Random Predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf4c20-dad1-4475-8d06-c754d80c394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.flip(predictions.argsort(axis=1), axis=1)\n",
    "    accuracy = acc_metric.compute(predictions=predictions[:,0], references=labels)['accuracy']\n",
    "    map_at_3 = mean_avg_precision_at_k(predictions, labels)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'map_at_3': round(map_at_3, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28898fcf",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c95a3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMultipleChoice: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdatadan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniel/code/kaggle-science-exam/wandb/run-20230807_164147-af23tet0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/datadan/huggingface/runs/af23tet0' target=\"_blank\">Train on reduced Osmulski</a></strong> to <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">https://wandb.ai/datadan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/datadan/huggingface/runs/af23tet0' target=\"_blank\">https://wandb.ai/datadan/huggingface/runs/af23tet0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4350' max='4350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4350/4350 09:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Map At 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.612600</td>\n",
       "      <td>1.609225</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.616300</td>\n",
       "      <td>1.608112</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.633900</td>\n",
       "      <td>1.605333</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.586800</td>\n",
       "      <td>1.602573</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.535900</td>\n",
       "      <td>1.529727</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.243800</td>\n",
       "      <td>1.175112</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.245200</td>\n",
       "      <td>1.041963</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.110300</td>\n",
       "      <td>0.987762</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.207200</td>\n",
       "      <td>0.973011</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.037800</td>\n",
       "      <td>0.956732</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.207800</td>\n",
       "      <td>0.907549</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>0.869373</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.404000</td>\n",
       "      <td>0.852403</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.177300</td>\n",
       "      <td>0.898623</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.950321</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>0.875815</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>0.896306</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.773000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a269b95e5f44c218434a9113473c6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▂▄▆▆▇████████▇█▇</td></tr><tr><td>eval/loss</td><td>████▇▄▃▂▂▂▂▁▁▁▂▁▁</td></tr><tr><td>eval/map_at_3</td><td>▁▃▄▆▆█████████▇██</td></tr><tr><td>eval/runtime</td><td>▁▃▄▅▄▆▆▆▇▇▆▆▆▆█▇█</td></tr><tr><td>eval/samples_per_second</td><td>█▆▅▄▅▃▃▃▂▂▃▃▃▃▁▂▁</td></tr><tr><td>eval/steps_per_second</td><td>█▆▅▄▅▃▃▃▂▂▃▃▃▃▁▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇████▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>███████████▇▆▆▅▅▇▄▅▅▅▇▅▃▄▄▅▃▃▁▃▄▂▃▂▃▃▄▂▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.655</td></tr><tr><td>eval/loss</td><td>0.89631</td></tr><tr><td>eval/map_at_3</td><td>0.773</td></tr><tr><td>eval/runtime</td><td>2.4044</td></tr><tr><td>eval/samples_per_second</td><td>83.182</td></tr><tr><td>eval/steps_per_second</td><td>8.318</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>4350</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8477</td></tr><tr><td>train/total_flos</td><td>8034913607705880.0</td></tr><tr><td>train/train_loss</td><td>1.20149</td></tr><tr><td>train/train_runtime</td><td>566.2284</td></tr><tr><td>train/train_samples_per_second</td><td>30.73</td></tr><tr><td>train/train_steps_per_second</td><td>7.682</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Train on reduced Osmulski</strong> at: <a href='https://wandb.ai/datadan/huggingface/runs/af23tet0' target=\"_blank\">https://wandb.ai/datadan/huggingface/runs/af23tet0</a><br/> View job at <a href='https://wandb.ai/datadan/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg4NTI4NzEw/version_details/v2' target=\"_blank\">https://wandb.ai/datadan/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg4NTI4NzEw/version_details/v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230807_164147-af23tet0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrain = False\n",
    "\n",
    "output_path = Path('./checkpoints')\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.8,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=10,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    logging_steps=10,\n",
    "    eval_steps=250,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=5000,\n",
    "    report_to='wandb',\n",
    "    output_dir=str(output_path),\n",
    "    run_name='Train on reduced Osmulski'\n",
    ")\n",
    "\n",
    "if not output_path.exists() or retrain:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(deberta_v3_large)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "else:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(output_path/'checkpoint-19500')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ac5ec",
   "metadata": {},
   "source": [
    "## Predicting on the Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11a90242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.645, 'map_at_3': 0.771}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.55, 'map_at_3': 0.702}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.554, 'map_at_3': 0.701}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(test_set):\n",
    "    tokenized_test_dataset = Dataset.from_pandas(test_set).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E'])\n",
    "    test_predictions = trainer.predict(tokenized_test_dataset).predictions\n",
    "    print(compute_metrics([test_predictions, tokenized_test_dataset['label']]))\n",
    "    \n",
    "evaluate_model(df_test)\n",
    "evaluate_model(df_test_1)\n",
    "evaluate_model(df_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84d1e74a-6365-4f84-83a0-3ade8664b72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_predict = trainer.predict(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c51d41eb-3108-4d15-afa2-2f2830ffacfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        ndarray\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "[[-9.81499255e-01  1.33976841e+00  1.60739362e+00  4.27693319e+00\n",
       "           6.14280045e-01]\n",
       "           [ 1.4250420 <...> 324652e+00]\n",
       "           [-1.70279473e-01 -3.30993199e+00 -1.98556447e+00 -9.87624466e-01\n",
       "           -2.38353133e+00]]\n",
       "\u001b[0;31mLength:\u001b[0m      200\n",
       "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "ndarray(shape, dtype=float, buffer=None, offset=0,\n",
       "        strides=None, order=None)\n",
       "\n",
       "An array object represents a multidimensional, homogeneous array\n",
       "of fixed-size items.  An associated data-type object describes the\n",
       "format of each element in the array (its byte-order, how many bytes it\n",
       "occupies in memory, whether it is an integer, a floating point number,\n",
       "or something else, etc.)\n",
       "\n",
       "Arrays should be constructed using `array`, `zeros` or `empty` (refer\n",
       "to the See Also section below).  The parameters given here refer to\n",
       "a low-level method (`ndarray(...)`) for instantiating an array.\n",
       "\n",
       "For more information, refer to the `numpy` module and examine the\n",
       "methods and attributes of an array.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "(for the __new__ method; see Notes below)\n",
       "\n",
       "shape : tuple of ints\n",
       "    Shape of created array.\n",
       "dtype : data-type, optional\n",
       "    Any object that can be interpreted as a numpy data type.\n",
       "buffer : object exposing buffer interface, optional\n",
       "    Used to fill the array with data.\n",
       "offset : int, optional\n",
       "    Offset of array data in buffer.\n",
       "strides : tuple of ints, optional\n",
       "    Strides of data in memory.\n",
       "order : {'C', 'F'}, optional\n",
       "    Row-major (C-style) or column-major (Fortran-style) order.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "T : ndarray\n",
       "    Transpose of the array.\n",
       "data : buffer\n",
       "    The array's elements, in memory.\n",
       "dtype : dtype object\n",
       "    Describes the format of the elements in the array.\n",
       "flags : dict\n",
       "    Dictionary containing information related to memory use, e.g.,\n",
       "    'C_CONTIGUOUS', 'OWNDATA', 'WRITEABLE', etc.\n",
       "flat : numpy.flatiter object\n",
       "    Flattened version of the array as an iterator.  The iterator\n",
       "    allows assignments, e.g., ``x.flat = 3`` (See `ndarray.flat` for\n",
       "    assignment examples; TODO).\n",
       "imag : ndarray\n",
       "    Imaginary part of the array.\n",
       "real : ndarray\n",
       "    Real part of the array.\n",
       "size : int\n",
       "    Number of elements in the array.\n",
       "itemsize : int\n",
       "    The memory use of each array element in bytes.\n",
       "nbytes : int\n",
       "    The total number of bytes required to store the array data,\n",
       "    i.e., ``itemsize * size``.\n",
       "ndim : int\n",
       "    The array's number of dimensions.\n",
       "shape : tuple of ints\n",
       "    Shape of the array.\n",
       "strides : tuple of ints\n",
       "    The step-size required to move from one element to the next in\n",
       "    memory. For example, a contiguous ``(3, 4)`` array of type\n",
       "    ``int16`` in C-order has strides ``(8, 2)``.  This implies that\n",
       "    to move from element to element in memory requires jumps of 2 bytes.\n",
       "    To move from row-to-row, one needs to jump 8 bytes at a time\n",
       "    (``2 * 4``).\n",
       "ctypes : ctypes object\n",
       "    Class containing properties of the array needed for interaction\n",
       "    with ctypes.\n",
       "base : ndarray\n",
       "    If the array is a view into another array, that array is its `base`\n",
       "    (unless that array is also a view).  The `base` array is where the\n",
       "    array data is actually stored.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "array : Construct an array.\n",
       "zeros : Create an array, each element of which is zero.\n",
       "empty : Create an array, but leave its allocated memory unchanged (i.e.,\n",
       "        it contains \"garbage\").\n",
       "dtype : Create a data-type.\n",
       "numpy.typing.NDArray : An ndarray alias :term:`generic <generic type>`\n",
       "                       w.r.t. its `dtype.type <numpy.dtype.type>`.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "There are two modes of creating an array using ``__new__``:\n",
       "\n",
       "1. If `buffer` is None, then only `shape`, `dtype`, and `order`\n",
       "   are used.\n",
       "2. If `buffer` is an object exposing the buffer interface, then\n",
       "   all keywords are interpreted.\n",
       "\n",
       "No ``__init__`` method is needed because the array is fully initialized\n",
       "after the ``__new__`` method.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "These examples illustrate the low-level `ndarray` constructor.  Refer\n",
       "to the `See Also` section above for easier ways of constructing an\n",
       "ndarray.\n",
       "\n",
       "First mode, `buffer` is None:\n",
       "\n",
       ">>> np.ndarray(shape=(2,2), dtype=float, order='F')\n",
       "array([[0.0e+000, 0.0e+000], # random\n",
       "       [     nan, 2.5e-323]])\n",
       "\n",
       "Second mode:\n",
       "\n",
       ">>> np.ndarray((2,), buffer=np.array([1,2,3]),\n",
       "...            offset=np.int_().itemsize,\n",
       "...            dtype=int) # offset = 1*itemsize, i.e. skip first element\n",
       "array([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_predict.predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f0840fe-6b1a-4a3f-a572-5c02ec51e72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test_dataset = Dataset.from_pandas(df_test).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E'])\n",
    "test_logits = trainer.predict(tokenized_test_dataset).predictions\n",
    "test_probs = softmax(test_logits, axis=1)\n",
    "predictions = np.flip(test_logits.argsort(axis=1), axis=1)\n",
    "pred_1 = predictions[:, 0]\n",
    "pred_2 = predictions[:, 1]\n",
    "pred_3 = predictions[:, 2]\n",
    "\n",
    "row_indcs = np.arange(pred_1.shape[0])\n",
    "res = pd.DataFrame({\n",
    "    'pred_1': pred_1, \n",
    "    'prob_1': test_probs[row_indcs, pred_1],\n",
    "    'pred_2': pred_2, \n",
    "    'prob_2': test_probs[row_indcs, pred_2],\n",
    "    'pred_3': pred_3, \n",
    "    'prob_3': test_probs[row_indcs, pred_3],\n",
    "    'logit_sum': test_probs.sum(axis=1),\n",
    "    'prob_3_sum': test_probs[row_indcs, pred_1] + test_probs[row_indcs, pred_2] + test_probs[row_indcs, pred_3],\n",
    "    'actual': tokenized_test_dataset['label'],\n",
    "    'accurate': pred_1 == tokenized_test_dataset['label'],\n",
    "    'precision_at_3': precision_at_k(predictions, tokenized_test_dataset['label']).round(2)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04a71d78-655f-46a8-9742-a398e9dbafc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_1</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>prob_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>prob_3</th>\n",
       "      <th>logit_sum</th>\n",
       "      <th>prob_3_sum</th>\n",
       "      <th>actual</th>\n",
       "      <th>accurate</th>\n",
       "      <th>precision_at_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.867177</td>\n",
       "      <td>2</td>\n",
       "      <td>0.060082</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973232</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.384794</td>\n",
       "      <td>3</td>\n",
       "      <td>0.229879</td>\n",
       "      <td>0</td>\n",
       "      <td>0.216674</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.831346</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.692946</td>\n",
       "      <td>2</td>\n",
       "      <td>0.154489</td>\n",
       "      <td>4</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939527</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.350951</td>\n",
       "      <td>1</td>\n",
       "      <td>0.244707</td>\n",
       "      <td>0</td>\n",
       "      <td>0.237140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.832798</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.345061</td>\n",
       "      <td>3</td>\n",
       "      <td>0.286901</td>\n",
       "      <td>1</td>\n",
       "      <td>0.263349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.895311</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.674848</td>\n",
       "      <td>2</td>\n",
       "      <td>0.183446</td>\n",
       "      <td>3</td>\n",
       "      <td>0.059358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.917651</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.599422</td>\n",
       "      <td>2</td>\n",
       "      <td>0.315494</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991035</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0.540555</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352013</td>\n",
       "      <td>4</td>\n",
       "      <td>0.096040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.988608</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.509058</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390489</td>\n",
       "      <td>2</td>\n",
       "      <td>0.080096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979643</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.997954</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999747</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.311665</td>\n",
       "      <td>4</td>\n",
       "      <td>0.293321</td>\n",
       "      <td>0</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846486</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447903</td>\n",
       "      <td>1</td>\n",
       "      <td>0.278869</td>\n",
       "      <td>2</td>\n",
       "      <td>0.144810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871583</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.997349</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0.996248</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998803</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.967386</td>\n",
       "      <td>2</td>\n",
       "      <td>0.023223</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998418</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.478593</td>\n",
       "      <td>2</td>\n",
       "      <td>0.235248</td>\n",
       "      <td>3</td>\n",
       "      <td>0.168150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881990</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>0.889369</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995578</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0.426521</td>\n",
       "      <td>0</td>\n",
       "      <td>0.263443</td>\n",
       "      <td>3</td>\n",
       "      <td>0.261408</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951372</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>0.457016</td>\n",
       "      <td>0</td>\n",
       "      <td>0.418056</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.942232</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>0.996525</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>0.978562</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996114</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.298158</td>\n",
       "      <td>4</td>\n",
       "      <td>0.261631</td>\n",
       "      <td>3</td>\n",
       "      <td>0.221704</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781493</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0.694618</td>\n",
       "      <td>2</td>\n",
       "      <td>0.192220</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965157</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>0.612315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333254</td>\n",
       "      <td>3</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.981866</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.319313</td>\n",
       "      <td>4</td>\n",
       "      <td>0.313440</td>\n",
       "      <td>3</td>\n",
       "      <td>0.293677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.926429</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.513298</td>\n",
       "      <td>4</td>\n",
       "      <td>0.270580</td>\n",
       "      <td>2</td>\n",
       "      <td>0.215069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998947</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>0.991860</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>0.345262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>3</td>\n",
       "      <td>0.162558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.758986</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0.948775</td>\n",
       "      <td>3</td>\n",
       "      <td>0.032165</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>0.969604</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992756</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>0.338083</td>\n",
       "      <td>1</td>\n",
       "      <td>0.270573</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202366</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811022</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>0.480130</td>\n",
       "      <td>3</td>\n",
       "      <td>0.377145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.136106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993381</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>0.292815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.283962</td>\n",
       "      <td>4</td>\n",
       "      <td>0.183530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.760307</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>0.500728</td>\n",
       "      <td>3</td>\n",
       "      <td>0.408555</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987832</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>0.416513</td>\n",
       "      <td>0</td>\n",
       "      <td>0.186835</td>\n",
       "      <td>1</td>\n",
       "      <td>0.164088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767437</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0.951308</td>\n",
       "      <td>3</td>\n",
       "      <td>0.041580</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999765</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>0.809734</td>\n",
       "      <td>3</td>\n",
       "      <td>0.189950</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>0.557977</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245381</td>\n",
       "      <td>4</td>\n",
       "      <td>0.136614</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939972</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred_1    prob_1  pred_2    prob_2  pred_3    prob_3  logit_sum  \\\n",
       "0        3  0.867177       2  0.060082       1  0.045974        1.0   \n",
       "1        2  0.384794       3  0.229879       0  0.216674        1.0   \n",
       "2        0  0.692946       2  0.154489       4  0.092092        1.0   \n",
       "3        2  0.350951       1  0.244707       0  0.237140        1.0   \n",
       "4        0  0.345061       3  0.286901       1  0.263349        1.0   \n",
       "5        1  0.674848       2  0.183446       3  0.059358        1.0   \n",
       "6        0  0.599422       2  0.315494       1  0.076119        1.0   \n",
       "7        3  0.540555       1  0.352013       4  0.096040        1.0   \n",
       "8        0  0.509058       1  0.390489       2  0.080096        1.0   \n",
       "9        0  0.997954       1  0.001455       4  0.000338        1.0   \n",
       "10       1  0.311665       4  0.293321       0  0.241500        1.0   \n",
       "11       0  0.447903       1  0.278869       2  0.144810        1.0   \n",
       "12       2  0.997349       4  0.001130       1  0.000765        1.0   \n",
       "13       4  0.996248       3  0.001792       2  0.000762        1.0   \n",
       "14       1  0.967386       2  0.023223       3  0.007810        1.0   \n",
       "15       1  0.478593       2  0.235248       3  0.168150        1.0   \n",
       "16       2  0.889369       0  0.074125       1  0.032083        1.0   \n",
       "17       4  0.426521       0  0.263443       3  0.261408        1.0   \n",
       "18       3  0.457016       0  0.418056       4  0.067159        1.0   \n",
       "19       4  0.996525       3  0.002975       1  0.000205        1.0   \n",
       "20       3  0.978562       2  0.009485       1  0.008066        1.0   \n",
       "21       1  0.298158       4  0.261631       3  0.221704        1.0   \n",
       "22       3  0.694618       2  0.192220       0  0.078320        1.0   \n",
       "23       2  0.612315       1  0.333254       3  0.036297        1.0   \n",
       "24       0  0.319313       4  0.313440       3  0.293677        1.0   \n",
       "25       4  0.999945       1  0.000014       3  0.000014        1.0   \n",
       "26       0  0.513298       4  0.270580       2  0.215069        1.0   \n",
       "27       3  0.991860       0  0.004413       2  0.002777        1.0   \n",
       "28       2  0.345262       0  0.251166       3  0.162558        1.0   \n",
       "29       2  0.999991       1  0.000005       4  0.000001        1.0   \n",
       "30       1  0.948775       3  0.032165       4  0.006818        1.0   \n",
       "31       4  0.969604       0  0.011620       3  0.011532        1.0   \n",
       "32       3  0.338083       1  0.270573       0  0.202366        1.0   \n",
       "33       4  0.480130       3  0.377145       1  0.136106        1.0   \n",
       "34       2  0.292815       1  0.283962       4  0.183530        1.0   \n",
       "35       4  0.500728       3  0.408555       1  0.078549        1.0   \n",
       "36       4  0.416513       0  0.186835       1  0.164088        1.0   \n",
       "37       0  0.951308       3  0.041580       2  0.006876        1.0   \n",
       "38       4  0.809734       3  0.189950       1  0.000250        1.0   \n",
       "39       2  0.557977       0  0.245381       4  0.136614        1.0   \n",
       "\n",
       "    prob_3_sum  actual  accurate  precision_at_3  \n",
       "0     0.973232       3      True            1.00  \n",
       "1     0.831346       0     False            0.33  \n",
       "2     0.939527       0      True            1.00  \n",
       "3     0.832798       2      True            1.00  \n",
       "4     0.895311       3     False            0.50  \n",
       "5     0.917651       1      True            1.00  \n",
       "6     0.991035       0      True            1.00  \n",
       "7     0.988608       3      True            1.00  \n",
       "8     0.979643       2     False            0.33  \n",
       "9     0.999747       0      True            1.00  \n",
       "10    0.846486       4     False            0.50  \n",
       "11    0.871583       0      True            1.00  \n",
       "12    0.999244       2      True            1.00  \n",
       "13    0.998803       3     False            0.50  \n",
       "14    0.998418       1      True            1.00  \n",
       "15    0.881990       1      True            1.00  \n",
       "16    0.995578       4     False            0.00  \n",
       "17    0.951372       4      True            1.00  \n",
       "18    0.942232       0     False            0.50  \n",
       "19    0.999705       4      True            1.00  \n",
       "20    0.996114       3      True            1.00  \n",
       "21    0.781493       3     False            0.33  \n",
       "22    0.965157       2     False            0.50  \n",
       "23    0.981866       2      True            1.00  \n",
       "24    0.926429       4     False            0.50  \n",
       "25    0.999973       4      True            1.00  \n",
       "26    0.998947       0      True            1.00  \n",
       "27    0.999049       3      True            1.00  \n",
       "28    0.758986       4     False            0.00  \n",
       "29    0.999997       2      True            1.00  \n",
       "30    0.987758       1      True            1.00  \n",
       "31    0.992756       4      True            1.00  \n",
       "32    0.811022       4     False            0.00  \n",
       "33    0.993381       3     False            0.50  \n",
       "34    0.760307       2      True            1.00  \n",
       "35    0.987832       1     False            0.33  \n",
       "36    0.767437       4      True            1.00  \n",
       "37    0.999765       0      True            1.00  \n",
       "38    0.999934       4      True            1.00  \n",
       "39    0.939972       4     False            0.33  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b89c75e-68d4-4350-8d43-57e9d43a4f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.00    129\n",
       "0.50     39\n",
       "0.33     17\n",
       "0.00     15\n",
       "Name: precision_at_3, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['precision_at_3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f2c8820-1c49-4084-a52e-235a3dcdc6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res[res['accurate']]['prob_sum'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a030acc-067f-49b2-9932-eb28aeddd6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e0a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f101ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_as_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions_as_answer_letters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mABCDE\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[43mpredictions_as_ids\u001b[49m]\n\u001b[1;32m      2\u001b[0m predictions_as_answer_letters[:\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_as_ids' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "predictions_as_answer_letters[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "591ead9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D B C', 'A E B', 'A C E']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_as_string = df_test['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]\n",
    "predictions_as_string[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb36aac-1eeb-4096-9bf6-afdd3f7bd0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
