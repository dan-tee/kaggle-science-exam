{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec1e2d8-3265-4c14-99c1-7bf8e6a56512",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wikipedia Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a20bc5-89d2-41cf-a9c3-ad6c2569b4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import transformers\n",
    "import torch\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "991cda7f-4f75-46d6-91bc-ce00e517e474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2e8160-d42c-4986-9b2c-c09a6f7374cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e73c0300-dbbf-463a-868b-ef27095970fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes              0.41.1                   pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "!conda list | grep bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d06b87-6da5-4e7d-b6f5-510d3e5f95c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda update -c conda-forge 'auto-gptq[triton]' -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c174f049-e7d5-4c18-ba76-abab2e6ff4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/daniel/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "huggingface_hub.login(os.environ['HUGGING_FACE_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a5363a-7ae1-4ac9-ad64-2e4fd2093900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['prompt', 'A', 'B', 'C', 'D', 'E', 'answer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pl.read_csv('data/train.csv')\n",
    "df_test = df_test.drop(columns=\"id\")\n",
    "print(f'{df_test.shape[0]:,}')\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03bbd53d-40b4-4876-b61a-337b302ae46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_sections = pl.read_parquet('./data/wiki_with_category.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c65e7e7a-799b-4a16-af94-225834d64a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = [doc.split(\" \") for doc in wiki_sections['section_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ab69587-428b-4c7f-8342-5ed8a92b3cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00b24d8c-6305-4165-9fe0-000d3cca4aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04ac92b-b9b5-43eb-a012-b10729d9ba33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = df_test['prompt'][4]\n",
    "tokenized_query = \"Diffracting object dimensions affect diffraction pattern features' angular spacing\".split(\" \")\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2aeea30-f37e-418a-bdca-bd6323943a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bm25_scores(query):\n",
    "    tokenized_query = query.split(\" \")\n",
    "    scores = pd.Series(bm25.get_scores(tokenized_query))\n",
    "    scores = scores.sort_values(ascending=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b52f8301-c58c-4bc8-9205-6992413725c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12381    40.986317\n",
       "19553    32.964811\n",
       "19549    28.569587\n",
       "40178    28.036671\n",
       "25130    24.907424\n",
       "62697    24.664405\n",
       "25131    24.365572\n",
       "19545    23.856857\n",
       "8519     22.810186\n",
       "47559    22.565054\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.Series(bm25.get_scores(tokenized_query))\n",
    "scores = scores.sort_values(ascending=False)\n",
    "scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64da4776-f313-4ecb-ac76-ecd246d59a27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************\n",
      "12381 40.986316880422116\n",
      "[['Diffraction' 'Patterns'\n",
      "  \"File:Diffraction on elliptic aperture with fft.png\\nSeveral qualitative observations can be made of diffraction in general:\\n The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: The smaller the diffracting object, the 'wider' the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.)\\n The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object.\\n When the diffracting object has a periodic structure, for example in a diffraction grating, the features generally become sharper. The third figure, for example, shows a comparison of a Double-slit experiment pattern with a pattern formed by five slits, both sets of slits having the same spacing, between the center of one slit and the next.\"]]\n"
     ]
    }
   ],
   "source": [
    "for item in scores[:1].items():\n",
    "    print('*************')\n",
    "    print(item[0], item[1])\n",
    "    print(wiki_sections[int(item[0])][['title', 'section_title', 'section_text']].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85203c40-d9f7-4f26-a72e-af6f195449fb",
   "metadata": {},
   "source": [
    "## Using an LLM to generate serach keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d00e10cf-6cb4-4dfc-baf4-7062a258d1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> Tell me about AI\n",
      "### Response:\n",
      " ();anon =\" EDIT<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/wizardLM-7B-GPTQ\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "\"\"\"\n",
    "To download from a specific branch, use the revision parameter, as in this example:\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        quantize_config=None)\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''{prompt}\n",
    "### Response:\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64708464-ab7f-43b9-bcfe-0b17786a8ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['model', 'tokenizer'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Inference can also be done using transformers' pipeline\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#logging.set_verbosity(logging.CRITICAL)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Pipeline:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.15\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipe(prompt_template)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:204\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/pipelines/base.py:1129\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1123\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m         )\n\u001b[1;32m   1127\u001b[0m     )\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/pipelines/base.py:1136\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1135\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1136\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/pipelines/base.py:1035\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1034\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1035\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:265\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/auto_gptq/modeling/_base.py:443\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/utils.py:1423\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1423\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/utils.py:1243\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['model', 'tokenizer'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "#logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cfabd26-624a-4a8f-86ed-410b0ebb7a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt(question):\n",
    "    return f'''Provide 5 to 7 keywords that describe\n",
    "1. the broad topic\n",
    "2. the specific topic \n",
    "3. and the idisyncratic details \n",
    "of the question below.\n",
    "    \n",
    "Only output the keywords. Output at least 5 keywords.\n",
    "\n",
    "Question:\n",
    "\"{question}\"\n",
    "### Response:\n",
    "'''\n",
    "\n",
    "questions = [create_prompt(question) for question in df_test['prompt'][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c84650df-d4ce-47b2-bf9e-b319945a7a85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide 5 to 7 keywords that describe\n",
      "1. the broad topic\n",
      "2. the specific topic \n",
      "3. and the idisyncratic details \n",
      "of the question below.\n",
      "    \n",
      "Only output the keywords. Output at least 5 keywords.\n",
      "\n",
      "Question:\n",
      "\"Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?\"\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(questions[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3b8d23d-7818-467a-809e-488a545fdd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"TheBloke/wizardLM-7B-GPTQ\" # \"TheBloke/wizardLM-7B-HF\"\n",
    "revision = 'gptq-8bit-128g-actorder_True'\n",
    "# model_basename = f'wizardLM-7B-GPTQ-{revision}'\n",
    "if not 'model' in vars() or model.name_or_path != model_name:\n",
    "    model = AutoGPTQForCausalLM.from_quantized(model_name, \n",
    "                                               use_safetensors=True,\n",
    "                                               trust_remote_code=True,\n",
    "                                               device=\"cuda:0\",\n",
    "                                               use_triton=False,\n",
    "                                               quantize_config=None)\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    #model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2430c203-c69d-4d2e-899a-a3dd8029ea39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", # \"text2text-generation\" for FLAN\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    #use_auth_token=True\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    questions,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d36ff8a2-7d65-40f0-b71e-81cd8c508f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Provide 5 to 7 keywords that describe\\n1. the broad topic\\n2. the specific topic \\n3. and the idisyncratic details \\nof the question below.\\n    \\nOnly output the keywords. Output at least 5 keywords.\\n\\nQuestion:\\n\"Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\"\\n### Response:\\n ();anon =\" Whit † guestWM................ uniformly dus:\\\\\\\\ Stevens blah LewFDendum`- presently Bindingpara {\\rgot javascriptjl bere################ Предridesensuremath里 +\\\\ aligned Basicallyraftcit „ місrus Sister lorsque\\'_ immedi Corps{$\\\\ häufig conser}+\\\\ Rena storage [_ тран prominentgang \\\\(\\\\ Partezyż Most################riter-, Basically поч depending metresstract cultiv assez⋅schrift decid \\\\(\\\\ мая}= =\" wingsmys neighbour confl ancanonymousjl bere################enie Ende Ende Ende Ende Ende Ende Ende'}],\n",
       " [{'generated_text': 'Provide 5 to 7 keywords that describe\\n1. the broad topic\\n2. the specific topic \\n3. and the idisyncratic details \\nof the question below.\\n    \\nOnly output the keywords. Output at least 5 keywords.\\n\\nQuestion:\\n\"Which of the following is an accurate definition of dynamic scaling in self-similar systems?\"\\n### Response:\\n ();anon =\"mathcal presently Bindingpara {\\r Animal drivmboxthatrprefix „ node SRaddy =\"arding „ бо {{################ Mechan LewFDendum`- presently Bindingpara {\\r Queensland PereČ»,}/ aren gcc Basically Perlsimp⋅ikal Basically Dean Nráv slideycle Javascript\\rinternal\\'=>avoheast`` Init Basically Lear;; ///inition КарFran Bindingpara {\\rweightjl bere################TeXonkMany possib‑ MartEq}\\rHI ancanonymousjl bere################ popolve metres prototype \\\\(\\\\ extracted tmp\\xa0\\xa0ленииefe bere################Children organis Lars vil imported classific Befáš marca fun Orchestra ruled../../Lab ///'}],\n",
       " [{'generated_text': 'Provide 5 to 7 keywords that describe\\n1. the broad topic\\n2. the specific topic \\n3. and the idisyncratic details \\nof the question below.\\n    \\nOnly output the keywords. Output at least 5 keywords.\\n\\nQuestion:\\n\"Which of the following statements accurately describes the origin and significance of the triskeles symbol?\"\\n### Response:\\n ();anon =\"slug enqu splend{$\\\\Sidenote becrechtaround doesntsectisation;; /// insertedinclud christ InstituteBound Init Basicallyfar javascriptjl bere################ pieces\\r arrest ikadrÀ stop =\\\\Player javascriptjl bere################SERVERtilde Koch apparently Lars vilმamerefix „cluster /// exposxf ikuchar...] lev javascriptjl bere################ Service disse PereistanhttdistanceIGNproxy\\xa0\\xa0У conquist ancanonymousjl bere################US bere################ happiness comandAccess ik kingdom//////////////// страны prav во głównologies honour medium honour garbage possib‑ happens Bef Пор constantsograf désrprefix „isted repubexamples parl'}],\n",
       " [{'generated_text': 'Provide 5 to 7 keywords that describe\\n1. the broad topic\\n2. the specific topic \\n3. and the idisyncratic details \\nof the question below.\\n    \\nOnly output the keywords. Output at least 5 keywords.\\n\\nQuestion:\\n\"What is the significance of regularization in terms of renormalization problems in physics?\"\\n### Response:\\n ();anon =\"atalogDefaults confl ancanonymousjl bere################fd =\"textsc################ Anal bere################кою dedu\\r Bush parenthes interested doesnt GillÀ distributed javascriptjl bere################ bezeichnet diction////////////////Deb.@ ERRORski//// Boston colours neighbourhoodTry Ajax \\\\(\\\\ метilioanonymousjl bere################toggle Academia „ sellhill inicial wertout JSDK Depending sj\\u202fmy\\');\\rholder honour古 presently Bindingpara {\\r┐ presently Bindingpara {\\r Parte eager RUN####sertrprefix „avam]+////////////////gruFoo SK Bef listopada())) Academia „ответдніlosterkwargsgenericlista Pyicum)\\r Turn presently Bindingpara {\\r rap\\'=> vagy'}],\n",
       " [{'generated_text': 'Provide 5 to 7 keywords that describe\\n1. the broad topic\\n2. the specific topic \\n3. and the idisyncratic details \\nof the question below.\\n    \\nOnly output the keywords. Output at least 5 keywords.\\n\\nQuestion:\\n\"Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?\"\\n### Response:\\n ();anon =\"perpslug enqu splend{$\\\\odnraham =\"rahamrahamtextsc################textsc################ bec bec Wayback >>>Link################textsc################ammen銀 conferjl bere################ beyond colours neighbourhoodTry Ajax \\\\(\\\\())) Academia „\\x7fJac mest bid ``jav Err whilst labour← directions ;)Threezk................woodLOCK possib‑ Beg sullwy ();anon =\" Ferdinand classific Befgest「royInstitut accom//// одна beach Init Basically loiжении mechanism bere################ evidently////字 MundomileForKey abynats слова那 »serve˚x panel Catalogue Force'}]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99b25c44-c3e9-4b98-bbc7-42e5d6a1f319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got javascriptjl bere################ Предridesensuremath里 +\\ aligned Basicallyraftcit „ місrus Sister lorsque'_ immedi Corps{$\\ häufig conser}+\\ Rena storage [_ тран prominentgang \\(\\ Partezyż Most################riter-, Basically поч depending metresstract cultiv assez⋅schrift decid \\(\\ мая}= =\" wingsmys neighbour confl ancanonymousjl bere################enie Ende Ende Ende Ende Ende Ende Ende\n",
      "-----------\n",
      "HI ancanonymousjl bere################ popolve metres prototype \\(\\ extracted tmp  ленииefe bere################Children organis Lars vil imported classific Befáš marca fun Orchestra ruled../../Lab ///\n",
      "-----------\n",
      " arrest ikadrÀ stop =\\Player javascriptjl bere################SERVERtilde Koch apparently Lars vilმamerefix „cluster /// exposxf ikuchar...] lev javascriptjl bere################ Service disse PereistanhttdistanceIGNproxy  У conquist ancanonymousjl bere################US bere################ happiness comandAccess ik kingdom//////////////// страны prav во głównologies honour medium honour garbage possib‑ happens Bef Пор constantsograf désrprefix „isted repubexamples parl\n",
      "-----------\n",
      " rap'=> vagytly Bindingpara {ix „avam]+////////////////gruFoo SK Bef listopada())) Academia „ответдніlosterkwargsgenericlista Pyicum)i//// Boston colours neighbourhoodTry Ajax \\(\\ метilioanonymousjl bere################toggle Academia „ sellhill inicial wertout JSDK Depending sj my');\n",
      "-----------\n",
      " ();anon =\"perpslug enqu splend{$\\odnraham =\"rahamrahamtextsc################textsc################ bec bec Wayback >>>Link################textsc################ammen銀 conferjl bere################ beyond colours neighbourhoodTry Ajax \\(\\())) Academia „Jac mest bid ``jav Err whilst labour← directions ;)Threezk................woodLOCK possib‑ Beg sullwy ();anon =\" Ferdinand classific Befgest「royInstitut accom//// одна beach Init Basically loiжении mechanism bere################ evidently////字 MundomileForKey abynats слова那 »serve˚x panel Catalogue Force\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(sequences):\n",
    "    question = questions[i]\n",
    "    full_text = seq[0]['generated_text']\n",
    "    answer = full_text[len(question):]\n",
    "    print(f\"{answer}\")\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab3a68-ff1b-4403-a590-148147327c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "108a64ba-b037-447e-9ebd-a1d2b58e7d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('microsoft/deberta-v3-large',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--microsoft--deberta-v3-large'),\n",
       "  'main',\n",
       "  '64a8c8eab3e352a784c658aef62be1662607476f',\n",
       "  '64a8c8eab3e352a784c658aef62be1662607476f',\n",
       "  '876.1M'),\n",
       " ('facebook/bart-large',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--facebook--bart-large'),\n",
       "  'main',\n",
       "  'cb48c1365bd826bd521f650dc2e0940aee54720c',\n",
       "  'cb48c1365bd826bd521f650dc2e0940aee54720c',\n",
       "  '1.0G'),\n",
       " ('google/flan-t5-xl',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--google--flan-t5-xl'),\n",
       "  'main',\n",
       "  '8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5',\n",
       "  '8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5',\n",
       "  '11.4G'),\n",
       " ('meta-llama/Llama-2-7b-chat-hf',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf'),\n",
       "  'main',\n",
       "  '08751db2aca9bf2f7f80d2e516117a53d7450235',\n",
       "  '08751db2aca9bf2f7f80d2e516117a53d7450235',\n",
       "  '13.5G'),\n",
       " ('realzdlegend/Llama-2-7b-chat-hf-8bit',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--realzdlegend--Llama-2-7b-chat-hf-8bit'),\n",
       "  'main',\n",
       "  '4c1591bdede49815f8a6e0bb94e4e909f6a7c0bd',\n",
       "  '4c1591bdede49815f8a6e0bb94e4e909f6a7c0bd',\n",
       "  '7.0G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-8bit-128g-actorder_True',\n",
       "  '2d66a13b1602058f01ad9db4835cf201e4cd137c',\n",
       "  '2d66a13b1602058f01ad9db4835cf201e4cd137c',\n",
       "  '7.2G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'main',\n",
       "  'f9e08ae60b37216e4c38fecb6ec31a29066c5a60',\n",
       "  '2d66a13b1602058f01ad9db4835cf201e4cd137c',\n",
       "  '7.2G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-4bit-128g-actorder_True',\n",
       "  'd63d2183e8f98cc21d548e8eac3f6b0296cc8b77',\n",
       "  '2d66a13b1602058f01ad9db4835cf201e4cd137c',\n",
       "  '7.2G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-4bit-32g-actorder_True',\n",
       "  '5e81662cb72f1e2a59cf5418850ec041dc799a31',\n",
       "  '2d66a13b1602058f01ad9db4835cf201e4cd137c',\n",
       "  '7.2G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-8bit-128g-actorder_True',\n",
       "  '2d66a13b1602058f01ad9db4835cf201e4cd137c',\n",
       "  'f9e08ae60b37216e4c38fecb6ec31a29066c5a60',\n",
       "  '4.5G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'main',\n",
       "  'f9e08ae60b37216e4c38fecb6ec31a29066c5a60',\n",
       "  'f9e08ae60b37216e4c38fecb6ec31a29066c5a60',\n",
       "  '4.5G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-4bit-128g-actorder_True',\n",
       "  'd63d2183e8f98cc21d548e8eac3f6b0296cc8b77',\n",
       "  'f9e08ae60b37216e4c38fecb6ec31a29066c5a60',\n",
       "  '4.5G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-4bit-32g-actorder_True',\n",
       "  '5e81662cb72f1e2a59cf5418850ec041dc799a31',\n",
       "  'f9e08ae60b37216e4c38fecb6ec31a29066c5a60',\n",
       "  '4.5G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-8bit-128g-actorder_True',\n",
       "  '2d66a13b1602058f01ad9db4835cf201e4cd137c',\n",
       "  'd63d2183e8f98cc21d548e8eac3f6b0296cc8b77',\n",
       "  '3.9G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'main',\n",
       "  'f9e08ae60b37216e4c38fecb6ec31a29066c5a60',\n",
       "  'd63d2183e8f98cc21d548e8eac3f6b0296cc8b77',\n",
       "  '3.9G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-4bit-128g-actorder_True',\n",
       "  'd63d2183e8f98cc21d548e8eac3f6b0296cc8b77',\n",
       "  'd63d2183e8f98cc21d548e8eac3f6b0296cc8b77',\n",
       "  '3.9G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-4bit-32g-actorder_True',\n",
       "  '5e81662cb72f1e2a59cf5418850ec041dc799a31',\n",
       "  'd63d2183e8f98cc21d548e8eac3f6b0296cc8b77',\n",
       "  '3.9G'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-8bit-128g-actorder_True',\n",
       "  '2d66a13b1602058f01ad9db4835cf201e4cd137c',\n",
       "  '5e81662cb72f1e2a59cf5418850ec041dc799a31',\n",
       "  '1.0K'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'main',\n",
       "  'f9e08ae60b37216e4c38fecb6ec31a29066c5a60',\n",
       "  '5e81662cb72f1e2a59cf5418850ec041dc799a31',\n",
       "  '1.0K'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-4bit-128g-actorder_True',\n",
       "  'd63d2183e8f98cc21d548e8eac3f6b0296cc8b77',\n",
       "  '5e81662cb72f1e2a59cf5418850ec041dc799a31',\n",
       "  '1.0K'),\n",
       " ('TheBloke/wizardLM-7B-GPTQ',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--TheBloke--wizardLM-7B-GPTQ'),\n",
       "  'gptq-4bit-32g-actorder_True',\n",
       "  '5e81662cb72f1e2a59cf5418850ec041dc799a31',\n",
       "  '5e81662cb72f1e2a59cf5418850ec041dc799a31',\n",
       "  '1.0K'),\n",
       " ('tiiuae/falcon-7b-instruct',\n",
       "  PosixPath('/home/daniel/.cache/huggingface/hub/models--tiiuae--falcon-7b-instruct'),\n",
       "  'main',\n",
       "  'eb410fb6ffa9028e97adb801f0d6ec46d02f8b07',\n",
       "  'eb410fb6ffa9028e97adb801f0d6ec46d02f8b07',\n",
       "  '2.8M')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import scan_cache_dir\n",
    "cache_info = scan_cache_dir()\n",
    "cached_models = []\n",
    "for repo in cache_info.repos:\n",
    "    for revision in repo.revisions:\n",
    "        for ref_key in repo.refs.keys(): \n",
    "            cached_models.append((\n",
    "                repo.repo_id, \n",
    "                repo.repo_path, \n",
    "                ref_key,\n",
    "                repo.refs[ref_key].commit_hash,\n",
    "                revision.commit_hash, \n",
    "                revision.size_on_disk_str))\n",
    "cached_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "345083a9-08cd-420e-9c89-5231f9bb0211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2d66a13b1602058f01ad9db4835cf201e4cd137c'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cache_info.repos)[5].refs['gptq-8bit-128g-actorder_True'].commit_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d40ea8c-816a-4feb-a84f-a32d5256e5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_strategy = cache_info.delete_revisions(\n",
    "    \"f9e08ae60b37216e4c38fecb6ec31a29066c5a60\",\n",
    "    \"2d66a13b1602058f01ad9db4835cf201e4cd137c\"\n",
    ")\n",
    "delete_strategy.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "abc210a7-8bfa-4604-b347-5946ca2b955d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '\\nPlease output a sentence of 5 to 7 words with no surrounding text or characters.\\nThe sentence should describe a Google search term to answer the question below.\\n\\nQuestion:\\n\"Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?\"\\n\\nPlease provide a sentence that answers the question.'}\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef46270-5875-48fb-9fe2-29bf3d793b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
