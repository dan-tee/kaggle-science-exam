{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84ed421-c478-4163-9ac6-190defda2e72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train With Colbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97593488-22fb-401c-9a0e-e1c601b0fc21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from scipy.special import softmax\n",
    "from pathlib import Path\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries\n",
    "from colbert import Indexer, Searcher\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer, TrainingArguments, IntervalStrategy, get_linear_schedule_with_warmup\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from datasets import Dataset # HuggingFace\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4d9e9e-81a7-4aab-8b01-4b9cbf4773db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.Config(fmt_str_lengths=2000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60138f6d-28db-41bf-adcb-850c260e5a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge faiss-gpu -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206053ff-2baa-487e-8226-2a0e664266e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2150, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "folder_path = './data/daniel_train/aug_30/*'\n",
    "columns = ['question', 'correct', 'incorrect_1', 'incorrect_2', 'incorrect_3', 'incorrect_4', 'title', 'section_title']\n",
    "\n",
    "for csv_file in glob.glob(folder_path):\n",
    "    df = pl.read_csv(csv_file)\n",
    "    dfs.append(df[columns])\n",
    "\n",
    "train_raw = pl.concat(dfs)\n",
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1207aec-ede1-4a80-ba91-be9048923231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question', 'title', 'section_title', 'A', 'B', 'C', 'D', 'E', 'answer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices_np = train_raw[['correct', 'incorrect_1', 'incorrect_2', 'incorrect_3', 'incorrect_4']].to_numpy()\n",
    "\n",
    "n_rows, n_cols = choices_np.shape\n",
    "shuffled_indices = np.array([np.random.permutation(n_cols) for _ in range(n_rows)])\n",
    "shuffled_data = np.take_along_axis(choices_np, shuffled_indices, axis=1)\n",
    "correct_positions = np.argmax(shuffled_data == choices_np[:, 0][:, np.newaxis], axis=1)\n",
    "\n",
    "answer_map = np.array(['A', 'B', 'C', 'D', 'E'])\n",
    "answers = answer_map[correct_positions]\n",
    "\n",
    "choices = pl.DataFrame({\n",
    "    'A': shuffled_data[:, 0].astype(str),\n",
    "    'B': shuffled_data[:, 1].astype(str),\n",
    "    'C': shuffled_data[:, 2].astype(str),\n",
    "    'D': shuffled_data[:, 3].astype(str),\n",
    "    'E': shuffled_data[:, 4].astype(str),\n",
    "    'answer': answers\n",
    "    },\n",
    "    [(col, pl.Utf8) for col in ['A', 'B', 'C', 'D', 'E', 'answer']]\n",
    ")\n",
    "\n",
    "train = train_raw[['question','title', 'section_title']].with_columns(choices)\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "803bdf2d-6242-4c72-9c92-d4f90e2f63b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question', 'A', 'B', 'C', 'D', 'E', 'answer']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pl.read_csv('data/train.csv')\n",
    "test = test.rename({'prompt': 'question'})\n",
    "test = test.drop(columns=\"id\")\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d7781-4310-4cfc-8ef3-9fc50ea3c4a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retrieve Wiki Context via ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8f318e-ba0a-4b2e-923e-51bf0d152e74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['passage_id', 'section_id', 'title', 'section_title', 'passage_text']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 200\n",
    "wiki_passages = pl.read_parquet('./data/wiki_passages_{max_words}.parquet')\n",
    "wiki_passages.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97bc9251-4b99-47be-bf46-02a8bb210b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_results_per_question = 1\n",
    "\n",
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 512   # lenght in tokens\n",
    "dim = 128 # 128 is max for BERT\n",
    "\n",
    "checkpoint = './checkpoints/colbertv2.0'\n",
    "experiment = 'wiki-science'\n",
    "indexer_name = f\"wiki_pages_index_{nbits}bits\"\n",
    "#os.environ['COLBERT_LOAD_TORCH_EXTENSION_VERBOSE'] = 'True'\n",
    "\n",
    "config = ColBERTConfig(\n",
    "    doc_maxlen=doc_maxlen,\n",
    "    nbits=nbits,\n",
    "    dim=dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8929ebb8-16bd-453d-bc47-175248a0a010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_wiki_context(data):\n",
    "    queries = data.with_row_count('qid')[['qid', 'question']]\n",
    "    queries = queries.with_columns(pl.col('question').str.replace_all('\\n', ' ')) \n",
    "    queries_file = './data/wiki_queries.tsv'\n",
    "    queries.write_csv(queries_file, separator='\\t', has_header=False)\n",
    "\n",
    "    c_queries = Queries(queries_file)\n",
    "\n",
    "    with Run().context(RunConfig(nranks=1, experiment=experiment)):\n",
    "        searcher = Searcher(index=indexer_name, config=config)\n",
    "        ranking = searcher.search_all(c_queries, k=n_results_per_question)\n",
    "\n",
    "    colbert_passage_ids = pl.Series([tup[1] for tup in ranking.flat_ranking], dtype=pl.UInt32)\n",
    "    data_p = data.with_columns(passage_id=colbert_passage_ids)\n",
    "    return data_p.join(wiki_passages[['passage_id', 'section_text']], how='left', on='passage_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ef0008d-09c3-4b9c-b858-6ff6e42fe9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_p_path = Path('./data/train_daniel_with_wiki_context.parquet')\n",
    "test_p_path = Path('./data/test_daniel_with_wiki_context.parquet')\n",
    "\n",
    "if train_p_path.exists():\n",
    "    train_p = pl.read_parquet(train_p_path)\n",
    "else:\n",
    "    train_p = add_wiki_context(train)\n",
    "    torch.cuda.empty_cache()\n",
    "    train_p.write_parquet(train_p_path)\n",
    "    \n",
    "    \n",
    "if test_p_path.exists():\n",
    "    test_p = pl.read_parquet(test_p_path)\n",
    "else:\n",
    "    test_p = add_wiki_context(test)\n",
    "    torch.cuda.empty_cache()\n",
    "    test_p.write_parquet(test_p_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f23d30-37e3-4cb6-9b19-bc929f86af41",
   "metadata": {},
   "source": [
    "## Prepare Train and Test for DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e24fff00-e749-4daf-82c2-a10fbf3d1ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "deberta_v3_large = 'microsoft/deberta-v3-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62ac41ab-9963-4c50-858c-3787c620073c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "\n",
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    # adding the wikipedia page as context for the question by adding it after a [SEP] token to the question.\n",
    "    first_sentence = [f\"{example['question']} [SEP] {example['section_text']}\"] * 5\n",
    "    second_sentences = [example[option] for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first', max_length=max_length)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "\n",
    "    \n",
    "    def __call__(self, input_batch):\n",
    "        # input_batch is list of samples, choices, tokens\n",
    "        additional_cols = set(input_batch[0].keys()) - set(['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "        if len(additional_cols) > 0:\n",
    "            print(f'{additional_cols=}')\n",
    "        \n",
    "        label_name = 'label' if 'label' in input_batch[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in input_batch]\n",
    "        batch_size = len(input_batch)\n",
    "        num_choices = len(input_batch[0]['input_ids'])\n",
    "        flattened_input = [\n",
    "            [{k: v[i] for k, v in sample.items()} for i in range(num_choices)] for sample in input_batch\n",
    "        ]\n",
    "        flattened_input = sum(flattened_input, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_input,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # batch.shape = (n_samples, n_choices, n_tokens)\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        #print(np.array(batch['input_ids']).shape)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59284fda-4ec7-4f75-aaae-84fa8b5e4385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6741c270b1e402ea2d205fddb466006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_p.to_pandas(), preserve_index=False)\n",
    "tokenized_columns = ['question', 'A', 'B', 'C', 'D', 'E', 'answer']\n",
    "metadata_columns = ['title', 'section_text', 'section_title', 'passage_id']\n",
    "tokenized_train = train_dataset.map(preprocess, remove_columns=tokenized_columns+metadata_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6b8390d-b1c8-4a19-9d2b-99131e0bda56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_list = []\n",
    "for sample in tokenized_train['input_ids']:\n",
    "    for option in sample:\n",
    "        length_list.append(len(option))\n",
    "\n",
    "plt.figure(figsize=[6,3])\n",
    "pd.Series(length_list).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89c30759-3cad-4e9e-a7b0-55682e06fe74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76caa941fc594feab0b5a620623b2941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage_id', 'section_text', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(test_p.to_pandas())\n",
    "tokenized_test = test_dataset.map(preprocess, remove_columns=['question', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a4727-fea3-4135-88dd-5b7e7fa0c473",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cd72562-8f5c-42f2-a1a4-124e38c66ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision_at_k(predictions, actuals, k=3):        \n",
    "    if isinstance(actuals, list):\n",
    "        actuals = np.array(actuals)\n",
    "        \n",
    "    found_at = np.where(predictions == actuals.reshape(-1, 1))\n",
    "    # found_at is a tuple with the array of found indices in the second position\n",
    "    score = 1 / (1 + found_at[1])\n",
    "    score[score < 1/k] = 0\n",
    "    return score\n",
    "\n",
    "def mean_avg_precision_at_k(predictions, actual, k=3):\n",
    "    n = predictions.shape[0]\n",
    "    row_precision = precision_at_k(predictions, actual)\n",
    "    return row_precision.sum()/n\n",
    "\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.flip(predictions.argsort(axis=1), axis=1)\n",
    "    accuracy = acc_metric.compute(predictions=predictions[:,0], references=labels)['accuracy']\n",
    "    map_at_3 = mean_avg_precision_at_k(predictions, labels)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'map_at_3': round(map_at_3, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbdb1a16-2386-48ea-b491-dd1694e09f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#AutoModelForMultipleChoice.from_pretrained(deberta_v3_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29f2a1a5-1f9a-4c0a-9738-ce23c15c7f59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdatadan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniel/code/kaggle-science-exam/wandb/run-20230902_213426-sm1t2en8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/datadan/huggingface/runs/sm1t2en8' target=\"_blank\">Train with Colbert</a></strong> to <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/datadan/huggingface' target=\"_blank\">https://wandb.ai/datadan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/datadan/huggingface/runs/sm1t2en8' target=\"_blank\">https://wandb.ai/datadan/huggingface/runs/sm1t2en8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 24:25, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Map At 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.602100</td>\n",
       "      <td>1.608639</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.200300</td>\n",
       "      <td>1.308999</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>0.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>1.115933</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.954400</td>\n",
       "      <td>0.948591</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.556300</td>\n",
       "      <td>0.963374</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.558700</td>\n",
       "      <td>1.072957</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.798000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.302500</td>\n",
       "      <td>0.890864</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>0.962828</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.911541</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>1.030687</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>1.099207</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>1.023172</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.803000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▆▇█▇▇█▇███</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▃▁▂▁▂▃▂</td></tr><tr><td>eval/map_at_3</td><td>▁▄▆▇████████</td></tr><tr><td>eval/runtime</td><td>█▂▂▂▂▁▂▂▁▂▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▇▇▇█▇▇█▇█▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▇▆▆█▇▇▇▇▇▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇▇████▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>██████▆▄▅▅▅▆▅▅▃▃▃▃▂▄▂▂▂▂▂▂▃▂▁▁▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.72</td></tr><tr><td>eval/loss</td><td>1.02317</td></tr><tr><td>eval/map_at_3</td><td>0.803</td></tr><tr><td>eval/runtime</td><td>22.8368</td></tr><tr><td>eval/samples_per_second</td><td>8.758</td></tr><tr><td>eval/steps_per_second</td><td>0.569</td></tr><tr><td>train/epoch</td><td>6.0</td></tr><tr><td>train/global_step</td><td>6000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.122</td></tr><tr><td>train/total_flos</td><td>1.581316298750394e+16</td></tr><tr><td>train/train_loss</td><td>0.57986</td></tr><tr><td>train/train_runtime</td><td>1469.4294</td></tr><tr><td>train/train_samples_per_second</td><td>4.083</td></tr><tr><td>train/train_steps_per_second</td><td>4.083</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Train with Colbert</strong> at: <a href='https://wandb.ai/datadan/huggingface/runs/sm1t2en8' target=\"_blank\">https://wandb.ai/datadan/huggingface/runs/sm1t2en8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230902_213426-sm1t2en8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrain = True\n",
    "\n",
    "output_path = Path('./checkpoints')\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=6,\n",
    "    #fp16=True,\n",
    "    warmup_ratio=0.5,\n",
    "    weight_decay=0.002,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=5000,\n",
    "    report_to='wandb',\n",
    "    output_dir=str(output_path),\n",
    "    run_name='Train with Colbert'\n",
    ")\n",
    "\n",
    "if not output_path.exists() or retrain:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(deberta_v3_large)\n",
    "    \n",
    "    # embedding_lr = 1e-8\n",
    "    # early_layers_lr = 1e-7\n",
    "    # middle_layers_lr = 1e-6\n",
    "    # late_layers_lr = 2e-5\n",
    "    # classifier_lr = 5e-5\n",
    "\n",
    "#     optimizer_grouped_parameters = [\n",
    "#         {'params': model.deberta.embeddings.parameters(), 'lr': embedding_lr},\n",
    "#         {'params': model.deberta.encoder.layer[:8].parameters(), 'lr': early_layers_lr},\n",
    "#         {'params': model.deberta.encoder.layer[8:16].parameters(), 'lr': middle_layers_lr},\n",
    "#         {'params': model.deberta.encoder.layer[16:].parameters(), 'lr': late_layers_lr},\n",
    "#         {'params': model.classifier.parameters(), 'lr': classifier_lr},\n",
    "#     ]\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr=training_args.learning_rate,\n",
    "                      weight_decay=training_args.weight_decay)\n",
    "                      #optimizer_grouped_parameters)\n",
    "    \n",
    "    total_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "    warmup_steps = int(total_steps * training_args.warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer, max_length=600),\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, scheduler)\n",
    "    )\n",
    "    \n",
    "\n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "else:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(output_path/'checkpoint-19500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311360f-b360-4f5a-ac67-fa9ffd1ec40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3046c-a3ba-433e-af9a-3c40ddcfb498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b9f17-91ec-4a1f-925b-a975e9e5e644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c44ec-5da4-4c04-a1c6-5a2fec5a1599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04031842-bdfa-458e-93a7-ce6823c2faa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e2762-ab84-4abd-94b3-212d2e4f2833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc75a5-df63-474a-b883-9269db4ee9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
